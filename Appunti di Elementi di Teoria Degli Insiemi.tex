\documentclass[11pt]{scrartcl}
\usepackage[italian]{babel}
\usepackage[sexy]{evan}
\usepackage{float}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% BOOST SOFTWARE LICENSE - VERSION 1.0 - 17 AUGUST 2003
%
% Copyright (c) 2022 Evan Chen [evan at evanchen.cc]
% https://web.evanchen.cc/ || github.com/vEnhance
%
% Available for download at:
% https://github.com/vEnhance/dotfiles/blob/main/texmf/tex/latex/evan/evan.sty
%
% Permission is hereby granted, free of charge, to any person or organization
% obtaining a copy of the software and accompanying documentation covered by
% this license (the "Software") to use, reproduce, display, distribute,
% execute, and transmit the Software, and to prepare derivative works of the
% Software, and to permit third-parties to whom the Software is furnished to
% do so, all subject to the following:
%
% The copyright notices in the Software and this entire statement, including
% the above license grant, this restriction and the following disclaimer,
% must be included in all copies of the Software, in whole or in part, and
% all derivative works of the Software, unless such copies or derivative
% works are solely in the form of machine-executable object code generated by
% a source language processor.
%
% THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
% IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
% FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
% SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
% FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
% ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
% DEALINGS IN THE SOFTWARE.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\title{Elementi Di Teoria Degli Insiemi}
\subtitle{\large\normalfont\rmfamily\scshape APPUNTI DEL CORSO DI ELEMENTI DI TEORIA DEGLI INSIEMI \\ TENUTO DAL PROF. MARCELLO MAMINO}
\author{Diego Monaco \\ \textnormal{\href{d.monaco2@studenti.unipi.it}{d.monaco2@studenti.unipi.it}} \\ Università di Pisa}
\date{Anno Accademico 2022-23}
\maketitle
\newpage

\tableofcontents
\eject
\newpage

\section*{Premessa}
Queste dispense sono la quasi esatta trascrizione in \LaTeX\,delle dispense del corso di Elementi di teoria degli insiemi, tenuto dal prof. Marcello Mamino nell'anno accademico 2022-23.

\section*{Ringraziamenti}
Francesco Sorce, Rubens Martino.

\mbox{}
\vfill
\begin{wrapfigure}{R}{0.2\textwidth}
	\centering
	\href{https://creativecommons.org/licenses/by-nc/4.0/deed.it}{\includegraphics[width=0.2\textwidth]{licenza.png}}
\end{wrapfigure}

Quest'opera è stata rilasciata con licenza Creative Commons Attribuzione - Condividi allo stesso modo 4.0 Internazionale. Per leggere
una copia della licenza visita il sito web \href{http://creativecommons.org/licenses/by-sa/4.0/deed.it}{\textcolor{blue}{https://creativecommons.org/licenses/by-nc/4.0/deed.it}}.\\

\newpage
\section{Prologo nel XIX secolo}
La nascita della teoria degli insiemi è una storia complicata di cui so pochissimo. Però, persone che ne sanno molto più di me hanno sostenuto l'opinione che il problema seguente
abbia avuto un ruolo. Come che sia, è almeno un'introduzione possibile.

\begin{problem}
Data una serie trigonometrica:
\[ S(x) = c_0 + \sum_{i=1}^{+\infty}a_i\sin{(ix)}+b_i\cos{(ix)}
	\]
se, per ogni $x \in \RR$, sappiamo che $S(x)$ converge a 0, possiamo dire che i coefficienti $c_0,a_i,b_i$ sono tutti 0?
\end{problem}

Risolto positivamente da \href{https://it.wikipedia.org/wiki/Georg_Cantor}{\textcolor{purple}{Georg Cantor}} nel 1870.

\begin{definition}
Diciamo che $X \subseteq \RR$ è un \vocab{insieme di unicità} se, per ogni serie trigonometrica:
\[ S(x) = c_0 + \sum_{i=1}^{+\infty}a_i\sin{(ix)}+b_i\cos{(ix)}
	\]
vale la seguente implicazione:
\[ \text{$S(x)$ converge a 0 per tutti gli $x\not\in X$} \implies \text{tutti i coefficienti $c_0,a_i,b_i$ sono nulli}
	\]
\end{definition}

\begin{example}
	Per il risultato di Cantor, $\emptyset$ è di unicità.
\end{example}

\begin{problem}
	Quali sottoinsiemi di $\RR$ sono di unicità?
\end{problem}

\begin{fact}
\label{unicità}
$X \subseteq \RR$ è di unicità se (ma non solo se) ogni funzione continua $f : \RR \longrightarrow \RR$ che soddisfi le ipotesi seguenti è necessariamente lineare\footnote{$f(x) = \alpha x + \beta$.}:
\begin{itemize}
	\item per ogni intervallo aperto $\left]a,b\right[$ con $]a,b[ \cap X = \emptyset$, $f_{|\left]a,b\right[}$ è lineare;
	\item per ogni $x \in \RR$, se $f$ ha derivate destre e sinistre in $x$, allora queste coincidono\footnote{Ovvero $f$ non ha punti angolosi.}.
\end{itemize}
\end{fact}

\begin{example}
	$X = \{\ldots,a_{-2},a_{-1},a_0,a_1,a_2,\ldots\} = \{a_i | i \in \ZZ\}$ con $\ldots < a_{-2} < a_{-1} < a_0 < a_1 < a_2 <\ldots$, $\displaystyle\lim_{i \to +\infty} a_i = +\infty$, $\displaystyle\lim_{i \to -\infty} a_i = -\infty$ ha la 
	proprietà data dal \hyperref[unicità]{Fatto 1.5}, quindi è di unicità.
\end{example}

\begin{notexample}
L'intervallo $[0,1]$ o $\RR$ non hanno la proprietà espressa dall'\hyperref[unicità]{Fatto 1.5}.
\end{notexample}

\begin{notexampleb}
Per l'\vocab{insieme di Cantor} non vale il \hyperref[unicità]{Fatto 1.5}.
\end{notexampleb}

Possiamo costruire l'insieme di Cantor a partire dall'intervallo $C_0 = [0,1]$ nel seguente modo:

\begin{center}
	\begin{figure}[h]
		\centering
		\includegraphics[width=12.5cm]{immagini/cantor.png}
	\end{figure}
\end{center}

ovvero, preso l'intervallo $[0,1]$ possiamo dividerlo in tre parti e rimuovere la parte centrale $\displaystyle\left]\frac 13, \frac 23\right[$, chiamiamo gli intervalli rimanenti $C_1$, possiamo iterare il procedimento sui due segmenti di $C_1$ ed ottenere $C_2,C_3,\ldots$, a questo punto 
definiamo l'insieme di Cantor $C$ come:
\[ C := \bigcap_{i \in \NN}C_i
	\]
Esiste una funzione continua (e crescente) $f : \RR \longrightarrow \RR$ detta \vocab{scala di Cantor} (o \vocab{scala del diavolo}), tale che $f^{\prime}(x) = 0$ per $x \not\in C$ e non è 
derivabile in $x \in C$.

\begin{center}
	\begin{figure}[h]
		\centering
		\includegraphics[width=13.5cm]{immagini/scalacantor.png}
	\end{figure}
\end{center}

tale funzione si costruisce aggiungendo tratti costanti (prima $\displaystyle\frac 12$, poi $\displaystyle\frac 14$, $\displaystyle\frac 34$ e così via, dividendo l'intervallo $[0,1]$ sull'asse delle ordinate in parti uguali) alle parti eliminate sull'intervallo
$[0,1]$ sull'asse delle ascisse per costruire l'insieme di Cantor.

\begin{note}
Per $\QQ$ e $C$ non vale il \hyperref[unicità]{Fatto 1.5} ma, in realtà, sono di unicità.
\end{note}

\begin{exampleb}
L'insieme degli elementi di una successione crescente col suo limite è un esempio di insieme di unicità.
\end{exampleb}

\begin{center}
	\begin{figure}[h]
		\centering
		\includegraphics[width=10.5cm]{immagini/succunic.png}
	\end{figure}
\end{center}

Dimostriamo quindi che $X$ è un insieme di unicità.
\begin{proof}
La funzione $f$ è lineare in $]-\infty, a_0[, ]a_0,a_1[, ]a_1,a_2[, \ldots$. Quindi nei punti $a_0,a_1,a_2,\ldots$ ammette derivata destra e sinistra. 
Siccome questi punti non possono essere angolosi, $f_{|]-\infty, a_0[}$, $f_{|]a_0,a_1[}$, etc. hanno lo stesso coefficiente angolare, quindi, sfruttando la cardinalità, $f_{|]-\infty, a_0[}$
è lineare. Siccome $f_{|]-\infty, a_0[}$ è lineare, usando nuovamente l'assenza di punti angolosi abbiamo la tesi.
\end{proof}

\begin{examplebb}
L'insieme degli elementi di una successione crescente di successioni crescenti è un insieme di unicità.
\end{examplebb}

\begin{center}
	\begin{figure}[h]
		\centering
		\includegraphics[width=12.5cm]{immagini/succunic2.png}
	\end{figure}
\end{center}

Dimostriamo che $X$ è di unicità.
\begin{proof}
In ciascuno degli intervalli $]a_{i0}, a_{(i+1)0}[$, $f$ è lineare, ragionando come nell'esempio precedente, ci siamo ridotti alla situazione
- di nuovo - dell'esempio precedente con $a_i^{\prime} = a_{i0}$.
\end{proof}

\subsection{Digressione: insiemi numerabili}
\begin{definition}
	Un insieme $X$ è \vocab{numerabile} se è il supporto di una successione, $X = \{a_0,a_1,a_2,\ldots\} = \{a_i | i \in \NN\}$, con $a_i \ne a_j$ per ogni $i \ne j$.\footnote{O in altre parole se esiste $f : \NN \longrightarrow X$ biunivoca.}
\end{definition}

\begin{example}
	Alcuni esempi di insiemi numerabili sono:
	\begin{itemize}
		\item $\NN$, l'insieme dei numeri naturali, infatti, la successione $a_i = i$ realizza la bigezione.
		\item I numeri dispari, con la bigezione data da $a_i = 2i + 1$.
		\item I numeri primi, $a_i = p_i$, con $p_i$ $i$-esimo numero primo.
		\item $\ZZ$ l'insieme dei numeri interi, con la bigezione data da $a_i = \displaystyle (-1)^i \left\lceil\frac{i}{2}\right\rceil$.
	\end{itemize}
\end{example}

\begin{examplem}
L'insieme $\NN \times \NN = \{(x,y) | x,y \in \NN\}$ è numerabile.
\end{examplem}

\begin{proof}
La funzione $f : \NN \times \NN \longrightarrow \NN : (x,y) \longmapsto 2^x(1+2y) - 1$ è biunivoca (perché?), quindi $a_i = f^{-1}(i)$ enumera $\NN \times \NN$.
\end{proof}

\begin{proposition}
Un sottoinsieme infinito di un insieme numerabile è, a sua volta, numerabile.
\end{proposition}

\begin{proof}
Sia $Y \subseteq X$ con $Y$ infinito e $X = \{a_i | i \in \NN\}$. La sottosuccessione $b_j = a_{i_j}$ degli $a_*$ che appartengono a $Y$ enumera $Y$. A essere precisi 
bisognerebbe dire esattamente chi sono gli indici $i_j$. Per ricorsione:
\[ i_0 = \min\{i | a_i \in Y\} \qquad i_{j+1} = \min\{i > i_j | a_i \in Y\}
	\]
dove i minimi esistono perché $Y$ non è finito.
\end{proof}

\begin{proposition}
Se $X$ e $Y$ sono numerabili $X \times Y = \{(a,b) | a \in X, b \in Y\}$ è anch'esso numerabile.
\end{proposition}

\begin{proof}
Fissiamo $X = \{a_i | i \in \NN\}$, $Y = \{b_j | j \in \NN\}$. Siccome $\NN \times \NN$ è numerabile, $\NN \times \NN = \{(i_t,j_t)|t \in \NN\}$.
Quindi $X \times Y = \{(a_{i_t}, a_{j_t}) | t \in \NN\}$.
\end{proof}

\begin{example}
$\QQ$ è numerabile.
\end{example}

\begin{proof}
$\QQ$ è in corrispondenza biunivoca con:
\[F = \{(\text{num.},\text{den.})\footnote{num. = numeratore, den. = denominatore.} | \text{num. $\in \ZZ$} \wedge \text{den. $\in\NN_{>0}$} \wedge \text{M.C.D.(num.,den.) = 1}\} \subseteq \ZZ \times \NN\]
\end{proof}

\begin{notexample}
$\RR$ non è numerabile.
\end{notexample}

\begin{proof}
Supponendo, per assurdo, che $\RR = \{a_i | i \in \NN\}$, cerchiamo un $x \in \RR$ che non compare fra gli $a_i$. Allo scopo, costruiamo la sottosuccessione $a_{i_j}$
definita per ricorrenza da:
\[ i_0 = 0 \qquad i_1 = \min\{i | a_i > a_0\} \qquad i_{j+1} = \min\{i | \, \text{$a_i$ è compreso tra $a_{j-1}$ e $a_j$}\}
	\]
graficamente:

\begin{center}
	\begin{figure}[h]
		\centering
		\includegraphics[width=10.5cm]{immagini/RRnum.png}
	\end{figure}
\end{center}

Si vede facilmente (esercizio!) che la successione $\{a_{i_{2k}}\}_k$ è crescente, $\{a_{i_{2k+1}}\}_k$ è decrescente 
e $\displaystyle \lim_{k \to +\infty} a_{i_{2k}} \leq \lim_{k \to +\infty}a_{i_{2k+1}}$. Fissiamo $x$ tale che $\displaystyle \lim_{k \to +\infty} a_{i_{2k}} \leq x \leq \lim_{k \to +\infty} a_{i_{2k+1}}$.
Chiaramente $x$ non è nessuno degli $a_{i_j}$, perché $a_{i_2k} < x < a_{i_{2k+1}}$. Supponiamo $x = a_n$, allora ci sarà $j$ tale che $i_j < n < i_{j+1}$, ma 
questo è assurdo perché allora $x = a_n$ è compreso fra $a_{i_{j-1}}$ e $a_{i_j}$, però $n < i_{j+1}$ contro la minimalità di quest'ultimo.

\begin{exercise}
Completare la dimostrazione nel caso $n < i$.
\end{exercise}

\begin{exercise}
Dimostrare che l'insieme di Cantor $C$ non è numerabile.
\end{exercise}
\end{proof}

\pagebreak
\subsection{Tornando agli insiemi di unicità}

\begin{theorem}
[Cantor-Lebesgue]
\label{CL}
Se $X \subseteq \RR$ è chiuso e numerabile, allora $X$ soddisfa il \hyperref[unicità]{Fatto 1.5}, ed è, quindi, di unicità.
\end{theorem}

La strategia di dimostrazione passa attraverso una definizione.

\begin{definition}
Dato $X \subseteq \RR$, il \vocab{derivato di Cantor-Bendixson} di $X$ è:
\[ X^{\prime} = X \setminus\{\text{punti isolati di $X$}\}
	\]
(dove $a \in X$ è un \vocab{punto di accumulazione} se $\exists \varepsilon > 0 : ]a - \varepsilon, a + \varepsilon[ \cap X = \{a\}$).
\end{definition}

\begin{remark}
Se $X$ è chiuso e per $X^{\prime}$ vale il \hyperref[unicità]{Fatto 1.5}, allora anche per $X$ vale il \hyperref[unicità]{Fatto 1.5}.
\end{remark}

Dimostriamo questo fatto.

\begin{proof}
Occorre dimostrare che se $f$ è continua, lineare, ristretta agli intervalli aperti che non intersecano $X$, e non ha punti angolosi, allora $f$ è
lineare ristretta agli intervalli aperti che non intersecano $X^{\prime}$. Fatto questo, usando l'ipotesi su $X^{\prime}$, $f$ è lineare - abbiamo quindi
mostrato che per $X$ vale \hyperref[unicità]{Fatto 1.5}.\\
Sia $]a,b[ \cap X^{\prime} = \emptyset$, dobbiamo dire che $f_{|]a,b[}$ è lineare. Ci basta dire che per ogni $\varepsilon > 0$, $f_{|[a+\varepsilon, b-\varepsilon]}$ è lineare.
Siccome $]a,b[ \cap X^{\prime} = \emptyset$, $]a,b[ \cap X = \{\text{punti isolati di $X$}\}$. Quindi $[a+\varepsilon, b-\varepsilon] \cap X$ è finito - se così non fosse, avrebbe un punto di accumulazione 
$\alpha$ che non può essere un punto isolato di $X$ (altrimenti si avrebbe un assurdo). Per cui $f_{|[a+\varepsilon, b-\varepsilon]}$ è lineare a tratti, e, siccome non ha punti angolosi, è lineare.
\end{proof}

\begin{corollary}
Sia $X^{(n)} = X^{\prime\prime\ldots\footnote{$n$ volte.}}$. Se $X^{(n)} = \emptyset$ per qualche $n \in \NN$, allora per $X$ vale il \hyperref[unicità]{Fatto 1.5}.
\end{corollary}

\begin{proof}
Induzione su $n$.
\end{proof}

Il guaio è che ci sono chiusi numerabili per cui $X^{(n)} \ne \emptyset$, qualunque sia $n$.

\begin{example}
Vogliamo costruire $X$ chiuso e numerabile tale che $X^{(n)} \ne \emptyset$ per ogni $n \in \NN$. Cominciamo col rivedere alcuni esempi già visti.
\end{example}

\begin{center}
	\begin{figure}[h]
		\centering
		\includegraphics[width=14.5cm]{immagini/es1.png}
	\end{figure}
\end{center}

Tutti i punti sono isolati, $X^{\prime} = \emptyset$.

\pagebreak

\begin{center}
	\begin{figure}[h]
		\centering
		\includegraphics[width=14.5cm]{immagini/es2.png}
	\end{figure}
\end{center}

``Successione con punto limite". Tutti i punti sono isolati salvo $l$, quindi $X^{\prime} = \{l\}$ e $X^{\prime\prime} = \emptyset$.

\begin{center}
	\begin{figure}[h]
		\centering
		\includegraphics[width=14.5cm]{immagini/es3.png}
	\end{figure}
\end{center}

``Successione di successioni", $X^{\prime} = \{a_{10}, a_{20}, \ldots, l\}$, $X^{\prime\prime} = \{l\}$ e $X^{\prime\prime\prime} = \emptyset$.\\
Si vede che possiamo proseguire, in qualche modo, costruendo una successione di successioni di successioni, etc. $n$ volte, $X_n$. Avremo $X_n^{(n)} \ne \emptyset$, $X_n^{(n+1)} = \emptyset$. Ora costruiamo 
$X_{\omega}$ fatto così:

\begin{center}
	\begin{figure}[h]
		\centering
		\includegraphics[width=14.5cm]{immagini/esomega.png}
	\end{figure}
\end{center}

È chiaro che, per ogni $n$, $X_\omega^{(n)} \ne \emptyset$. D'altro canto, $X_\omega$ soddisfa il \hyperref[unicità]{Fatto 1.5}, perché $f$ deve essere lineare in ciascuno degli intervalli
$[a_n,a_{n+1}]$, perché $X_{n+1}$ soddisfa il \hyperref[unicità]{Fatto 1.5}, quindi ci si riduce al caso della successione.

\begin{exercise}
Perché $X_\omega$ è numerabile?
\end{exercise}

Ora potremmo pensare che, pazienza se $X_\omega$ non si smonta a furia di derivati, sarà un caso particolare. Però adesso, possiamo fare una successione di insiemi come $X_\omega$, chiamiamola $X_{\omega+1}$, e 
una successione di questi $X_{\omega+2}$, etc.\\
Al diavolo, serve un nuovo corollario!

\begin{corollary}
Se $X^{(n)}$ è di ``tipo $X_\omega$", allora per $X$ vale il \hyperref[unicità]{Fatto 1.5}.
\end{corollary}

Ok, questo corollario copre $X_\omega$, $X_{\omega + 1}$, $X_{\omega + 2}$, ma copre anche $X_{\omega \cdot 2}$?
\pagebreak
\begin{center}
	\begin{figure}[h]
		\centering
		\includegraphics[width=14.5cm]{immagini/2omega.png}
	\end{figure}
\end{center}

No: occorre un nuovo corollario.

\begin{corollary}
Se $X^{(n)}$ è di ``tipo $X_{\omega \cdot 2}$", allora per $X$ vale il \hyperref[unicità]{Fatto 1.5}.
\end{corollary}

E poi un altro per $X_{\omega \cdot 3}$, e un altro per $X_{\omega \cdot 4}$, etc.\\
E ora abbiamo finito? No, perché possiamo costruire una nuova successione con $X_{\omega},X_{\omega \cdot 2},X_{\omega \cdot 3}$, etc.\\
Se chiamiamo questa follia $X_{\omega \cdot \omega}$, ecco che si riparte a fare successioni di $X_{\omega \cdot \omega}$. Ora si sarà capito che definiremo
una serie aritmetica di queste cose, per cui potremo fare anche $\omega^\omega$, $\omega^{\omega^{\omega}}$, etc. È questa la soluzione allora?\\
No, ogni sforzo di trovare l'induzione a capo delle induzioni è vano. Se ho $X_{\omega}$, $X_{\omega^\omega}$, $X_{\omega^{\omega^{\omega}}}$, etc., allora,
ecco che faccio una successione con queste cose, la battezzo in qualche modo - ad esempio, $X_{\varepsilon_0}$ - e si riparte!\\
Per smontare ogni possibile insieme chiuso e numerabile occorre un \textbf{nuovo tipo di induzione}, l'\vocab{induzione transfinita}, che è strettamente più potente dell'induzione aritmetica.
Questa tecnica è stata sviluppata da Cantor, forse prendendo le mosse dal problema degli insiemi di unicità, e sarà uno degli argomenti centrali del corso.

\begin{exercise}[per la fine del corso]
Dimostrare il teorema di \hyperref[CL]{Cantor-Lebesgue}.
\end{exercise}

\subsection{Giochi di parole}
Descrivere un oggetto matematico non basta per crearlo. Se bastasse, si incorrerebbe in contraddizioni come queste.
\paragraph*{Paradosso di Russell}\mbox{}\\
Tipicamente le collezioni - uso questa parola perché daremo, al termine ``insieme", un senso tecnico preciso - non sono membro di se stesse: la collezione di 
tutti i numeri primi non è un numero primo. Però ci sono anche collezioni che sono membri di se stessi: per esempio la collezione di tutte le collezioni. Consideriamo:
\[ N = \{\text{collezioni $X$}\, | X \not\in X \}
	\]
la collezione delle collezioni che non sono membri di se stessi - la $N$ sta per collezioni normali. Quindi ci chiediamo se $N \in N$ oppure no? $N \in N$ se e solo se per definizione $N \not \in N$, che è assurdo.\\
Il paradosso di Russell ci dice che, del principio di collezione - ossia l'idea che data una proprietà ben definita $P$ si possa costruire la collezione $\{X | P(X)\}$ - non ci si può fidare.

\paragraph*{Paradosso di Berry}\mbox{}\\
L'italiano annovera un numero finito di parole, è quindi possibile formare solo un numero finito di frasi di meno di centro parole. Alcune di queste descrivono un numero naturale, altre no. Comunque, solo un numero 
finito di numeri naturali può essere descritto con meno di cento parole. Per il principio del minimo, esiste:
\begin{align*}
	h = \text{``il più piccolo numero naturale che l'italiano non può} \\ 
 \text{descrivere con meno di cento parole"}
\end{align*}
Il guaio chiaramente, è che lo abbiamo appena descritto con sedici parole.\\
Quindi non ci si può fidare troppo neppure dell'italiano, o meglio, non è possibile descrivere precisamente cosa sia una descrizione precisa.\\
In conclusione, occorre fissare un linguaggio formale in cui si esprimano le proposizioni della teoria degli insiemi, e occorre fissare un sistema di assiomi, espressi in questo linguaggio, che 
dicano quali costruzioni sono lecite: quali insiemi esistono. Il ruolo della teoria degli insiemi è, poi, di fondare l'edificio della matematica. L'ambizione, quindi, è che il linguaggio e gli assiomi della teoria degli insiemi, 
siano in realtà, il linguaggio e gli assiomi della matematica.

\subsection{Scopi del corso}
Questo corso persegue due obiettivi:
\begin{enumerate}[(1)]
	\item Studiare i \textbf{fondamenti della matematica}, nella forma più comunemente accettata nel XX secolo e fino ad ora, la teoria degli insiemi di 
	\href{https://it.wikipedia.org/wiki/Ernst_Zermelo}{\textcolor{purple}{Zermelo}}-\href{https://it.wikipedia.org/wiki/Adolf_Abraham_Halevi_Fraenkel}{\textcolor{purple}{Fraenkel}} con l'assioma della scelta (ZFC).
	\item Studiare tecniche e strumenti che sono stati sviluppati grazie alla teoria degli insiemi, per esempio: la teoria delle cardinalità, la teoria dei numeri ordinali, l'induzione e la ricorsione transfinita.
\end{enumerate}

In questo corso non ci occupiamo dei modelli della teoria degli insiemi. Mi spiego. Per esempio, in teoria dei gruppi si assiomatizza cosa sia un gruppo, e poi si studia come possano essere fatti i diversi gruppi. In 
teoria degli insiemi si assiomatizza l'universo di tutti gli insiemi, però, per il teorema di incompletezza di \href{https://it.wikipedia.org/wiki/Kurt_G%C3%B6del}{\textcolor{purple}{Gödel}}, questa assiomatizzazione non 
può essere completa. Quindi esistono tanti universi insiemistici possibili. Indagare queste possibilità - i modelli della teoria degli insiemi - è argomento di corsi più avanzati.

\newpage
\section{Il linguaggio della teoria degli insiemi}
Per non incorrere in contraddizione, accettiamo che le sole proposizioni ad avere senso siano quelle esprimibili mediante \vocab{formule insiemistiche}. Le formule si costruiscono ricorsivamente.
\begin{itemize}
	\item Le lettere $a,b,c,\ldots,A,B,C,\ldots,\alpha,\beta,\gamma,\ldots$ rappresentano \vocab{variabili}. I valori delle variabili sono sempre insiemi, e non ci sono altri oggetti salvo gli insiemi.
	\item Le \vocab{formule atomiche} sono:
	\[ \text{variabile = variabile} \qquad \qquad \text{variabile $\in$ variabile}\footnote{\,``appartiene a".}
		\]
	sono formule atomiche $x=y$, $x=x$, $\alpha = C$, e anche $x \in y$, $x \in x$, $\alpha \in C$.
	\item Le formule atomiche si combinano tra loro mediante:
	\begin{itemize}
		\item \vocab{connettivi logici} ovvero il ``non'' la ``e'' e la ``o'' (inclusiva):
		\[ \text{$\neg$ formula} \qquad \text{formula $\land$ formula} \qquad \text{formula $\lor$ formula}
			\]
		quindi ad esempio:
		\begin{flalign*}
			&\neg\Phi \equiv \text{``$\Phi$ è falsa''} &\\
			&\Phi \land \psi \equiv \text{``$\Phi$ e $\psi$ sono entrambe vere''} &\\
			&\Phi \lor \psi \equiv \text{``almeno una fra $\Phi$ e $\psi$ è vera''}
		\end{flalign*}
		\item \vocab{quantificatori} ovvero quello universale ``per ogni'' e quello esistenziale ``esiste'':
		\[ \forall x \, \text{formula} \qquad \exists x \, \text{formula}
			\]
		ad esempio:
		\begin{flalign*}
			&\forall x \, \Phi \equiv \text{``$\Phi$ è vera qualunque sia l'insieme $x$''} &\\
			&\exists x \, \Phi \equiv \text{``c'è un insieme $x$ che fa si che $\Phi$ sia vera''}
		\end{flalign*}
		\begin{exercise}
			Chiaramente varranno $\forall x \, x = x,$ $ \forall x \, \exists y \, x = y,$ $ \neg (\exists x \, \forall y \, x = y)$.
		\end{exercise}
	\end{itemize}
\end{itemize}

\textbf{\underline{L'intuizione}} è che l'universo insiemistico sia un gigantesco \href{https://it.wikipedia.org/wiki/Digrafo_aciclico}{\textcolor{purple}{grafo diretto aciclico}} i cui vertici sono gli insiemi,
ed in cui le frecce rappresentano la relazione di appartenenza.

\begin{center}
	\begin{figure}[h]
		\centering
		\includegraphics[width=10.5cm]{immagini/graf.png}
	\end{figure}
\end{center}

Possiamo solo fare affermazioni a proposito di vertici e frecce di questo grafo. Per esempio:
\[ \text{``$a$ è un elemento di un certo $b$''} \equiv \text{``c'è un percorso di due frecce fra $a$ e $b$''} 
	\]
che corrisponde mediante formule insiemistiche a $ \exists x (a \in x \land x \in b)$. E ancora:
\[\text{``$a$ è un sottoinsieme di $b$''} \equiv \text{``ogni elemento di $a$ è elemento di $b$''} \equiv \]\[
		\equiv\text{``non c'è un insieme che è elemento di $a$ e non di $b$''}\equiv\]\[
	 \equiv \text{``non c'è un vertice con una freccia verso $a$ e non una verso $b$''}
	\]
che corrisponde mediante formule insiemistiche a $\neg\exists x (x \in a \land \neg x \in b)$ (tutto ciò che raggiunge $a$ deve raggiungere anche $b$).\\
\textbf{\underline{Parentesi}} Ad essere precisi, avremmo dovuto definire le formule includendo un mucchio di parentesi, allo scopo di eliminare ogni possibilità
di formare una combinazione di simboli ambigua. Per esempio $\textcolor{red}{\Phi_1 \land \Phi_2 \lor \Phi_3}$ è ambigua, perché si potrebbe leggere $(\Phi_1 \land \Phi_2) \lor \Phi_3$
o $\Phi_1 \land (\Phi_2 \lor \Phi_3)$. In una notazione completamente parentesizzata, per esempio, la formula per ``$a$ è un sottoinsieme di $b$'' sarebbe:
\[ \neg(\exists x((x \in a)\land(\neg(x \in b))))
	\]
Non useremo, in generale, questa notazione, ma useremo le parentesi selettivamente per evitare ambiguità. \footnote{Mi riservo in queste dispense di modificare un pochino questa regola, qualora alcune formule risultassero più leggibili con le parentesi.}\\
\textbf{\underline{Abbreviazioni}} Le formule appena descritte costituiscono il linguaggio della teoria degli insiemi \textbf{puro}. Durante il corso estenderemo
più volte questo linguaggio mediante abbreviazioni, che semplicemente rimpiazzano formule più lunghe con scritture convenzionali più compatte, e quindi non alterano 
la potenza espressiva del linguaggio. Vediamo le prime abbreviazioni:
\[ x \ne y \Mydef \neg x = y \footnote{Cioè ``non è vero che $x$ è uguale a $y$''.} \qquad x \not\in y \Mydef \neg x \in y \qquad \not\exists x \,\Phi \Mydef \neg \exists x \, \Phi
	\]\[ \Phi \rightarrow \psi \Mydef \psi \lor \neg \Phi \qquad \Phi \leftrightarrow \psi \Mydef (\Phi \rightarrow \psi) \land (\psi \rightarrow \Phi)
		\]\[ \exists x \in y \; \Phi \Mydef \exists x (x \in y \land \Phi) \qquad \forall x \in A \; \Phi \Mydef \forall x (x \in A \rightarrow \Phi)
			\]\[ \exists !\, x\, \Phi(x) \Mydef \exists x (\Phi(x) \land \forall y(\Phi(y) \rightarrow y = a))
				\]\[ \exists !\, x \in A \,\Phi(x) \Mydef \exists! \, x(x \in A \land \Phi(x))
					\]\[ A \subseteq B \Mydef \forall x (x \in A \rightarrow x \in B) \qquad A \subsetneq B \Mydef( A \subseteq B) \land (A \ne B)
						\]\[ C = A \cup B \Mydef \forall x \, x \in C \leftrightarrow (x \in A \lor x \in B)
							\]\[ C = A \cap B \Mydef \forall x \, x \in C \leftrightarrow (x \in A \land x \in B)
								\]
\begin{note}
	Il fatto che possiamo dire $C = A \cup B$ o $C = A \cap B$ non significa né che questi oggetti esistano né che siano unici. Dimostreremo fra poco l'esistenza e unicità 
	di unione e intersezione.
\end{note}

\begin{exercise}
Esprimi queste proposizioni mediante formule insiemistiche pure:
\begin{itemize}
	\item gli elementi degli elementi di $A$ sono elementi di $A$;
	\item $B$ è l'insieme dei sottoinsiemi di $A$;
	\item l'unione degli elementi di $A$ è l'intersezione di quelli di $B$\footnote{Qui assumi che l'unione e intersezione esistano e siano uniche.}
\end{itemize}
\end{exercise}

\subsection{Le regole di inferenza}
La teoria assiomatica degli insiemi si compone di tre parti: il linguaggio formale che abbiamo appena descritto, gli assiomi della teoria che studieremo durante il corso, 
ed un sistema di regole che specificano precisamente quali passaggi sono leciti nelle dimostrazioni. Possiamo immaginare questa ultima componente come una specie di algebra dei ragionamenti,
che permette di verificare i passaggi di una dimostrazione in maniera puramente meccanica, come se fossero semplici manipolazioni algebrica. Noi non vedremo le regole di inferenza, e voglio spiegare qui il perché.
\begin{enumerate}[1]
	\item Sono argomento del corso di logica.
	\item In realtà, scrivere le dimostrazioni in maniera formale, le renderebbe lunghissime e particolarmente incomprensibili.
	\item In pratica, non si sbaglia facendo ragionamenti che non reggono, si sbaglia dicendo cose fumose che non possono essere espresse nel linguaggio della teoria. Per esempio, le parole ``e così via'' sono pericolose.
	\item Conoscere le regole - fidatevi - non aiuta né a trovare né a capire le dimostrazioni.
\end{enumerate}
Pur senza dare un sistema completo di regole, vediamo qualche manipolazione formale che potrebbe servire.\\
\textbf{\underline{Tavole di verità}} Due combinazioni mediante connettivi logici ($\neg$, $\land$, $\lor$, $\rightarrow$, $\leftrightarrow$)
delle stesse formule - ``\vocab{combinazioni booleane}'' - alle volte, dicono la stessa cosa. Per esempio, $\neg \Phi \lor \neg \psi \equiv\footnote{\,``equivale a''.} \neg (\Phi \land \psi)$.
Per verificare questo fatto basta considerare tutte le possibili combinazioni di valori di verità che possono assumere le formule combinate - nell'esempio $\Phi$ e $\psi$ - compilando una ``\vocab{tabella di verità}''.
\begin{center}
	\begin{tabular}{>{$}l<{$}>{$}l<{$}|*{7}{>{$}l<{$}}}
	\Phi & \psi & \neg\Phi   & \neg\psi   & \neg\Phi \lor \neg\psi   & \Phi \land \psi & \neg(\Phi \land \psi)    \\
	\hline\vrule height 14pt width 0pt
	V & V & F & F & \textcolor{red}{F} & V & \textcolor{red}{F}\\
	V & F & F & V & \textcolor{red}{V} & F & \textcolor{red}{V}\\
	F & V & V & F & \textcolor{red}{V} & F & \textcolor{red}{V}\\
	F & F & V & V & \textcolor{red}{V} & F & \textcolor{red}{V}
	\end{tabular} 
\end{center}
Come si osserva le due colonne corrispondenti ai valori di verità delle nostre formule iniziali hanno gli stessi valori di verità in ogni caso.\\
Conviene tenere a mente alcune delle equivalenze elementari:
\[ \neg\neg \Phi \equiv \Phi \qquad \Phi \land (\psi \lor \Theta) \equiv (\Phi \land \psi) \lor (\Phi \land \Theta) \qquad \Phi \lor (\psi \land \Theta) \equiv (\Phi \lor \psi) \land (\Phi \lor \Theta)
	\]\[ \neg(\Phi \land \psi) \equiv \neg \Phi \lor \neg \psi \qquad \neg(\Phi \lor \psi) = \neg \Phi \land \neg \psi \, \footnote{\href{https://it.wikipedia.org/wiki/Leggi_di_De_Morgan}{\textcolor{purple}{Leggi di De Morgan}}.}
		\]\[ \Phi \rightarrow \neg \psi \equiv \psi \rightarrow \neg \Phi \qquad \Phi \rightarrow \psi \equiv \neg \psi \rightarrow \neg \Phi
			\]

\begin{exercise}
Dimostrare le equivalenze delle formule elencate sopra.
\end{exercise}

Per quanto riguarda i quantificatori ricordiamo le regole seguenti, che tuttavia non sono esaustive.
\[ \neg\forall x \, \Phi \equiv \exists x \, \neg\Phi \qquad \neg\forall x \, \neg \Phi \equiv \exists x \, \Phi
	\]\[ \neg\exists x \, \Phi \equiv \forall x \, \neg \Phi \qquad \neg \exists x \, \neg \Phi \equiv \forall x \, \Phi
		\]

\begin{exercise}
Convinciti della validità delle equivalenze precedenti.
\end{exercise}

\begin{exercise}
Dimostra che:
\[ \neg \forall x \in A \, \Phi \equiv \exists x \in A \, \neg \Phi \qquad \neg \exists x \in A \, \Phi \equiv \forall x \in A \, \Phi
	\]
\end{exercise}

\begin{exercise}
Dimostra che:
\[ \forall x (x \in A \rightarrow x \in B) \equiv \neg \exists x (x \in A \land \neg x \in B)
	\]
\end{exercise}

\begin{exercise}
Secondo te, la seguente formula è vera?
\[ \forall A ((\exists x \, x \in A) \rightarrow \exists x \in A (x \in B \rightarrow \forall y \in A \, y \in B))
	\]
\end{exercise}

Infine vi sono regole per la relazione di uguaglianza, che dicono, in sostanza, che se $x = y$ allora $x$ e $y$ non sono distinguibili, ossia vale $\Phi(x) \leftrightarrow \Phi(y)$ qualunque sia $\Phi$.
Per quanto ci riguarda, \textbf{se $x = y$ allora $x$ e $y$ sono nomi della stessa cosa}.

\newpage
\section{I primi assiomi}
\subsection{Assiomi dell'insieme vuoto e di estensionalità}
\begin{axiom}
[Assioma dell'insieme vuoto]
\label{ax1}
Esiste un insieme vuoto.
\[ \exists x \; \forall y \; y \not\in x
		\]
\end{axiom}

\begin{note}
Questo assioma non sarebbe strettamente necessario, in quanto potremmo ottenere un insieme vuoto anche come sottoprodotto, per esempio, dell'assioma dell'infinito che vedremo in seguito.
Tuttavia è bello poter partire avendo per le mani almeno un insieme.
\end{note}

\begin{axiom}
[Assioma di estensionalità]
\label{ax2}
Un insieme è determinato dalla collezione dei suoi elementi. Due insiemi coincidono se e solo se hanno i medesimi elementi.
\[ \forall a \; \forall b \; a = b \leftrightarrow \forall x (x \in a \leftrightarrow x \in b)
	\]
\end{axiom}

\begin{exercise}
Dimostra che la freccia $a = b \rightarrow \forall x (x \in a \leftrightarrow x \in b)$, in realtà, segue dal fatto che se $a = b$ allora $a$ e $b$ sono indistinguibili\footnote{Nel senso che abbiamo descritto in precedenza, cioè sono nomi della stessa cosa.}.
\end{exercise}

\textbf{\underline{Convenzione}} Le variabili libere (= non quantificate), se non specificato altrimenti, si intendono quantificate universalmente all'inizio della formula. Per cui possiamo scrivere
l'assioma di estensionalità semplicemente nella forma:
\[ a = b \leftrightarrow \forall x (x \in a \leftrightarrow x \in b)
	\]

\begin{proposition}[Unicità dell'insieme vuoto]
C'è un unico insieme vuoto.
\[ \exists ! \, x \; \forall y \; y \not \in x
	\]
\end{proposition}

\begin{proof}
Consideriamo due insiemi vuoti $x_1$ e $x_2$, ossia supponiamo $\forall y \, y \not\in x_1$, e $\forall y \, y \not \in x_2$. Allora:
\[ \forall y (y \in x_1 \leftrightarrow y \in x_2)
	\]
[sono coimplicate logicamente] perché $y \in x_1$ e $y \in x_2$ sono entrambe necessariamente false (quindi la proposizione così com'è scritta è sempre vera). Per \hyperref[ax2]{estensionalità}, la proposizione sopra (sempre vera) è equivalente a $x_1 = x_2$ (che quindi a sua volta sarà sempre vera), e quindi abbiamo la tesi.
\end{proof}

\emph{Dimostrazione formale.} Questo livello di pedanteria non è necessario, ma, per una volta, proviamo a dimostrare in ogni dettaglio la formula $\exists ! x (\forall y (y \not \in x))$. Per definizione di $\exists !$, ciò equivale a:
\[ \exists x_1 ((\forall y \, y \not \in x_1) \land \forall x_2 ((\forall y \, y \not \in x_2) \rightarrow x_2 = x_1))
	\]
Per l'\hyperref[ax1]{assioma del vuoto}, $\exists x_1 \, \forall y \, y \not \in x_1$: fissiamo questo $x_1$. Resta da dimostrare che:
\[ (\forall y \, y \not \in x_1) \land \forall x_2(\forall y \, y \not \in x_2) \rightarrow x_2 = x_1
	\]
Per costruzione, $\forall y \, y \not\in x_1$, è vera (avendo fissato $x_1$), quindi resta:
\[ \forall x_2 (\forall y \, y \not \in x_2) \rightarrow x_2 = x_1
	\]
Ora prendiamo un $x_2$ qualunque, dobbiamo dimostrare:
\[ \forall y (y \not \in x_2) \rightarrow x_2 = x_1
	\]
Si danno due casi: o $\forall y (y \not \in x_2)$ è vera o è falsa. Nel secondo caso, l'implicazione è vera per via della tabella di verità. Nel primo abbiamo sia $\forall y \, y \not \in x_1$, [vera] per
costruzione, sia $\forall y \, y \not \in x_2$, [vera] per ipotesi. Quindi, preso un qualunque $y$, $y \in x_1$ e $y \in x_2$ sono entrambe false. La tabella di verità di $\leftrightarrow$ ci dice quindi che vale $y \in x_1 \leftrightarrow y \in x_2$, e, per 
l'arbitrarietà di $y$:
\[ \forall y (y \in x_1 \leftrightarrow y \in x_2)
	\]
Dall'\hyperref[ax2]{assioma di estensionalità}:
\[ \forall y (y \in x_1 \leftrightarrow y \in x_2) \rightarrow x_1 = x_2
	\]
Abbiamo quindi $x_1 = x_2$, da cui segue la verità dell'implicazione iniziale. $\hfill\square$


Chiaramente, ho voluto scrivere questa dimostrazione delirante per convincervi che NON È UNA BUONA IDEA.

\begin{notation}
L'unicità dell'insieme vuoto ci giustifica ad introdurre delle nuove abbreviazioni:
\[ x = \emptyset \Mydef \forall y \, y \not\in x \qquad \emptyset \in x \Mydef \exists z (z = \emptyset \land z \in x)
	\]
\end{notation}

\subsection{Assioma di separazione}
\begin{axiom}
[Assioma di separazione]
\label{ax3}
Se $A$ è un insieme, e $\psi(x)$ una formula insiemistica qualunque, allora $\{x \in A | \psi (x)\}$\footnote{Stiamo usando già questa notazione, ma la definiremo a breve.} è un insieme.
\[ \forall A \; \exists B \; \forall x \; x \in B \leftrightarrow (x \in A \land \psi (x))
	\]
\end{axiom}

\begin{note}
Tecnicamente l'assioma di separazione è uno \vocab{schema di assiomi}, ossia una regola che, per ogni possibile formula $\psi$, ci permette di scrivere un assioma.
\end{note}

\begin{proposition}
Fissati $A$ e $\psi(x)$, l'insieme $\{x \in A | \psi(x)\}$ è univocamente definito. Ossia:
\[ \forall A \; \exists \textcolor{red}{!} B \; \forall x \; x \in B \leftrightarrow (x \in A \land \psi(x))
	\]
\end{proposition}

\begin{proof}
Come per l'unicità dell'insieme vuoto, supponiamo di avere $B_1$ e $B_2$ tali che:
\[ \forall x \, x \in B_1 \leftrightarrow (x \in A \land \psi(x)) \qquad \forall x \, x \in B_2 \leftrightarrow (x \in A \land \psi(x))
	\]
Allora, $\forall x \, x \in B_1 \leftrightarrow (x \in A \land \psi(x)) \leftrightarrow x \in B_2$, quindi ciò coimplica, per \hyperref[ax2]{estensionalità}, che $B_1 = B_2$.
\end{proof}

\begin{exercise}[Transitività della coimplicazione]
Verificare che se $\psi \leftrightarrow \Phi$ e $\Phi \leftrightarrow \Theta$, allora $\psi \leftrightarrow \Theta$.
\end{exercise}

\begin{notation}
Vista l'unicità, possiamo introdurre una nuova abbreviazione:
\[ B = \{x \in A | \psi(x)\} \Mydef \forall x \, x \in B \leftrightarrow (x \in A \land \psi(x))
	\]
\end{notation}

Osserviamo che l'assioma di separazione è una forma indebolita del principio di collezione\footnote{Quel principio che definisce gli insiemi come tutte le cose che soddisfano una certa formula.}. Rimpiazzando il principio con questo assioma, il Paradosso di Russell diventa una proposizione.

\begin{proposition}[Insieme di tutti gli inisemi]
Non esiste l'insieme di tutti gli insiemi.
\[ \not\exists V \; \forall x \; x \in V
	\]
\end{proposition}

\begin{proof}
Supponiamo, per assurdo, che esista questo $V$. Allora, per \hyperref[ax3]{separazione} con la formula $\psi (x) \equiv x \not \in x$, esiste l'insieme:
\[ N = \{x \in V | x \not\in x\}
	\]
che, per definizione (via separazione), ha la proprietà:
\[ \forall x \, x \in N \leftrightarrow (x \in V \land x \not \in x)
	\]
Per ipotesi assurda, $x \in V$ è sempre vera (stiamo considerando l'insieme di tutti gli insiemi), quindi quanto scritto si riduce a:
\[ \forall x \, x \in N \leftrightarrow x \not\in x
	\]
prendendo ora come insieme $N$: $x = N$, abbiamo $N \in N \leftrightarrow N \not\in N$, assurdo.
\end{proof}

\subsection{Classi e classi proprie}
Sebbene, abbiamo detto che gli unici oggetti della teoria degli insiemi sono gli insiemi, usualmente ci si riferisce alla collezione di tutti gli insiemi 
che soddisfano una certa formula come ad una specie di insieme: una \vocab{classe}. Più precisamente, data una formula $\psi(x)$, se diciamo: ``sia $C$ la classe degli insiemi $x$ tali che $\psi(x)$''
intendiamo dire che useremo la scrittura $x \in C$ come una semplice abbreviazione per la formula $\psi(x)$.\footnote{Ovvero per tutti gli oggetti (solo gli insiemi in questo caso) che soddisfano una tale formula $\psi(x)$.} \\
Non avrebbe senso scrivere \textcolor{red}{$C \in$ qualcosa}, perché il simbolo $\in$ in $x \in C$ non ha senso (ha senso solo tra oggetti di tipo insieme), se non nel tutt'uno $\in C$. In altri termini, se scriviamo $x \in C$ in luogo di $\psi(x)$ è solo come ausilio dell'intuizione (per comodità insomma, senza intendere qualcosa di formale all'interno della teoria degli insiemi):
avremmo potuto decidere di scrivere $x$\ding{168}, o nient'altro che $\psi(x)$.

\begin{definition}[Classe universale]
La classe $V$ si dice \vocab{classe universale} ed è la classe di tutti gli insiemi.
\[ x \in V \Mydef x = x \footnote{Cioè la classe degli insiemi che soddisfano il predicato $\psi(x): x = x$ (ovvero tutti gli insiemi per quanto assunto all'inizio della teoria), $V = \{x | \psi(x)\} = \{x | x = x\}$ (dove naturalmente non sto usando separazione ma il principio di collezione perché stiamo definendo una classe).}
	\]
\end{definition}

Insomma, scrivere $x \in V$ non dice molto: è una formula sempre vera.

\begin{notation}[Uguaglianza tra classi]
Date due classi $C$ e $D$, che, ricordiamo, non significa altro che ``date due formule$\ldots$'', definiamo l'abbreviazione:
\[ C = D \Mydef \forall x ((x \in C) \leftrightarrow (x \in D)) \footnote{Non è altro che un abbreviazione per dire che le formule che definiscono le classi $C$ e $D$ sono soddisfatte dagli stessi insiemi $x$.}
	\]
\end{notation}

Ora, dato un qualunque insieme $A$, possiamo definire la classe $\hat{A}$ degli $x$ tali che $x \in A$ (cioè la classe degli $x$ che soddisfano $\psi(x) : x \in A$). Se $\hat{A} = \hat{B}$, per l'abbreviazione data non stiamo dicendo altro che:
\[ \forall x ((x \in A) \leftrightarrow (x \in B))
	\]
che equivale $A = B$ per \hyperref[ax2]{estensionalità}. Ha quindi senso, con un leggero abuso di notazione, omettere il cappelletto $\hat{}$ e ``identificare'' la classe $\hat{A}$ semplicemente con $A$. In questo senso,
abbiamo classi che sono insiemi - formalmente $C$ è un insieme se $C = \hat{A}$ per qualche insieme $A$ - e classi che non sono insiemi. Chiamiamo \vocab{classe propria} una classe che non è un insieme.\footnote{Essere un insieme per una classe significa quindi moralmente identificarvisi nel senso riportato sopra, se ciò non fosse possibile parliamo di classi proprie.}

\begin{example}
$V$ è una classe propria.
\end{example}

\textbf{\underline{L'intuizione}}, che sarà più chiara via via che procediamo nel corso, è che le classi proprie sono troppo grandi per essere insiemi.

\subsection{Assioma del paio e coppia di Kuratowski}
I primi tre assiomi ci dicono, a grandi linee, che, entro i limiti di quanto si può fare rinunciando al principio di collezione - che esiste $\{x | \, \text{una qualunque proprietà}\}$ -, gli insiemi sono delle specie di collezioni.
Sono determinati dai loro elementi, e li si può dividere in collezioni più piccole in maniera arbitraria. \\ Ci troviamo, però, adesso, nella necessità di procurarci qualche insieme con cui lavorare. I prossimi assiomi serviranno per giustificare le costruzioni con cui,
usualmente, si definiscono nuovi insiemi. Per esempio, abbiamo bisogno di costruire certi insiemi di base, tipo l'insieme dei numeri interi o insiemi finiti i cui elementi sono elencati esplicitamente, fare prodotti di insiemi esistenti, 
considerare le funzioni fra insiemi esistenti, etc.

\begin{axiom}
[Assioma del paio]
\label{ax4}
Dati $a$ e $b$ esiste l'insieme $\{a,b\}$.
\[ \forall a \; \forall b \; \exists P \; \forall x \; x \in P \leftrightarrow (x = a \lor x = b)
	\]
\end{axiom}

\begin{proposition}
[Unicità del paio]
Fissati $a$ e $b$, l'insieme $\{a,b\}$ è univocamente determinato.
\[\forall a \; \forall b \; \exists\textcolor{red}{!} P \; \forall x \; x \in P \leftrightarrow (x = a \lor x = b)
	\]
\end{proposition}

\begin{exercise}
	Dimostra la proposizione precedente.
\end{exercise}

\begin{soln}
	Supponiamo che esistano $P_1$ e $P_2$ tali che:
	\[ \forall x (x \in P_1 \leftrightarrow (x = a \lor x = b)) \qquad \text e \qquad \forall x (x \in P_2 \leftrightarrow (x = a \lor x = b))
		\]
	da ciò segue che:
	\[ \forall x (x \in P_1 \leftrightarrow x \in P_2)
		\]
	dunque per \hyperref[ax2]{estensionalità} l'espressione sopra equivale a $P_1 = P_2$.
\end{soln}

\begin{proposition}[Esistenza dei singoletti]
	Dato $a$, esiste ed è unico $\{a\}$.
	\[ \forall a \; \exists ! S \; \forall x \; x \in S \leftrightarrow x = a
		\]
\end{proposition}

\begin{proof}
	Ponendo $b = a$ nella proposizione precedente, si ha che:
	\[ \forall a \; \exists ! S \; \forall x \; x \in S \leftrightarrow (x = a \lor x= a)
		\]
	ora $x = a \lor x = a$ equivale a $x = a$\footnote{Stiamo dicendo che in generale $\{a,a\} = \{a\}$ poiché $a \lor a = a$ (in base alle regole dei connettivi logici).}.
\end{proof}

\begin{notation}[Paio (o coppia) e singoletto]
	Possiamo ora introdurre delle abbreviazioni per il paio (o coppia) ed i singoletti:
	\[ P = \{a,b\} \Mydef \forall x \, x \in P \leftrightarrow (x = a \lor x = b)
		\]\[ S = \{a\} \Mydef \forall x \, x \in S \leftrightarrow x = a
			\]
\end{notation}

\begin{remark}
	Osserviamo che $\{a,b\} = \{b,a\}$.
\end{remark}

\begin{proof}
	Segue dal fatto che $\lor$ è commutativo:
	\[ x \in \{a,b\} \leftrightarrow (x = a \lor x = b) \leftrightarrow (x = b \lor x = a) \leftrightarrow x \in \{b,a\}
		\]
	quindi per \hyperref[ax2]{estensionalità} $\{a,b\} = \{b,a\}$.
\end{proof}

Il paio $\{a,b\}$ è, quindi, una coppia non ordinata. È possibile codificare le coppie ordinate con il seguente trucco.

\begin{definition}
	[Coppia di \href{https://it.wikipedia.org/wiki/Kazimierz_Kuratowski}{\textcolor{purple}{Kuratowski}}]
	Definiamo la \vocab{coppia di Kuratowski}:
	\[(a,b) \Mydef \{a,\{a,b\}\}
		\]
\end{definition}

\begin{proposition}[Proprietà di coppia ordinata]
	La coppia di Kuratowski $(a,b)$ rappresenta la coppia ordinata di $a$ e $b$, ossia vale che:
	\[ (a,b) = (a^{\prime},b^{\prime}) \leftrightarrow (a = a^{\prime} \land b = b^{\prime})
		\]
\end{proposition}

\begin{proof}
	Detto $c = (a,b)$, vogliamo determinare univocamente $a$ e $b$. Osserviamo che $a$ è determinata da:
	\[ x = a \leftrightarrow \forall y \in c (x \in y) \, \footnote{Sostanzialmente stiamo dicendo che preso un elemento $x$, $x = a$ se e solo se, preso un elemento di $(a,b) = \{\{a\},\{a,b\}\}$, $x$ appartiene sempre a tale elemento (dovendo appartenere sia ad $\{a\}$ che ad $\{a,b\}$ sarà per forza $a$).}
		\]
	la freccia $\rightarrow$ segue da come è definita la coppia $(a,b)$, mentre $\leftarrow$ segue dal fatto che, sempre per definizione di coppia di Kuratowski, $\{a\} \in c = (a,b)$, per cui:
	\[ \forall y \in c (x \in y) \overset{\text{ipotesi}}{\implies} x \in \{a\} \overset{\text{singoletto}}{\implies} x = a
		\]
	Determiniamo ora $b$, studiamo prima il caso in cui $\exists ! x (x \in c)$\footnote{Cioè sto dicendo la coppia è in realtà un insieme fatto da un solo insieme.}:
	\[ \begin{split}
		\exists ! x (x \in c) &\iff \{a\} = \{a,b\} \\
							&\iff b = a
	\end{split}
		\]
	ovvero se e solo se i due insiemi che formano $c = (a,b)$ sono il singoletto $\{a\}$ (per \hyperref[ax2]{estensionalità}). In questo caso $b$ è determinato, se non fosse così allora $\{a,b\}$ (che corrisponde a $b$ nella coppia ordinata) sarebbe univocamente determinato da:
	\[ x = \{a,b\} \leftrightarrow (x \in c \land x \ne \{a\})
		\]
	in tal modo abbiamo che:
	\[ x = b \leftrightarrow (x \in \{a,b\} \land x \ne a)
		\]
	Possiamo quindi ricavare la tesi come segue:
	\[ \begin{split}
		(a = a' \land b = b') & \leftrightarrow (\forall y \in c (a' \in y)) \land (b' \in \{a,b\} \land b' \ne a) \\
							  & \leftrightarrow \{a\} = \{a'\} \land \{a,b\} = \{a,b'\} \\
							  & \leftrightarrow (a,b) = (a',b')
	\end{split}
		\]
	(dove nel secondo passaggio abbiamo usato \hyperref[ax2]{estensionalità} per giustificare le uguaglianze).
\end{proof}

\begin{definition}[$n$-upla ordinata]
	Possiamo estendere la definizione di coppia ordinata con il seguente trucco:
	\[ \begin{split}
	   (a,b,c) &\Mydef ((a,b),c) \\
	   (a,b,c,d) &\Mydef (((a,b),c),d) \\
	   (a_1,a_2,\ldots,a_n) &\Mydef ((a_1,a_2,\ldots,a_{n-1}),a_n)
	\end{split}
	\]
\end{definition}

\begin{note}
	Quest'ultima definizione è, in realtà, uno schema di definizioni: una per ogni $n$. Per ora, \textcolor{red}{NON} siamo in grado di scrivere, per esempio,
	una formula insiemistica che dica ``Esiste un $n$ ed una $n$-upla $(a_1,\ldots,a_n)$ tale che…''. Però, per ogni $n$ dato, chessò 92, possiamo scrivere esplicitamente una formula che dice $x = (a_1,a_2,a_3,\ldots,a_{92})$.
\end{note}

\begin{proposition}[Proprietà di $n$-upla ordinata]
	Si ha che:
	\[ (a,b,c) = (a',b',c') \leftrightarrow a = a' \land b = b' \land c = c'
		\]\[ (a_1,\ldots,a_n) = (a_1',\ldots,a_n') \leftrightarrow a_1 = a_1' \land \ldots \land a_n = a_n'
			\]
\end{proposition}

\begin{exercise}
	Dimostra la prima e convinciti che, dato un qualunque $n$ esplicito, potresti dimostrare la seconda.
\end{exercise}

\subsection{Assioma dell'unione e operazioni booleane}

\begin{axiom}[Assioma dell'unione]
	\label{ax5}
	Dato un insieme $A$ esiste un insieme $B$ i cui elementi sono gli elementi degli elementi di $A$. Ovvero, dato un insieme $A$ esiste l'unione degli elementi di $A$.
	\[ \forall A \; \exists B \; \forall x \; x \in B \leftrightarrow \exists y \in A \; x \in y\footnote{Cioè $x$ è un elemento di $B$ se e solo se è un elemento di un elemento di $A$.}
		\]
\end{axiom}

\begin{proposition}
	[Unicità dell'unione]
	Vale l'unicità dell'unione:
	\[ \forall A \; \exists \textcolor{red}{!} B \; \forall x \; x \in B \leftrightarrow \exists y \in A \; x \in y
		\]
\end{proposition}

\begin{proof}
	Supponiamo di avere $B_1$ e $B_2$ tali che:
	\[ \forall x \, x \in B_1 \leftrightarrow \exists y \in A \, x \in y
		\]\[ \forall x \, x \in B_2 \leftrightarrow \exists y \in A \, x \in y
			\]
	quindi $\forall x (x \in B_1 \leftrightarrow x \in B_2)$, e per \hyperref[ax2]{estensionalità} $B_1 = B_2$.
\end{proof}

\begin{notation}[Unione di un insieme]
	Possiamo introdurre l'abbreviazione:
	\[ B = \bigcup A\footnote{\,``Unione di $A$''.} \Mydef \forall x \,( x \in B \leftrightarrow \exists y (x \in y))
		\]
\end{notation}

\begin{exercise}
	Dimostra che l'assioma dell'unione segue che:
	\[ \forall A \; \exists B \; (\forall y \in A \; \forall x \in y \; x \in B)\footnote{Cioè per ogni insieme esiste l'insieme di tutti gli elementi degli elementi di $A$.}
		\]
\end{exercise}

Combinando l'assioma dell'unione e del paio possiamo definire $a \cup b$.

\begin{definition}[Unione di insiemi]
	Poniamo:
	\[ a \cup b \Mydef \bigcup\{a,b\}
		\]
\end{definition}

\begin{proposition}[Caratterizzazione unione di insiemi]
	Dati $a,b$ e $a \cup b$ vale che:
	\[ x \in a \cup b \leftrightarrow (x \in a \lor x \in b)
		\]
\end{proposition}

\begin{proof}
	Dire che $x$ è un elemento di $a \cup b$ equivale a dire che $x$ è un elemento di un elemento di $\{a,b\}$, ossia
	che $x$ è un elemento di uno tra $a$ e $b$ ($x \in a \lor x \in b$).
\end{proof}

Ora definiamo le intersezioni: \emph{riesci a vedere perché, a differenza delle unioni, non servirà un nuovo assioma?}

\begin{definition}[Intersezione di un insieme]
	Sia $C$ una \textcolor{red}{classe}\footnote{Quindi, in particolare, $C$ può essere un insieme (in questo caso la definizione è comunque lecita in generale con le classi, i cui elementi sono  appunto insiemi).} non vuota.
	L'\textcolor{red}{insieme} $B$ è l'\vocab{intersezione} di $C$ se:
	\[ B = \bigcap C \Mydef \forall x (x \in B \leftrightarrow \forall y \in C (x \in y))
		\]
	cioè $x$ sta in $b$ se è elemento di ogni elemento di $C$.
\end{definition}

\begin{proposition}[Esistenza e unicità dell'intersezione]
	Data una classe non vuota $C$, l'intersezione $\bigcap C$ esiste ed è unica. In particolare, nel caso dell'intersezione di un insieme vale:
	\[ \forall A (A \ne \emptyset \rightarrow \exists ! B \; \forall x(x \in B \leftrightarrow \forall y \in A (x \in y)))
		\]
\end{proposition}

\begin{note}
	L'ipotesi $C \ne \emptyset$ è necessaria perché altrimenti si avrebbe che $\bigcap \emptyset$ è la classe universale $V$ ($x \in \bigcap \emptyset \leftrightarrow \forall y \in \emptyset(x \in y)$ (dove il RHS è sempre falso per costruzione, quindi gli $x$ che soddisfano l'enunciato sono tutti)), che non è un insieme.
\end{note}

\begin{proof}
	L'unicità segue per \hyperref[ax2]{estensionalità} al solito modo. Veniamo all'esistenza. Dal momento che $C$ non è vuota [per ipotesi], possiamo prendere $z \in C$. 
	Ora consideriamo (un sottoinsieme di $B$ ottenuto per \hyperref[ax3]{separazione} nel modo seguente):
	\[ B = \{x \in z | \forall y \in C (x \in y)\}
		\]
	ovvero il sottoinsieme di $z$ di tutti gli elementi che appartengono a tutti gli elementi di $C$.
	Chiaramente (per definizione) $x \in B \rightarrow \forall y \in C (x \in y)$, d'altro canto, $\forall y \in C (x \in y)$ implica, in particolare (un tale $x$ appartiene a tutti gli elementi della classe e quindi anche a $z$), $x \in z$, quindi in automatico $x \in B$.\\
	Abbiamo così verificato che $x \in B \leftrightarrow \forall y \in C (x \in y)$, ossia $B = \bigcap C$ (moralmente abbiamo costruito l'intersezione di un insieme per separazione su un elemento della classe $C$ (o insieme se lo è), come il sottoinsieme di tutti gli elementi che stanno in tutti gli elementi della classe). L'ultimo ragionamento può essere pensato anche nel seguente modo:
	\[ \begin{split}
		\forall x \, x \in B & \leftrightarrow (x \in z \land (\forall y \in C(x \in C)))\\
							 & \overset{\text{def.}}{\leftrightarrow} (x \in z) \land x \in \bigcap C \\
							 & \leftrightarrow x \in \bigcap C
	\end{split}
		\]
	dove l'ultima equivalenza è giustificata dal fatto che se $x$ sta in tutti gli elementi degli elementi di $C$ allora $x$ sta in particolare anche in $z$ e quindi il primo termine dell'$\land$ può essere rimosso.
\end{proof}

\begin{notation}[Intersezione e differenza di insiemi]
	Poniamo:
	\[ a \cap b \Mydef \bigcap\{a,b\} \qquad \text e \qquad a\setminus b \Mydef \{x \in a | x \not\in b\}
		\]
\end{notation}

\begin{proposition}[Caratterizzazione intersezione e differenza di insiemi]
	Vale che:
	\[ x \in a \cap b \leftrightarrow (x \in a \land x \in b)
		\]\[ x \in a \setminus b \leftrightarrow (x \in a \land x \not\in b)
			\]
\end{proposition}

\begin{exercise}
	Dimostrare la proposizione precedente (la seconda è semplicemente la definizione).
\end{exercise}

\begin{proposition}[Proprietà di unione, intersezione e differenza di insiemi]
	Alcune proprietà delle operazioni $\cup$, $\cap$, $\setminus$:
	\[ \begin{split}
		\text{\textcolor{red}{commutatività:}} \qquad & a \cup b = b \cup a \qquad \text e \qquad a \cap b = b \cap a \\
		\text{\textcolor{red}{associatività:}} \qquad & a \cup (b \cup c) = (a \cup b) \cup c \Mydef a \cup b \cup c \\
	                                        	      & a \cap (b \cap c) = (a \cap b) \cap c \Mydef a \cap b \cap c \\
		\text{\textcolor{red}{distributività:}} \qquad & a \cup (b \cap c) = (a \cup b) \cap (a \cup c) \\
													   & a \cap (b \cup c) = (a \cap b) \cup (a \cap c) \\
	    \text{\textcolor{red}{leggi di \href{https://it.wikipedia.org/wiki/Augustus_De_Morgan}{\textcolor{red}{De Morgan}}:}} \qquad & a \setminus (b \cup c) = (a \setminus b) \cap (a \setminus c) \\
																													& a \setminus (b \cap c) = (a \setminus b) \cup (a \setminus c)
	   \end{split}
	\]
\end{proposition}

\begin{proof}
	Tutte queste proprietà su deducono immediatamente dalle corrispondenti proprietà dei connettivi logici, le quali, a loro volta, si vedono con le tabelle di verità. Per esempio, dimostriamo 
	la prima delle leggi di De Morgan (facendo uso della corrispondente legge per i connettivi logici):
	\[ \begin{split}
		x \in a \setminus (b \cup c) \iff & x \in a \land x \not\in (b \cup c)\\
		\iff & x \in a \land \neg(x \in b \lor x \in c)\\
		\overset{\text{De Morgan}}{\iff} & x \in a \land x \not\in b \land x \not\in c\\
		\iff & x \in a \land x \not\in b \land \underbrace{x \in a}_{\text {non cambia nulla}} \land x \not\in c\\
		\iff & x \in (a \setminus b) \land x \in (a \setminus c)\\
		\iff & x \in (a \setminus b) \cap (a \setminus c)
	\end{split}
		\]
\end{proof}

Ora possiamo costruire insiemi finiti elencandone gli elementi, come si fa di solito, con la notazione $\{\ldots\}$\footnote{Paradossalmente prima di aggiungere l'assioma dell'unione
alla teoria potevamo costruire $n$-uple ordinate di lunghezza arbitraria, ma non un insieme con più di due elementi.}.

\begin{notation}[Insiemi di $n$ elementi]
	Possiamo ora introdurre un'abbreviazione per indicare insiemi con più di due elementi (costruiti usando l'\hyperref[ax5]{assioma dell'unione}):
	\[ \begin{split}
	   \{a,b,c\} &\Mydef \{a\} \cup \{b\} \cup \{c\} \\
	   \{a,b,c,d\} &\Mydef \{a\} \cup \{b\} \cup \{c\} \cup \{d\} \\
	   \{a_1,\ldots,a_n\} &\Mydef \{a_1\} \cup \ldots \cup \{a_n\}
	\end{split}
	\]
\end{notation}

\begin{proposition}[Caratterizzazione di insieme con $n$ elementi]
	Vale che:
	\[  \begin{split}
		x \in \{a,b,c\} & \leftrightarrow (x = a \lor x = b \lor x = c) \\
	    x \in \{a_1,\ldots,a_n\} & \leftrightarrow (x = a_1 \lor \ldots \lor x = a_n) 
		\end{split}
			\]
\end{proposition}

\begin{exercise}
	Dimostrare la proposizione precedente.
\end{exercise}

\subsection{Assioma delle parti e prodotto cartesiano}
Abbiamo definito le coppie $(x,y)$, però, per esempio, ancora nulla ci assicura che dati $A$ e $B$ esista:
\[ A \times B = \{(x,y) | x \in A \land y \in B\}
	\]
Le funzioni $A \rightarrow B$ saranno poi sottoinsiemi di $A \times B$, e vorremo parlare dell'insieme ${}^{A}B$
delle funzioni $A \rightarrow B$. Per tutto questo ci manca un solo ingrediente: l'insieme delle parti.

\begin{axiom}
	[Assioma delle parti]
	\label{ax6}
	Dato un insieme $A$ esiste l'insieme $\ps(A)$ i cui elementi sono i sottoinsiemi di $A$.
	\[ \forall A \; \exists B \; \forall x \; x \in B \leftrightarrow x \subseteq A 
		\]
\end{axiom}

\begin{proposition}
	[Unicità delle parti]
	Vale che:
	\[\forall A \; \exists ! B \; \forall x \; x \in B \leftrightarrow x \subseteq A 
		\]
\end{proposition}

\begin{proof}
	Segue come sempre per \hyperref[ax2]{estensionalità}, in quanto, se avessimo $B_1$, $B_2$, allora:
	\[ \forall x (x \in B_1 \leftrightarrow x \subseteq A) \qquad \text e \qquad \forall x(x \in B_2 \leftrightarrow x \subseteq A)
		\]
	quindi $\forall x((x \in B_1) \leftrightarrow (x \subseteq A) \leftrightarrow (x \in B_2)) \leftrightarrow \forall x (x \in B_1 \leftrightarrow x \in B_2) \leftrightarrow B_1 = B_2$.
\end{proof}

\begin{notation}[Insieme delle parti (o insieme potenza)]
	Data l'unicità possiamo porre:
	\[ B = \ps(A) \Mydef \forall x \; x \in B \leftrightarrow x \subseteq A
		\]
\end{notation}

\begin{proposition}[Esistenza ed unicità del prodotto cartesiano]
	Dati $A$ e $B$  esiste un unico insieme $A \times B$ tale che:
	\[ \forall z (z \in A \times B) \leftrightarrow \exists x \in A \, \exists y \in B \, z = (x,y)\footnote{Ossia, informalmente, $ z \in A \times B$ se e solo se si può scrivere come coppia ordinata di un elemento di $A$ ed uno di $B$.}
		\]
\end{proposition}

\begin{proof}
	L'unicità è conseguenza immediata della definizione e dell'\hyperref[ax2]{assioma di estensionalità} (stessa dimostrazione di sempre). Per l'esistenza, definiamo per \hyperref[ax3]{separazione}:
	\[ A \times B \Mydef \{z \in \ps(\ps(A \cup B)) | \exists x \in A \ \exists y \in B \ z = (x,y)\}
		\]
	così come scritto, siamo sicuri che è un insieme che contiene coppie ordinate di elementi di $A$ e $B$, tuttavia dobbiamo dimostrare anche che ogni coppia $(x,y)$ con $x \in A$ e $y \in B$ appartiene a questo insieme. Per fare ciò bisogna dimostrare che tutte queste coppie
	appartengono a $\ps(\ps(A \cup B))$:\footnote{Poniamo $a,b,\ldots \in z \Mydef a \in z \land b \in z \land \ldots$ e $a,b,\ldots \subseteq z \Mydef a \subseteq z \land b \subseteq z \land \ldots$}\,\footnote{Tutte le implicazioni si basano sul fatto che se un oggetto è sottoinsieme di un qualche insieme allora è un elemento del corrispondente insieme delle parti per definizione.}
	\[\begin{split}
		a \in A \land b \in B &\implies \{a\},\{a,b\} \subseteq A \cup B \\
		& \implies \{a\},\{a,b\} \in \ps(A \cup B) \\
		& \overset{\text{\hyperref[ax4]{paio}}}{\implies} (a,b) = \{\{a\},\{a,b\}\} \subseteq \ps(A \cup B)\\
		& \implies (a,b) \in \ps(\ps(A \cup B))
	\end{split}
		\]
	pertanto tutte le coppie ordinate di elementi di $A$ e $B$ appartengono a $\ps(\ps(A \cup B))$ e per separazione possiamo costruire il prodotto cartesiano $A \times B$ come l'insieme di \underline{tutte} le coppie ordinate.
\end{proof}

\begin{note}
	Avremmo potuto costruire $A \times B$ usando, anziché l'assioma delle parti, l'assioma del rimpiazzamento, che vedremo più avanti.
\end{note}

\subsection{Relazioni di equivalenza e di ordine, funzioni}
Ora rivedremo alcuni concetti ben noti dai primi corsi del primo anno (\emph{o dalla scuola superiore?}). Lo facciamo molto rapidamente, essenzialmente per completezza, e per fissare le notazioni.

\begin{definition}[Relazione binaria]
	Si dice \vocab{relazione binaria} fra $A$ e $B$ un sottoinsieme di $A \times B$.
\end{definition}

\begin{notation}[Relazione binaria]
	Data una relazione $\rel \subseteq A \times B$, definiamo l'abbreviazione:
	\[ a \rel b \Mydef (a,b) \in \rel
		\]
\end{notation}

\begin{example}
	Per esempio scriviamo $a < b$ per indicare che $(a,b) \in\, <$.
\end{example}

Considerando il caso di $A \times A$ possiamo definire le seguenti relazioni.

\begin{definition}
	Una relazione $\sim \,\subseteq A \times A$ è una \vocab{relazione di equivalenza} se è:
	\begin{enumerate}[(i)]
		\item \textbf{\underline{riflessiva}}: $\forall x \in A \; x \sim x$.
		\item \textbf{\underline{simmetrica}}: $\forall x,y \in A\footnote{$\forall x_1,\ldots,x_n \Mydef \forall x_1 \ldots \forall x_n$, e lo stesso con $\exists$ e con i quantificatori limitati.} x \sim y \leftrightarrow y \sim x$.
		\item \textbf{\underline{transitiva}}: $\forall x,y,z \in A \; (x \sim y \land y \sim z) \rightarrow x \sim z$.
	\end{enumerate}
\end{definition}

\begin{definition}
	$\leq \, \in A \times A$ è una \vocab{relazione di ordine (largo)} se è:
	\begin{enumerate}[(i)]
		\item \textbf{\underline{riflessiva}}: $\forall x \in A \; x \leq x$.
		\item \textbf{\underline{antisimmetrica}}: $\forall x,y \in A \; (x \leq y \land y \leq x) \rightarrow x = y$.
		\item \textbf{\underline{transitiva}}: $\forall x,y,z \in A \; (x \leq y \land y \leq z) \rightarrow x \leq z$.
	\end{enumerate}
\end{definition}

\begin{definition}
	$< \, \in A \times A$ è una \vocab{relazione di ordine stretto} se è:
	\begin{enumerate}[(i)]
		\item \textbf{\underline{irriflessiva}}: $\forall x \in A \; \neg(x < x)$.
		\item \textbf{\underline{transitiva}}: $\forall x,y,z \in A \; (x < y \land y < z) \rightarrow x < z$.
	\end{enumerate}
\end{definition}

\begin{exercise}
	Dimostra che una relazione di ordine stretto $<$ su $A$ è automaticamente asimmetrica:
	\[ \forall x,y \in A \; x < y \rightarrow \neg (y < x)
		\]
\end{exercise}

\begin{soln}
Se valesse che $\forall x,y \in A \; x < y \rightarrow y < x$, allora sarebbero contemporaneamente vere $x < y$ e $y < x$, da cui, per transitività si avrebbe $x < x$ che è falso.
\end{soln}

\begin{proposition}[Corrispondenza tra ordini stretti e larghi]
	Data una relazione di ordine stretto $<$ su $A$, la relazione:
	\[ \leq\, = \{(x,y) \in A \times A | x < y \lor x = y\}\footnote{Formalmente: $\{z \in A \times A | \exists x,y \in A \; z = (x,y) \land \ldots\}$.}
		\]
	è una relazione di ordine largo. Viceversa, se $\leq$ è una relazione di ordine largo, la seguente relazione è dei ordine stretto:
	\[ <\, = \{(x,y) \in A \times A | x \leq y \land x \ne y\}\footnote{Come la nota sopra.}
		\]
	Inoltre, in questo modo, le relazioni di ordine stretto e di ordine largo sono poste in corrispondenza una - a - uno.
\end{proposition}

\begin{proof}
	Definiamo la \vocab{diagonale di una relazione} di $A \times A$ come:
	\[ \Delta_A \Mydef \{(x,y) \in A \times A | x = y\}
		\]
	Allora è facile verificare che, se $<$ è una relazione di ordine stretto, allora $< \cap \,\Delta_A = \emptyset$ e $< \cup \,\Delta_A$ è una relazione di ordine largo corrispondente.
	Viceversa, se $\leq$ è una relazione di ordine largo, allora $\Delta_A \subseteq \, \leq$ e $\leq \setminus \Delta_A$ è la relazione di ordine stretto corrispondente.
\end{proof}

\begin{notation}[Relazioni d'ordine strette e larghe]
	Fissata una relazione di ordine largo $\textcolor{red}{\leq}$ su $A$, ci sentiremo liberi di usare la corrispondente relazione di ordine stretto $\textcolor{red}{<}$ fintanto che la scelta del simbolo sia indizio sufficiente dell'operazione.
	Inoltre scriveremo $x > y$ per $y < x$ e $x \geq y$ per $y \leq x$.
\end{notation}

\begin{definition}[Relazione di ordine totale]
	Una \vocab{relazione di ordine totale} su $A$ è una relazione di ordine $\leq$ tale che:
	\[ \forall x,y \in A \, (x \leq y) \lor (x = y) \lor (y \leq x)
		\]
\end{definition}

\begin{exercise}
	Formula la definizione precedente per ordini stretti.
\end{exercise}

\begin{soln}
Diciamo che $<$ è un ordinamento totale (stretto) su $A$ se:
\[ \forall x \in A \, \forall y \in A (x \ne y \land ((x < y) \lor (x > y))) \lor (x = y)
	\]
o anche semplicemente:
\[ \forall x \in A \, \forall y \in A (x = y) \lor (x < y) \lor (x > y)
	\]
E per quanto detto possiamo anche pensare che:
\[ \text{$\leq$ ordine totale} \iff \text{$< \cup \Delta_A$ ordine totale}
	\]
(infatti nella prima definizione non è strettamente necessario che compaia l'uguaglianza, la si può ottenere quanto entrambe le disuguaglianze sono vere per antisimmetria, mentre per ordini stretti è necessario aggiungere la diagonale nella definizione di totalità).
\end{soln}

\begin{definition}[Restrizione di una relazione]
	Data una relazione $\rel \subseteq A \times B$, e dati $A' \subseteq A$, $B' \subseteq B$, possiamo definire la \vocab{restrizione} di $\rel$ a $A'\times B'$:
	\[ \rel_{|A' \times B'} \Mydef \rel \cap (A' \times B')
		\]
	``restrizione di $\mathcal R$ a $A' \times B'$\,''.
\end{definition}

\begin{exercise}
	Data $\rel$ relazione di equivalenza/ordine su $A$ e $A' \subseteq A$, dimostra che $\rel_{|A' \times A'}$ è una relazione di equivalenza/ordine su $A'$.
\end{exercise}

\begin{soln}
Vediamolo per le relazioni di equivalenza. È facile osservare che $\forall a' \in A'$, vale che $(a',a') \in \mathcal{R}_{|A' \times A'}$ (sta in $A' \times A'$ per definizione di prodotto cartesiano e sta in $\mathcal{R}$ essendo una relazione di equivalenza per ipotesi (vale il per ogni)),
analogamente valgono simmetria e riflessività. 
\end{soln}

\begin{definition}[Dominio e immagine di una relazione]
	Data una relazione $\rel \subseteq A \times B$, definiamo:
	\[  \begin{split}
		\Dom(\rel) &\Mydef \{x \in A | \exists y \in B \; x\rel y\} \qquad \text{\vocab{dominio} di $\rel$} \\
	    \Imm(\rel) &\Mydef \{y \in B | \exists x \in A \; x \rel y \} \qquad \text{\vocab{immagine} di $\rel$}
	\end{split}
			\]
	(notare che $\Dom(\rel)$ e $\Imm(\rel)$ non coincidono necessariamente con $A$ e $B$).
\end{definition}

\begin{definition}[Funzione]
	Chiamiamo \vocab{funzione} $f:A \rightarrow B$ una relazione $f \subseteq A \times B$ tale che:
	\[ \forall x \in A \; \exists ! \, y \in B \; (x,y) \in f
		\]
		(Intuitivamente $f$ è l'insieme delle coppie $(x,f(x))$ per $x \in A$).
\end{definition}

\begin{notation}[Immagine e immagine di un sottoinsieme]
	Data una funzione $f$ possiamo indicare la coppia $(x,y) \in f$ con la seguente abbreviazione:
	\[ y = f(x) \Mydef (x,y) \in f
		\]
	Dato $S \subseteq \Dom(f)$, indichiamo l'immagine di un sottoinsieme (ovvero l'insieme delle immagini del sottoinsieme) come:
	\[ f[S] \Mydef \{y \in \Imm(f)| \exists x \in S \; \underbrace{y = f(x)}_{= (x,y) \in f}\} = \underbrace{\{f(x) | x \in S\}}_{\text{informalmente}}
		\]
\end{notation}

\begin{definition}[Iniettività, suriettività e bigettività]
	Una funzione $f: A \rightarrow B$ è:
	\[ \begin{split}
		\text{\vocab{iniettiva} se:}\; & \forall y \in \Imm(f)\; \exists! \, x \in \Dom(f) \; f(x) = y\\
		\text{\vocab{suriettiva} se:}\; & B = \Imm(f)\; \text{ossia $\forall y \in B \; \exists x \in A \; f(x) = y$.}\\
		\text{\vocab{bigettiva} se:}\; &\text{è sia iniettiva sia surgettiva.}
	\end{split}
		\]
\end{definition}

\begin{definition}[Funzione inversa]
	Data $f$ iniettiva:
	\[ f^{-1} \Mydef \{(y,x) \in B \times A | f(x) = y\} \subseteq B \times A
		\]
\end{definition}

\begin{remark}
	Se $f$ iniettiva, $f^{-1} : \Imm(f) \rightarrow \Dom(f)$ è una funzione\footnote{Altrimenti è la semplice controimmagine di un sottoinsieme dell'immagine (che non è una funzione).} a
	sua volta iniettiva (basta pensare alla definizione di $f^{-1}$ iniettiva e usare che per l'iniettività di $f$ c'è un'unica $x \in \Dom(f)$ tale che $y = f(x)$).
	In particolare se $f : A \rightarrow B$ è bigettiva, allora $f^{-1}$ è bigettiva.
\end{remark}

\begin{definition}[Restrizione di una funzione]
	Data $f: A \rightarrow B$ e $A' \subseteq A$ definiamo:
	\[ f_{|A'} \Mydef \{(x,y) \in A' \times B | f(x) = y\}
		\]
	 ``$f$ \vocab{ristretta} ad $A'$\,'' è una funzione: $A' \rightarrow B$.
\end{definition}

\begin{definition}[Composizione di funzioni]
	Date $g : A \rightarrow B$ e $f : B \rightarrow C$:
	\[ f \circ g \Mydef \{(x,z) \in A \times C | z = f(g(x))\}\footnote{O più formalmente $\exists y(y = g(x) \land z = f(y))$.}
		\]
	``$f$ \vocab{composta} con $g$'' è una funzione: $A \rightarrow C$.
\end{definition}

\begin{notation}[Funzione identità]
	Indichiamo con $\id_A$ la \vocab{funzione identità} su $A$:
	\[ \id_A \Mydef \{(x,y) \in A \times A | x = y\} = \Delta_A
		\]
\end{notation}

\begin{remark}[Caratterizzazione funzione inversa]
	Data $f : A \rightarrow B$ bigettiva e $g : B \rightarrow A$ è equivalente scrivere:
	\[ g = f^{-1} \qquad g \circ f = \id_A \qquad f \circ g = \id_B
		\]
\end{remark}

\begin{exercise}[Composizione di funzioni iniettive/surgettive/bigettive]
	Data $f : A \rightarrow B$ e $g: B \rightarrow C$, sotto quali condizioni $g \circ f$ è iniettiva, suriettiva, bigettiva?
\end{exercise}

\begin{soln}
	Indaghiamo il problema partendo prima dalle singole funzioni con delle proprietà e componendole. Se $f$ e $g$ sono iniettive, allora $g \circ f$ è iniettiva, infatti:
	\[ g(f(x)) = g(f(y)) \overset{\text{$g$ iniett.}}{\iff} f(x) = f(y) \overset{\text{$f$ iniett.}}{\iff} x = y \qquad \forall x,y \in A
		\]
		che è equivalente alla definizione di $g \circ f : A \rightarrow C$ iniettiva. Se $f$ e $g$ sono surgettive, allora $g \circ f$ è surgettiva:
	\begin{align*}
		\text{$g$ surgettiva} &\iff \forall z \in C \; \exists y \in B \; g(y) = z \\
		\text{$f$ surgettiva} &\iff \forall y \in B \; \exists x \in A \; f(x) = y
	\end{align*}
	che messe assieme ci danno che $g(f(x)) = z$, cioè per ogni $z \in C$ esiste $x \in A$ tale che $(g \circ f)(x) = z$, che è equivalente alla definizione di $g \circ f$ surgettiva.
	Naturalmente, mettendo assieme i risultati precedenti, otteniamo che $f$ e $g$ bigettive implica $g \circ f$ bigettiva. Viceversa, osserviamo che se $g \circ f$ è iniettiva, allora $f$ è iniettiva,
	infatti, se per assurdo $f(x) = f(y)$, con $x \ne y$, allora, applicando $g$, si ha $g(f(x)) = g(f(y))$ (perché immagini di cose uguali), ma per iniettività di $g \circ f$, ciò equivale a $x = y$,
	che è assurdo, pertanto $x = y$\footnote{Abbiamo dimostrato per assurdo che $f(x) = f(y) \implies x = y$ (sotto l'ipotesi che $g \circ f$ iniettiva), il viceversa è banale e con questo si ha l'equivalenza con la definizione di $f$ iniettiva}.
	Se $g \circ f$ è surgettiva, allora $g$ è surgettiva, infatti, per ipotesi, $\forall z \in C \; \exists x \in A \; g(f(x)) = z$, e, dato che $f(x) \in B$, abbiamo trovato che per ogni $z \in C$ esiste $y = f(x) \in B$ tale che $g(y) = z$, ovvero $g$ surgettiva.\\
	Infine, verrebbe da chiedersi, se date $f$ iniettiva e $g$ surgettiva, $g \circ f$ sia necessariamente bigettiva (così da avere magari un'equivalenza tra la bigettività della composizione e le proprietà delle funzioni in partenza), sfortunatamente ciò è falso: presa
	$f : \{0,1\} \hookrightarrow \{0,1,2,3\}$ e $g : \{0,1,2,3\} \twoheadrightarrow \{0,1,2\}$, con:
	\begin{align*}
		& g(0) = 0 \qquad f(0) = 0 \\
		& g(1) = 0 \qquad f(1) = 1 \\
		& g(2) = 2 \\
		& g(3) = 3
	\end{align*}
	abbiamo $f$ iniettiva, $g$ surgettiva, ma $g \circ f$ non è né iniettiva ($g(f(0)) = g(f(1))$) né surgettiva ($\Imm(g \circ f) = \{0\}$).
\end{soln}

\begin{exercise}[Insieme quoziente e proiezione]
	\label{3.73}
	Data una relazione di equivalenza $\sim$ su $A$, dimostra che esiste un insieme $\faktor{A}{\sim}$ ed una funzione surgettiva $i_\sim$ da $A$ a $\faktor{A}{\sim}$
	tale che:
	\[ \forall x,y \in A \; x \sim y \leftrightarrow i_\sim(x) = i_\sim(y)
		\]
\end{exercise}

\begin{soln}
	Possiamo definire l'insieme $\faktor A \sim$ per separazione nelle parti di $A$ come segue:
	\[ \faktor{A}{\sim} \Mydef \{B \in \ps(A) | \forall x,y \in B \; x \sim y\}
		\]
	Osserviamo che per ogni $B, C \in \faktor A \sim$, vale che $B \cap C \ne \emptyset \iff B = C$, infatti, se esiste $x \in B \cap C$, allora $x \sim y$, $\forall y \in B$, e $x \sim z$, $\forall z \in C$.
	Da cui $w \in B \iff w \sim x \iff w \in C$ e quindi per l'arbitrarietà di $x$, vale $B = C$.\footnote{Essendo che ogni elemento, per quanto detto è in una classe di equivalenza di $\faktor{A}{\sim}$, si ha anche che $\bigcup \faktor{A}{\sim} = A$, dunque le classi di equivalenza sono disgiunte
	e la loro unione dà proprio l'insieme, pertanto si dirà che formano una \vocab{partizione} dell'insieme $A$.}\\
	Da quanto appena osservato segue quindi che ogni $x \in A$ appartiene ad una e una sola \vocab{classe di equivalenza} (gli elementi di $\faktor A\sim$), in quanto è sempre almeno in relazione con se stesso per riflessività, possiamo quindi definire $i_\sim$ come 
	la funzione da $A$ a $\faktor A \sim$ che manda $x$ nella sua classe di equivalenza. Naturalmente $i_\sim(x) = i_\sim(y)$ equivale al dire che le due classi di equivalenza sono la stessa, dunque per definizione
	si ottiene proprio che $x \sim y$. Inoltre $i_\sim$ è surgettiva in quanto in ogni classe di equivalenza di $\faktor A \sim$ c'è almeno un elemento (per la riflessività delle relazioni di equivalenza), la cui immagine via $i_\sim$ dà appunto la classe.
\end{soln}

\begin{exercise}[Primo teorema di ``omomorfismo'', per insiemi]
	Data una relazione di equivalenza $\sim$ su $A$ e $f : A \rightarrow B$, affinché esista la funzione $\widetilde{f}: \faktor{A}{\sim} \rightarrow B$ tale che $f = \widetilde{f} \circ i_\sim$,
	è necessario e sufficiente che $\forall x,y \in A \; x \sim y \rightarrow f(x) = f(y)$.
\end{exercise}

\begin{soln}
	Osserviamo che\footnote{Per essere formalissimi, staremmo usando che $f = \widetilde{f} \circ i_\sim \iff f(x) = (\widetilde{f} \circ i_\sim)(x)$, $\forall x \in A$, ovvero
	l'estensionalità per funzioni vista in un'osservazione precedente.} $f(x) = (\widetilde{f} \circ i_\sim)(x)$, $\forall x \in A$ se e solo se $f(x) = \widetilde{f}(i_\sim(x))$, ora ciò equivale al fatto che 
	l'immagine dell'elemento $x \in A$ al LHS è uguale a quella della classe di equivalenza (che è un sottoinsieme di $A$) $i_\sim(x)$ tramite $\widetilde{f}$ al RHS. Per rispettare la relazione richiesta (che sarebbe poi la commutatività di un diagramma)
	possiamo definire $\widetilde{f}(C)$, $C \in \faktor{A}{\sim}$, come $f(z)$ per un qualunque $z \in C$.\\ Ora ci basta osservare che questa è una buona definizione, e lo è in quanto tutti gli elementi in $C$ sono in relazione $\sim$ tra loro e per ipotesi tale relazione 
	è che la loro immagine via $f$ sia la stessa, pertanto $f(x) = f(y)$, $\forall x,y \in C$. Infine, poiché $\forall x \in A \; x \in i_\sim(x)$, si ha proprio che $\widetilde{f}(i_\sim(x)) = f(x)$. Abbiamo quindi dimostrato che l'uguaglianza iniziale è vera 
	se $\sim$ è definita come nelle ipotesi, osserviamo che se tale uguaglianza funziona, allora due elementi sono in relazione via $\sim$ se e solo se hanno la stessa immagine. Infatti, si avrebbe che:
	\[ \begin{split}
		f(x) = f(y) &\iff \widetilde{f}(i_\sim(x)) = \widetilde{f}(i_\sim(y)) \\
					&\iff i_\sim(x) = i_\sim(y) \\
					&\iff x \sim y
	\end{split}
		\]
	dove la prima equivalenza è l'assunto, la seconda è la definizione di $\widetilde{f}$ (che è una bigezione tra $\faktor A\sim$ e $\Imm(f)$, per questo abbiamo usato l'iniettività), mentre l'ultima equivalenza è la definizione di classi di equivalenza.
\end{soln}

\newpage
\section{Assioma dell'infinito e numeri naturali}
Il nostro prossimo obiettivo è definire i numeri naturali. I soli oggetti della teoria degli insiemi sono gl insiemi, per cui va da sé che 
i numeri saranno determinati insiemi. Il nostro scopo non è quindi tanto definire, quanto codificare i numeri naturali per mezzo di insiemi opportuni.
La scelta della codifica non è obbligata: per esempio potremmo decidere che:
\[ \text{``codifica buffa di $n$''} = \underbrace{\{\{\{\ldots \emptyset\ldots\}\}\}}_{\text{$n$ parentesi}}
	\]
Sceglieremo, invece, quest'altra codifica:
\[ n = \{0,1,\ldots,n-1\} = \{x \in \NN | x < n\}
	\]\[ 0 = \emptyset \qquad 1 = \{0\} \qquad 2 = \{0,1\} \qquad 3 = \{0,1,2\} \qquad \text{etc.}
		\]
che presenta alcuni vantaggi: per esempio $n$ è rappresentato da un insieme di $n$ elementi, e dire $m < n$ equivale semplicemente a dire $m \in n$.\\
L'ostacolo è ora parlare di questi oggetti in maniera precisa nel linguaggio della teoria degli insiemi. A dire il vero, potremmo già scrivere una formula $\Phi(n)$ che dice 
``$n$ è un numero naturale'' si tratta di un \textcolor{red}{esercizio} difficile, che sarà reso più facile da idee che vedremo più avanti. Noi non scriviamo questa formula, ma, anche a farlo,
non potremmo comunque dimostrare che esiste un insieme i cui elementi sono i numeri naturali, questo perché gli assiomi visti finora non permettono di uscire dalla classe degli insiemi finiti (degli insiemi ``ereditariamente finiti'',
ad essere precisi: definiremo questi concetto a tempo debito).\\
Servirà un nuovo assioma. E l'idea da sfruttare è che, siccome $n = \{0,\ldots,n-1\}$, per ottenere il successore di $n$, ossia $n+1 = \{0,\ldots,n-1,n\}$ dobbiamo aggiungere a $n$ l'elemento $n$ stesso: $n+1 = n \cup \{n\}$.
Avendo una formula per denotare il successore, possiamo postulare l'esistenza di un insieme chiuso per successori, e questo ci darà $\NN$.

\begin{definition}
	[Successore]
	Definiamo il \vocab{successore} di $x$:
	\[ s(x) \Mydef x \cup \{x\}
		\]
\end{definition}

\begin{definition}
	[Insiemi induttivi]
	Diciamo che $A$ è un \vocab{insieme induttivo} se contiene $\emptyset$ ed è chiuso per successori \footnote{Ciò non esclude che ci possano essere altri elementi oltre a $\emptyset$ che non siano successori (questa cosa è sempre falsa in $\omega$).}, ossia:
	\[ \text{$A$ è induttivo} \iff \emptyset \in A \land \forall x \in A \; s(x) \in A
		\]
\end{definition}

\begin{axiom}
	[Assioma dell'infinito]
	\label{ax7}
	Esiste un insieme induttivo.
	\[ \exists A (\emptyset \in A \land (\forall x \in A \; s(x) \in A))
		\]
\end{axiom}

Finalmente definiamo l'insieme dei numeri naturali - che, per qualche buffa ragione, chiamiamo $\omega$ - come l'intersezione della classe, non vuota per l'assioma dell'infinito,
di tutti gli insiemi induttivi.\footnote{Aver introdotto l'assioma dell'infinito
ci assicura che tale intersezione è non vuota, e ciò basta affinché $\omega$ sia un insieme (in caso contrario avremmo avuto l'intersezione del vuoto, che, come visto, non è un insieme).}

\begin{definition}[Numeri naturali]
	L'insieme $\omega$ è l'intersezione di tutti gli insiemi induttivi, ossia $\omega$ è l'unico insieme tale che:
	\[ \forall x (x \in \omega \leftrightarrow(\forall A \; \text{``$A$ è induttivo''}\rightarrow x \in A)) \footnote{Cioè $x$ è in $\omega$ se e solo se è elemento
	di qualsiasi insieme induttivo (nella \underline{classe} degli insiemi induttivi), e, inoltre, essendo l'intersezione di una classe, è in particolare un insieme (perché per definizione
	stiamo intersecando gli elementi di una classe, che sono insiemi).}
		\]
\end{definition}

Adesso che abbiamo $\omega$, possiamo facilmente dimostrare che ogni dato numero naturale vi appartiene.

\begin{definition}[Codifica dei numeri naturali]
	Definiamo:
	\[ 0 \Mydef \emptyset \qquad 1 \Mydef s(0) \qquad 2 \Mydef s(1) \qquad 3 \Mydef s(2) \qquad \text{etc.}
		\]
\end{definition}

\begin{exercise}
	Dimostra che $0,1,2,3 \in \omega$.
\end{exercise}

\begin{soln}
	Avendo definito $\omega$ come:
	\[ \omega = \bigcap_{\text{$A$ induttivo}} A
		\]
	sappiamo che $\emptyset \in A$, per ogni insieme induttivo (per definizione), dunque $0 \in \omega$. Inoltre vale che l'intersezione di insiemi induttivi è chiusa per successore (e quindi per quanto appena mostrato è a sua volta un insieme induttivo), infatti:
	\[ \forall x \in \bigcap_{\text{$A$ induttivo}} A \leftrightarrow \forall A \, \text{$A$ induttivo}\, (x \in A)
		\]
	ed essendo tutti gli $A$ chiusi per successore (in quanto induttivi) segue che:
	\[ s(x) \in \bigcap_{\text{$A$ induttivo}} A \implies s(x) \in \omega
		\]
	Pertanto, avendo osservato che $0 \in \omega$, si avrà anche che $1 = s(0) \in \omega$, $2 = s(1) \in \omega$, $3 = s(2) \in \omega$ e così via.
\end{soln}

Un esercizio un po' più difficile è esibire insiemi che non appartengono a $\omega$.

\begin{exercise}
	Dimostra che $\{\{\emptyset\}\} \not \in \omega$.\footnote{\textbf{\underline{Idea}}: Esibisci un insieme induttivo che non contiene $\{\{\emptyset\}\}$.}
\end{exercise}

\begin{soln}
	Osserviamo che $\{\{\emptyset\}\}$ non è un successore, se fosse che $s(x) = x \cup \{x\} = \{\{\emptyset\}\}$, dato che $x$ è elemento di $s(x)$ e che $\{\{\emptyset\}\}$ ha un solo elemento, per \hyperref[ax2]{estensionalità} deve essere che che $x = \{x\} = \{\emptyset\}$ (ossia tutti gli elementi di $s(x)$ devono essere 
	uguali all'unico elemento di $\{\{\emptyset\}\}$). Pertanto avremmo che $x = \{\emptyset\}$, ma $s(x) = s(\{\emptyset\}) = \{\emptyset\} \cup \{\{\emptyset\}\} =\footnote{Volendo essere pignoli possiamo usare la definizione dell'unione come il prendere gli elementi degli elementi: $\{\emptyset\} \cup \{\{\emptyset\}\} = \bigcup \{\{\emptyset\},\{\{\emptyset\}\}\}$,
	e l'unione di tale insieme è formata appunto da tutti gli elementi degli elementi (quindi naturalmente il vuoto $\emptyset$ e anche $\{\emptyset\}$).} \{\emptyset,\{\emptyset\}\}$, ma $\{\emptyset\} \ne \emptyset$, perché $\{\emptyset\}$ è non vuoto e $\emptyset$ è proprio il vuoto.\\
	Avendo dimostrato che $\{\{\emptyset\}\}$ non è né un successore né (ovviamente) il vuoto, ci basta mostrare mostrare che non appartiene ad un insieme induttivo $A$ che non ha altri elementi (oltre a $\emptyset$) che non sono successori. Dando per buono che $\omega$ non contenga elementi che non sono successori, si ottiene che
	$\{\{\emptyset\}\} \not \in \omega$.\footnote{Non abbiamo usato l'hint di Mamino e abbiamo usato un fatto non dimostrato.}
\end{soln}

\subsection{Gli assiomi di Peano}
Per convincerci, però, che $\omega$ è, a buon diritto, l'insieme dei numeri naturali, serve qualcosa di più. Classicamente, i numeri naturali si definiscono per mezzo degli
\vocab{assiomi di \href{https://it.wikipedia.org/wiki/Giuseppe_Peano}{\textcolor{purple}{Peano}}}. Questi assiomi, che caratterizzano a meno di isomorfismi l'insieme $\NN$ dotato della funzione di successore, \textbf{per noi diventano dei teoremi} che
dimostreremo a proposito dell'insieme $\omega$\footnote{Cioè gli assiomi di Peano diventano enunciati dimostrabili all'interno della ZFC.}. In questo senso\footnote{Classicamente gli assiomi definivano $\NN$ a meno di isomorfismo, mostrando che $\omega$ li soddisfa siamo sicuri di avere l'oggetto (insieme) $\NN$ definito da tali assiomi nella ZFC, e tale oggetto è appunto $\omega$.}, quindi, $\omega$ codifica legittimamente i numeri naturali.

\begin{definition}
	[Assiomi di Peano al secondo ordine\protect\footnote{qualunque cosa questo significhi...}]
	Dato un insieme $\NN$, un elemento $0 \in \NN$, e una funzione:
	\[ \text{succ} : \NN \longrightarrow \NN
		\]
	diciamo che $(\NN,0,\text{succ})$\footnote{La 3-upla ordinata formata dai tre insiemi $\NN$,$0$,succ: (($\NN$,0),succ)$= \{(\NN,0),\{(\NN,0),\text{succ}\}\} = \{\{\NN,\{\NN,0\}\},\{\{\NN,\{\NN,0\}\},\text{succ}\}\}$.} soddisfa gli assiomi di Peano se:
	\begin{enumerate}[(a)]
		\item \label{a}Il successore è iniettivo:
		\[ \forall n,m \in \NN \; \text{succ}(m) = \text{succ}(n) \rightarrow m = n \, \footnote{L'altra freccia è banale e sarà data sempre per scontata.}
			\]
		\item \label{b}Lo zero non è un successore:
		\[ \not\exists n \in \NN \; \text{succ}(n) = 0
			\]
		\item \vocab{Principio di induzione}: data una qualunque formula insiemistica (proprietà) $\Phi(n)$ vale:
		\[ (\Phi(0) \land \forall n \in \NN \; \Phi(n) \rightarrow \Phi(\text{succ}(n))) \rightarrow \forall n \in \NN \; \Phi(n)
			\]
	\end{enumerate}
\end{definition}


\begin{figure*}[h]
		\centering
		\includegraphics[width=10.5cm]{immagini/peano.png}
		\captionsetup{labelformat=empty}
		\caption{Apparivano così in ``\emph{Arithmetices principia}'', nel 1889, gli assiomi di Peano.}
\end{figure*}


\begin{theorem}[$\omega$ soddisfa gli assiomi di Peano]
	La funzione $\text{succ}: \omega \rightarrow \omega : n \mapsto s(n)$, è ben definita e $(\omega,\emptyset,\text{succ})$
	soddisfa gli assiomi di Peano.
\end{theorem}

\begin{proof}
	Per controllare che $\text{succ}$ sia ben definita, occorre assicurarsi che se $n \in \omega$, allora $\text{succ}(n) = s(n) \in \omega$. Fissiamo $n \in \omega$
	e consideriamo un qualunque insieme induttivo $A$. Siccome $A$ è induttivo $\omega \subseteq A$, quindi $n \in A$, e, di conseguenza $s(n) \in A$. Per 
	l'arbitrarietà di $A$, allora, $s(n)$ appartiene a ogni insieme induttivo (quindi all'intersezione, ovvero $\omega$).\\
	Dimostriamo ora che $\omega$ rispetta gli assiomi di Peano. Iniziamo con dimostrare (b) e (c), poi passeremo ad (a):
	\begin{enumerate}
		\item[(b)] Supponiamo, per assurdo, $s(n) = \emptyset$. Abbiamo allora che:
		\[ n \in s(n) =  n \cup \{n\} = s(n) = \emptyset
			\]
		contro la definizione di $\emptyset$.
		\item[(c)] Dimostriamo che l'insieme $A = \{n \in \omega | \Phi(n)\} \subseteq \omega$ è induttivo, da cui $\omega = A$\footnote{Stiamo costruendo $A$ come sottoinsieme di $\omega$ (che
		a sua volta sarà contenuto in $A$, non appena avremo dimostrato che quest'ultimo è induttivo, per definizione).}, quindi varrà che $\forall n \in \omega \; \Phi(n)$.
		\begin{enumerate}[(1)]
			\item Per ipotesi abbiamo che $\Phi(\emptyset)$, quindi $\emptyset \in A$.
			\item $n \in A \overset{\text{def. $A$}}{\implies} \Phi(n) \overset{\text{ipotesi}}{\implies} \Phi(\text{succ}(n)) = \Phi(s(n)) \overset{\text{$n \in \omega \rightarrow s(n) \in \omega$}}{\implies} s(n) \in A$
		\end{enumerate}
		\item[(a)] La dimostrazione passa attraverso due lemmi.
					\begin{lemma}
					[Lemma 1]
					L'unione di un elemento di $\omega$ è contenuta nell'elemento: $\forall n \in \omega \; \bigcup n \subseteq n$.
					\end{lemma}
					\begin{proof}
						Avendo dimostrato in (c) che in $\omega$ vale l'induzione possiamo usarla con $\Phi(n) \Mydef \bigcup n \subseteq n$. 
						\[
							\begin{split}
							\boxed{\Phi(\emptyset)} & \qquad \bigcup \emptyset = \emptyset \subseteq \emptyset \\
							\boxed{\Phi(n) \rightarrow \Phi(s(n))} & \qquad \bigcup s(n) = \bigcup(n \cup \{n\}) \overset{\star}{=} \underbrace{\left(\bigcup n\right)}_{\subseteq n} \cup n \overset{\text{Hp. indutt.}}{\subseteq} n \cup n = n \subseteq s(n)
							\end{split}
						\]
						(si noti che il passo base è coerente con le definizioni delle abbreviazioni date), e $\star$ vale in quanto:
						\[ \begin{split}
							x \in \bigcup(n \cup \{n\}) \overset{\text{def.}}{\iff} &\quad \exists y ( x \in y) \land (y \in (n \cup\{n\})) \\
														\overset{\text{caratt. $\cup$}}{\iff} &\quad \exists y( x \in y) \land (y \in n \lor y = n)\\
														\overset{\text{distrib. $\land$}}{\iff} &\quad \exists y \; ( x \in y \land y \in n) \lor (x \in y \land y = n)\\
														\iff &\quad \exists y ( x \in y \land y \in n) \lor \exists y(x \in y \land y = n)\\
														\overset{\text{def.}}{\iff} &\quad x \in \bigcup n \lor x \in n\\
														\overset{\text{caratt. $\cup$}}{\iff} &\quad x \in \left(\bigcup n\right) \cup n
						\end{split}
							\]
						(dove alla secondo membro della seconda equivalenza abbiamo che $y \in \{n\}$ e per \hyperref[ax2]{estensionalità} equivale a $y = n$.).
					\end{proof}
					\begin{lemma}
						[Lemma 2]
						L'unione dei successori di un elemento in $\omega$ è proprio l'elemento: $\forall n \in \omega \; \bigcup s(n) = n$.
					\end{lemma}
					\begin{proof}
						Ricopiando quanto fatto nel passo induttivo della dimostrazione precedente abbiamo:
						\[ \bigcup s(n) = \bigcup (n \cup \{n\}) = \left(\bigcup n\right) \cup n \overset{\star}{\subseteq} n
							\]
						dove in $\star$ abbiamo usato che $\bigcup n \subseteq n$, non per ipotesi induttiva (visto che non stiamo facendo alcuna induzione), ma stiamo usando direttamente il risultato del Lemma 1.
						Naturalmente vale anche che $n \subseteq \bigcup s(n)$ (ogni elemento di $n$ è elemento dell'elemento $n$ in $s(n)$), dunque vale la tesi.
					\end{proof}
					Finalmente abbiamo che, per il Lemma 2:
					\[ s(m) = s(n) \implies \bigcup s(m) = \bigcup s(n) \overset{\text{Lemma 2}}{\iff} m = n
						\]
					dove la prima freccia è data dal fatto che stiamo considerando l'unione di insiemi uguali, dunque succ$: \omega \rightarrow \omega$ è iniettiva.
	\end{enumerate}
\end{proof}

\subsection{L'ordine di omega}
Conviene, adesso, sviluppare un po' di tecnologia per manipolare i numeri interi. Dopo, dimostreremo altresì che gli assiomi di Peano hanno un unico modello $(\NN, 0, \text{succ})$
a meno di isomorfismi.

\begin{notation}[Relazione di ordine su $\omega$]
	Dati $m,n \in \omega$, scriviamo:
	\[ m < n \Mydef m \in n \footnote{Per essere precisi non stiamo usando $\in$ come una relazione (visto che abbiamo assunto all'inizio che fosse un simbolo del linguaggio della teoria degli insiemi), ma stiamo definendo $< \Mydef \{(m,n) \in \omega \times \omega | m \in n\}$.}
		\]
\end{notation}

\begin{proposition}[Ordinamento totale di $\omega$]
	La relazione $<$ è un ordine totale su $\omega$. 
\end{proposition}

Per dimostrare questa proposizione, sono comodi alcuni lemmi.

\pagebreak

\begin{remark}[Successore del secondo termine in un'appartenenza]
	\label{succ2}
	Si osserva che valgono le seguenti cose:
	\begin{enumerate}[(1)]
		\item $m \in n \rightarrow m \in s(n)$, infatti $n \subseteq n \cup \{n\} = s(n)$ (banalmente se $m$ è contenuto in $n$ allora è contenuto anche nel suo successore).
		\item $m \in s(n) \rightarrow (m \in n \lor m = n)$, ciò se $m$ è nel successore di $n$, allora è $n$ stesso o un suo elemento, infatti:
		 \[ \begin{split}
			m \in s(n) = n \cup \{n\} & \iff m \in (n \cup \{n\}) \\
									& \iff (m \in n) \lor (m \in \{n\}) \\
									&\iff (m \in n) \lor (m = n)
		 \end{split}
			\]
		(nella seconda equivalenza si è usata la caratterizzazione data dell'appartenenza ad un unione di insiemi, e nella terza il fatto che se $m$ appartiene ad un singoletto, allora per \hyperref[ax2]{estensionalità} è proprio l'unico elemento del singoletto).
	\end{enumerate}
\end{remark}

\begin{lemma}[Successore del primo termine in un'appartenenza]
	$\forall a,b \in \omega \; a \in b \rightarrow (s(a) \in b \lor s(a) = b)$.\footnote{Moralmente: se un numero è strettamente più piccolo di un altro, o il suo successore è a sua volta più piccolo del secondo numero, o coincide con quest'ultimo.}
\end{lemma}

\begin{proof}
	Procediamo per induzione su $b$.
	\begin{itemize}
		\item[$\boxed{\text{caso $b = 0$}}$] $a \in \emptyset \rightarrow \ldots$ vera a vuoto, perché $a \in \emptyset$ è falsa (dunque l'implicazione è sempre vera, indipendentemente dal valore di verità dell'antecedente).
		\item[$\boxed{\text{caso $b = s(n)$}}$] L'ipotesi induttiva è $a \in n \rightarrow (s(a) \in n \lor s(a) = n)$. Dobbiamo dimostrare:
		\[ a \in s(n) \rightarrow (s(a) \in s(n) \lor s(a) = s(n))
			\]
		abbiamo che $a \in s(n) \iff a \in n \cup \{n\} \iff a \in n \lor a = n$. Quindi abbiamo due casi:
		\[ \begin{split}
			& a \in n \overset{\text{Hp. indutt.}}{\implies} (s(a) \in n) \lor (s(a) = n) \overset{\text{def. $s(n)$}}{\iff} s(a) \in s(n) \\
			& a = n \iff s(a) = s(n)
		\end{split}
			\]
		(la seconda equivalenza è giustificata dal fatto che abbiamo dimostrato che la funzione successore in $\omega$ è iniettiva).
	\end{itemize}
\end{proof}

Possiamo ora dimostrare la proposizione iniziale.

\begin{proof}
	Per verificare che $<$ è una relazione di ordine stretto totale, dobbiamo verificare che è irriflessiva, transitiva e totale (cioè presi qualsiasi due elementi di $\omega$ la loro coppia ordinata appartiene a $<$).
	\begin{itemize}
		\item[$\boxed{\text{transitività}}$] Vogliamo verificare che $(a \in b \land b \in c) \rightarrow a \in c$. Procediamo per induzione su $c$:
		\begin{itemize}
			\item[$\boxed{\text{caso $c = 0$}}$] la premessa $b \in c$ è falsa, quindi l'implicazione è vera a vuoto (l'antecedente è sempre falso, quindi l'implicazione sempre vera).
			\item[$\boxed{\text{caso $c = s(n)$}}$] assumiamo per ipotesi induttiva $(a \in b \land b \in n) \rightarrow a \in n$, e vogliamo dimostrare:
				\[ (a \in b \land b \in s(n)) \rightarrow a \in s(n)
					\]
				Osserviamo che $a \in b \implies a \in s(b)$, e che $b \in s(n) \overset{\text{Lemma}}{\implies} s(b) \in s(n) \lor s(b) = s(n)$, abbiamo quindi due casi in base a $s(b)$:
				\[ \begin{split}
					& s(b) = s(n) \implies a \in s(b) = s(n) \implies a \in s(n)\\
					& s(b) \in s(n) \implies a \in s(b) \in s(n) \implies a \in s(n)
				\end{split}
					\]
				questo usando il lemma precedente, potevamo anche scegliere di usare l'osservazione per dire che $b \in s(n) \implies b = n \lor b \in n$ e ottenere ancora i casi:
				\[  \begin{split}
					& b = n \implies a \in b = n \overset{\text{Oss.}}{\implies} a \in s(n) \\
					& b \in n \implies a \in b \in n \implies a \in n \overset{\text{Oss.}}{\implies} a \in s(n)
					\end{split}
					\]
		\end{itemize}
		\item[$\boxed{\text{irriflessività}}$] Vogliamo verificare $\neg \,a \in a$, e lo facciamo per induzione su $a$:
		\begin{itemize}
			\item[$\boxed{\text{caso $a = 0$}}$] $\neg \, \emptyset \in \emptyset$, vero per definizione di $\emptyset$.
			\item[$\boxed{\text{caso $a = s(n)$}}$] L'ipotesi induttiva è $\neg \, n \in n$, e vogliamo verificare che $\neg s(n) \in s(n)$. Procediamo per assurdo, supponiamo che $s(n) \in s(n)$, e per l'osservazione abbiamo due casi:
			\[ \begin{split}
				& s(n) = n \implies n \in n \; \lightning\\
				& s(n) \in n \implies n \in s(n) \in n \implies n \in n \; \lightning
			\end{split}
				\]
			($n \in n$ è falso perché per ipotesi induttiva $\neg(n \in n)$ è vero).
		\end{itemize}
		\item[$\boxed{\text{totalità}}$] Vogliamo dimostrare che $\forall a,b \in \omega (a \in b) \lor (a = b) \lor (b \in a)$. Iniziamo per induzione su $a$:
		\begin{itemize}
			\item[$\boxed{\text{caso $a = 0$}}$] La tesi diventa $\forall b \in \omega (\emptyset \in b) \lor (\emptyset = b) \lor (b \in \emptyset) \footnote{Ovviamente quest'ultimo caso è sempre faslo e quindi può essere escluso.}$. Procediamo quindi per induzione su $b$:
			\begin{itemize}
				\item \textbf{\underline{caso $b = 0$}} La tesi diventa $(\emptyset \in \emptyset) \lor (\emptyset = \emptyset)$, dove naturalmente la prima affermazione è sempre falsa, mentre la seconda è sempre vera, dunque la tesi è vera.
				\item \textbf{\underline{caso $b = s(m)$}} La tesi è $(\emptyset \in s(m)) \lor (\emptyset = s(m))$, con ipotesi induttiva $(\emptyset \in m) \lor (\emptyset = m)$. Abbiamo quindi due casi in base all'ipotesi induttiva:
				\[ \begin{split}
					& \emptyset \in m \implies \emptyset \in s(m) \\
					& \emptyset = m \implies \emptyset \in \{\emptyset\} = s(m)
				\end{split}
					\]
				in entrambi casi è vera la tesi perché è sempre vero il primo termine.
			\end{itemize}
			\item[$\boxed{\text{caso $a = s(n)$}}$] La tesi è $\forall b \in \omega (s(n) \in b) \lor (s(n) = b) \lor (b \in s(n))$, mentre l'ipotesi induttiva è $(n \in b) \lor (n = b) \lor (b \in n)$. Dall'ipotesi induttiva abbiamo quindi tre casi:
				\[ \begin{split}
					& n \in b \overset{\text{Lemma}}{\implies} s(n) \in b \lor s(n) = b \\
					& n = b \overset{\text{Iniett. del succ.}}{\implies} s(n) = s(b) \implies b \in s(b) = s(n) \implies b \in s(n)\\
					& b \in n \overset{\text{Oss.}}{\implies} b \in s(n) \implies b \in a 
				\end{split}
					\]
				in tutti e tre i casi almeno una delle tre proposizioni della tesi è vera, dunque la tesi è sempre vera.
		\end{itemize}
	\end{itemize}
\end{proof}

\begin{remark}[$\leq$ ordina totalmente $\omega$]
	Avendo dimostrato che $<$ è un ordine totale su $\omega$, abbiamo dimostrato in automatico che anche $\leq = < \cup \Delta$ lo è, infatti, per la corrispondenza tra i due (come si è visto precedentemente in una proposizione), anche le definizioni di ordine totale sono corrispondenti (in particolare per 
	$\leq$ ci basta che valga una tra $\leq$ e $\geq$, se valgono entrambe c'è l'=, mentre per $<$ chiedevamo nella definizione che valesse $<$, $>$ o $=$, quindi se nella dimostrazione precedente avessimo usato $\leq$ al posto di $<$ avremmo ottenuto lo stesso risultato perché le richieste nella definizione di ordine totale sono le stesse).
\end{remark}

\begin{corollary}[Rappresentazione dei numeri naturali]
	Un numero naturale è l'insieme dei numeri naturali minori di lui.
	\[ \forall m \in \omega \; m = \{n \in \omega | n < m\}
		\]
\end{corollary}

\begin{proof}
	Vogliamo dire che $m = \{ n \in \omega | n \in m\}$, ossia per definizione di sottoinsieme che $m \subseteq \omega$. Per induzione: $\emptyset \subseteq \omega$ è vera (perché $\omega$ è induttivo).
	Assumiamo che $m \subseteq \omega$, allora $s(m) = \underbrace{m}_{\subseteq \omega} \cup \{m\}$ e $\{m\} \subseteq \omega$ perché $m \in \omega$ per ipotesi iniziale, quindi si conclude che $s(m) \subseteq \omega$.
\end{proof}

\begin{corollary}[Più piccolo = contenuto]
	$\forall m,n \in \omega (m \leq n \leftrightarrow m \subseteq n$).
\end{corollary}

\begin{proof}
	Siccome $\omega$ è totalmente ordinato, si danno due casi (nel primo dimostro $\rightarrow$, nel secondo dimostro che la negazione della premessa implica la negazione della conseguenza, che è equivalente a $\leftarrow$):
	\[  \begin{split}
		& m \leq n \implies \forall x \in \omega (x < m \rightarrow x < n) \implies \forall x \in \omega (x \in m \rightarrow x \in n) \implies m \subseteq n \\
	    & n < m \implies n \in m \; \text{tuttavia} \; n \not\in n \; \text{quindi non può essere che $m \subseteq n$ ovvero} \; m \not\subseteq n
		\end{split}
			\]
	($n \not \in n$ perché abbiamo dimostrato che $<$ è di ordine stretto su $\omega$, quindi irriflessiva).
\end{proof}

\subsection{Induzione forte e principio del minimo}
\begin{theorem}
	[Principio di induzione - forma forte]
	Data una formula insiemistica $\Phi(x)$, vale:
	\[ (\forall n \in \omega (\forall x < n \; \Phi (x)) \rightarrow \Phi(n)) \rightarrow \forall n \in \omega \; \Phi(n)
		\]
	Ovvero, se assumendo $\Phi(x)$ per tutti gli $x < n$, abbiamo $\Phi(n)$, allora $\Phi(n)$ è vera per tutti i numeri $n$.
\end{theorem}

\begin{remark}
	Chiaramente questa forma è ``forte'' perché permette di assumere un'ipotesi induttiva più forte dell'induzione di Peano. In quella, infatti, si deve dedurre $\Phi(n)$ a 
	partire da $\Phi$ del numero precedente. Qui, invece, possiamo far conto di sapere $\Phi$, non solo per il precedente, ma per tutti i numeri minori di $n$.
\end{remark}

\begin{proof}
	Assumiamo vero l'antecedente per ipotesi ovvero assumiamo vera l'implicazione:
	\[ \forall n \in \omega \, (\forall x < n \; \Phi(x)) \rightarrow \Phi(n)
		\]
	Dalle tavole di verità quest'espressione può essere vera sia se antecedente e conseguente sono veri sia se l'antecedente è falso. Mostriamo di essere nel primo caso,
	ovvero dimostriamo per induzione (debole) che [la premessa è vera], ovvero $\forall m \in \omega \; \psi(m)$ dove:
	\[ \psi(m) \Mydef \forall x < m \; \Phi(x)
		\]
	\begin{itemize}
		\item[$\boxed{\text{caso $m = 0$}}$] $\forall x < \emptyset \; \Phi(x)$ è vera a vuoto.
		\item[$\boxed{\text{caso $m = s(n)$}}$] Per ipotesi induttiva abbiamo $\forall x < n \; \Phi(x).$ Vogliamo che $x < s(n) \;\Phi(x)$, dall'osservazione sappiamo che ciò equivale a $x < n \lor x = n$.
		Si danno quindi due casi:
		\begin{itemize}
			\item Nel caso $x < n$ abbiamo $\Phi(x)$ per ipotesi induttiva.
			\item Nel caso $x = n$, l'ipotesi induttiva, combinata con l'antecedente ci dà $\Phi(n)$, ossia $\Phi(x)$ (perché abbiamo che $\forall x < n \;\Phi(x) \rightarrow \Phi(n)$, ma tutta l'espressione è vera per ipotesi e per ipotesi induttiva l'antecedente è vero, quindi anche $\Phi(n)$ lo è).
			(Per l'arbitrarietà di $x<m$ abbiamo dimostrato $\forall x < m \; \Phi(x)$)\footnote{Stiamo solo giustificando formalmente il per ogni.}.
		\end{itemize}
	\end{itemize}
	Ora abbiamo dimostrato che $\forall m \in \omega \; \forall x < m \; \Phi(x)$, quindi siamo nel secondo caso, e otteniamo che nella premessa $\Phi(n)$ è vera. Ora dato un $n \in \omega$ qualunque, ci basta prendere nell'antecedente $m = n + 1$ e $x = n$ e otteniamo in automatico $\Phi(n)$ (e siamo sicuri sia vera 
	visto che abbiamo per ipotesi un'implicazione con antecedente vero).
\end{proof}

\begin{theorem}
	[Principio del minimo]
	Sia $A \subseteq \omega$. Se $A \ne \emptyset$ allora esiste $n \in A$ tale che $\forall x \in A \; n \leq x$. Ovvero, ogni sottoinsieme non vuoto di $\omega$ ha un minimo elemento.
\end{theorem}

\begin{remark}
	[Idea $\lbrack$e parte$\rbrack$ della dimostrazione] Si dimostra per induzione forte che, se $n \in A$, allora $A$ ha un minimo. Poi, siccome $A$ non è vuoto, deve esserci qualche $n \in A$, quindi $A$ ha minimo. L'induzione 
	funziona così. Se $n \in A$, si danno due casi. O esiste $x < n$ con $x \in A$, e allora $A$ ha minimo per ipotesi induttiva (che è quello che stiamo per dimostrare), oppure $\forall x < n \; x \not\in A$, ma allora $n$ è il minimo di $A$ (e abbiamo concluso).
\end{remark}

\begin{proof}
	Dimostriamo la contronominale della tesi (nel caso in cui $x \in A$), ovvero dobbiamo dimostrare che se $A$ non ha un minimo elemento, allora $A$ è vuoto. \\
	Assumiamo quindi per ipotesi induttiva che esista un elemento strettamente più piccolo di tutti gli altri, ovvero $\forall n \in A \; \exists x \in A \; x < n$ (stiamo usando il $<$ perché il caso in cui $x = n$ è già contemplato nell'osservazione dicendo che che $x \not \in A$).
	Osserviamo che la contronominale della nostra tesi\footnote{Cioè di questo caso della dimostrazione come visto nell'osservazione.} è:
	\[ (\neg \exists x < n \; x \in A) \rightarrow n \not \in A
		\]
	ed equivale a:
	\[	\begin{split}
		& (\neg \exists x (x < n) \land (x \in A)) \rightarrow n \not \in A \\
		\overset{\text{$\land$ commut.}}{\iff} & (\neg \exists \; x \in A \; x < n) \rightarrow n \not \in A \\
		\overset{\text{contronom.}}{\iff} & n \in A \rightarrow (\exists x \in A \; x < n)
		\end{split}
		\]
	ma la cosa appena scritta è equivalente all'ipotesi induttiva, pertanto la contronominale della tesi è vera, e quindi anche la tesi. Abbiamo quindi dimostrato anche il secondo caso dell'induzione forte e ciò conclude la dimostrazione del principio del minimo (perché stiamo supponendo ci sia sempre un elemento, come visto nell'osservazione iniziale).
\end{proof}


\begin{remark}
	Per completare l'equivalenza tra induzione, induzione forte e principio del minimo, andrebbe dimostrato anche che principio del minimo$\implies$induzione.
\end{remark}

\begin{definition}[Insieme ben ordinato]
	Un insieme totalmente ordinato $(S, <)$ si dice \vocab{bene ordinato} se ogni sottoinsieme non vuoto ha un minimo.\footnote{Cioè se vale il principio del minimo c(come vale in $\omega$).}
	\[ \forall A \subseteq S \; A \ne \emptyset \rightarrow \exists m \in A \; \forall x \in A \; m \leq x
		\]
\end{definition}

La nozione di buon ordine è stata introdotta da Cantor agli albori della teoria degli insiemi, e giocherà un ruolo centrale in questo corso.

\begin{example}
	$(\omega, <)$ è un insieme bene ordinato\footnote{Si usa la notazione di coppia ordinata per indicare sia l'insieme sia la relazione che c'è sopra.} per quanto visto nel teorema precedente.
\end{example}

\begin{exercise}
	Dimostra che $X = s(s(s(\omega)))$ è bene ordinato dalla relazione $a < b \Mydef a \in b$.
\end{exercise}

\begin{soln}

\end{soln}

\subsection{Ricorsione numerabile}
La ricorsione è il procedimento per cui si costruisce una funzione $f : \omega \rightarrow \text{qualcosa}$, definendo $f(s(n))$ a partire da $f(n)$, o,
più in generale da $f(\emptyset),\ldots,f(n)$. Questo è un procedimento fondamentale: potremmo dire che è IL modo di pensare gli infidi puntini ($\ldots$). Vediamo qualche esempio.

\begin{example}
	[Operazioni aritmetiche]
	Possiamo definire somma e prodotto come:
	\[ \begin{cases}
		a + \textcolor{red}{0} = a \\
		a + \textcolor{red}{s(b)} = s(a + b)
	\end{cases}
	\qquad
	\begin{cases}
		a \cdot \textcolor{red}{0} = 0\\
		a \cdot \textcolor{red}{s(b)} = a \cdot b + a
	\end{cases}
		\]
	anziché $a + b = \underbrace{s(s(\ldots a \ldots))}_{\text{$b$ successori}}$ (abbiamo il caso base con 0, e poi si procede ricorsivamente dal caso base fino a $b$) e $a \cdot b = \underbrace{a + a + \ldots + a}_{\text{$b$ volte}}$ (ricorsivamente ad un certo 
	punto si partirà da $a$ e si inizierà a sommare).
\end{example}

\begin{example}
	[Potenza e fattoriale]
	Possiamo definire ricorsivamente potenze e fattoriali come segue:
	\[ \begin{cases}
		a^{\textcolor{red}{0}} = 1\\
		a^{\textcolor{red}{s(b)}} = a^b \cdot a
	\end{cases}
	\qquad
	\begin{cases}
		\textcolor{red}{0}! = 1\\
		\textcolor{red}{s(a)}! = a! \cdot s(a)
	\end{cases}
		\]
	anziché $a^b = \underbrace{a \cdot a \cdot \ldots \cdot a}_{\text{$b$ volte}}$ e $a! = 1 \cdot 2 \cdot \ldots \cdot (a - 1) \cdot a$.
\end{example}

\begin{example}
	[Sommatoria]
	Possiamo definire la sommatoria come:
	\[\begin{cases}
		\displaystyle
		\sum_{i = 0}^{\textcolor{red}{0}} f(i) = 0\\
		\displaystyle
		\sum_{i = 0}^{\textcolor{red}{s(a)}} f(i) = \left(\sum_{i=0}^{a} f(i) \right) + f(s(a))
	\end{cases}
		\]
	anziché $\displaystyle\sum_{i = 0}^a f(i) = f(0) + f(1) + \ldots + f(a)$ (cioè con la sommatoria definita ricorsivamente stiamo eliminando il fastidioso discorso (non formale) dei puntini \dots).
\end{example}

Altre \vocab{successioni} - \textbf{ossia funzioni con dominio $\omega$} - sono definite nella maniera più naturale proprio per ricorsione.

\begin{example}[Esempio di applicazione della ricorsione]
	In quanti modi posso coprire una sequenza di $n$ caselle $\underbrace{\square\square\square\ldots\square\square}_{n}$ con tessere di una o due caselle,
	$\square$ e $\square\square$, che non si sovrappongano e non lascino caselle scoperte?
\end{example}

\begin{soln}
	Detto $F_n$ il numero di ricoprimenti di una sequenza lunga $n$, vediamo che la tessera più a sinistra può essere $\square$ o $\square \square$. Nel primo caso, ci sono 
	$F_{n-1}$ modi di completare il ricoprimento, nel secondo caso $F_{n-2}$. Abbiamo quindi trovato una relazione ricorsiva del numero di ricoprimenti in funzione di $n$:
	\[ F_n = F_{n-1} + F_{n-2}\footnote{Cioè il numero totale di modi di ricoprire la sequenza di $n$ caselle deriva dalla somma dei due casi, che rappresentano i modi di ricoprire le altre caselle fissata quella/e iniziale/i, ciò fissati i casi base ci definisce bene (via ricorsione numerabile) una successione che conta il numero 
	di ricoprimenti in funzione di $n$.}
		\]
	La sequenza risulta completamente determinata, per ricorsione, osservando che $F_0 = F_1 = 1$: sono i \vocab{numeri di Fibonacci}.
\end{soln}

\textbf{In un certo senso, induzione e ricorsione sono due facce della stessa medaglia}: dove l'induzione dimostra $\Phi(s(n))$ assumendo di sapere 
$\Phi(n)$, la ricorsione calcola $f(s(n))$ assumendo di sapere $f(n)$. Lo stesso parallelismo, vedremo, si presenterà per l'induzione e la ricorsione transfinita.
Tornando al numerabile: come abbiamo enunciato due forme dell'induzione, enunceremo due forme della ricorsione.\\
La semplice osservazione che segue dice che due funzioni sono uguali precisamente quando assumono gli stessi valori.

\begin{remark}
	[Estensionalità per funzioni]
	Date $f,g : A \rightarrow B$, allora:
	\[ f = g \leftrightarrow \forall x \in A \; f(x) = g(x)
		\]
	(dove l'uguaglianza di funzioni non è altro che uguaglianza di sottoinsiemi in $A \times B$).
\end{remark}

\begin{proof}
	Si osserva che:
	\[ (x,y) \in f \overset{\text{def. $f$}}{\iff} y = f(x) \overset{\text{Hp.}}{\iff} y = g(x) \overset{\text{def. $g$}}{\iff} (x,y) \in g
		\]
	e si conclude per \hyperref[ax2]{estensionalità} che quanto scritto sopra equivale a dire che gli insiemi $f$ e $g$ sono uguali.
\end{proof}

\begin{notation}[Insieme delle funzioni da $A$ a $B$]
	Indichiamo con ${}^{A}B$ l'insieme delle funzioni da $A$ a $B$, che esiste per \hyperref[ax3]{separazione} in $\ps(A \times B)$.
\end{notation}

\begin{theorem}
	[Ricorsione numerabile - prima forma]
	\label{ric1}
	Dato un insieme $A$, un elemento $k \in A$\footnote{$k$ sarà il caso base della ricorsione.} e una funzione:
	\[ h : \omega \times A \longrightarrow A
		\]
	esiste un'unica funzione $f : \omega \rightarrow A$ tale che:
	\[ \forall n \in \omega \quad f(s(n)) = h(n,f(n))
		\]
\end{theorem}

\begin{example}[Potenza e fattoriale con la ricorsione numerabile]
	Per definire $a^b$ considero $k = 1$, $h(n,x) = a \cdot x$, e $h(0,x) = k = 1$. Per definire il fattoriale $k = 1$, $h(n,x) = s(n) \cdot x$ e $h(0,x) = k = 1$.
\end{example}

\begin{exercise}
	Come potrei costruire $F_n$ usando questo teorema?
\end{exercise}

\begin{proof}
	Il piano consiste nel trovare una formula $\Phi(x,y)$ che dice ``$y = f(x)$'' - questa è la vera difficoltà 
	della dimostrazione - poi semplicemente otteniamo $f$ per separazione nell'insieme $\omega \times A$ ($f$ è una funzione da $\omega$ ad $A$) usando la formula $\Phi$.\\
	Per dire ``$y = f(x)$'' diremo equivalentemente ``i primi $x$ passaggi della ricorsione, partendo da $k$, conducono a $y$''.
	Dato $x \in \omega$ diciamo che $g$ è una \vocab{$x$-approssimazione} se la vale la formula seguente:
	\[ (g \in {}^{s(x)}A ) \land (g(\emptyset) = k) \land \forall n \in x (g(s(n)) = h(n,g(n)))
		\]
	ovvero la funzione $g : \{0,\ldots,x\} \rightarrow A$ soddisfa la definizione ricorsiva di $f$, ristretta, naturalmente, al dominio $\{0,\ldots,x\}$ (cioè $s(x)$).
	Il vantaggio di tagliuzzare $f$ in $x$-approssimazioni è che così otteniamo un parametro, $x$, su cui impostare un'induzione.
	
	\begin{lemma}[Esistenza e unicità delle $x$-approssimazioni in $\omega$]
		$\forall x \in \omega \; \exists ! \, g$ ``$g$ è una $x$-approssimazione''.
	\end{lemma}

	\begin{proof}
		Induzione su $x$.
		\begin{itemize}
			\item[$\boxed{\text{caso $x = \emptyset$}}$] Basta osservare che l'unica $\emptyset$-approssimazione è la funzione $\{(\emptyset,k)\}$. Infatti il dominio
			è $\{\emptyset\}$ per definizione, e per soddisfare la definizione deve valere necessariamente $g(\emptyset) = k$, quindi l'unica $\emptyset$-approssimazione possibile è la funzione $g = \{(\emptyset,k)\}$.\\
			\item[$\boxed{\text{caso $x = s(a)$}}$] Per ipotesi induttiva abbiamo che esiste un'unica \textbf{$a$-approssimazione $g$}. Poniamo:
			\[ g' = g \cup \{(s(a), h(a,g(a)))\}
				\]
			ossia $g'(t) = g(t)$ per $t \leq a$, e $g'(s(a)) = h(a,g(a))$. È immediato verificare che $g'$ è una $s(a)$-approssimazione (l'abbiamo costruita apposta per verificare la definizione).\\
			Per verificare l'unicità, osserviamo che, date le $s(a)$-approssimazione $g'$ e $g''$, la loro restrizione a $s(a)$ è una $a$-approssimazione (per definizione), quindi, per ipotesi induttiva $g'_{|s(a)} = g = g''_{|s(a)}$.
			D'altro canto il dominio di una $s(a)$-approssimazione è $s(s(a)) = s(a) \cup \{s(a)\}$, e abbiamo detto che $g'$ e $g''$ coincidono su $s(a)$, quindi coincidono:
			\[ g'(s(a)) = h(a,g'(a)) = h(a,g''(a)) = g''(s(a))
				\]
		\end{itemize}	
	\end{proof}
	Stabilito il lemma, introdurremo la formula $\Phi$:
	\[ \Phi(x,y) \Mydef \exists g \in {}^{s(x)}A \quad \text{``$g$ è una $x$-approssimazione''} \land g(x) = y
		\]
	Per l'unicità della $x$-approssimazione $\forall x \in \omega \; \exists ! \, y \; \Phi(x,y)$, possiamo quindi definire, per ogni $x \in \omega$ e $y \in A$ la funzione via \hyperref[ax3]{separazione}:
	\[ f(x) = y \Mydef \Phi(x,y)\footnote{Formalmente $f = \{(x,y) \in \omega \times A | \Phi(x,y)\} = \{(x,y) \in \omega \times A | \exists! g \in {}^{s(x)}A \; \text{``$g$ è una $x$-approssimazione''} \land g(x) = y\}$, in altre parole,
	dato $x \in \omega$ affido alla sua (unica) $x$-approssimazione il compito di trovare un'immagine, e quindi definisco $f$ attraverso $g$ (che dipende dalla $x$ in input).}
		\]
	Occorre verificare che $f$ soddisfa le condizioni della ricorsione.
	\begin{itemize}
		\item[$\boxed{f(\emptyset) = k}$] Immediata, infatti $f(\emptyset) = g(\emptyset)$, ma abbiamo visto nel lemma che l'unica $\emptyset$-approssimazione possibile in $\omega$ è $\{(0,k)\}$ (cioè soddisfa semplicemente il caso base), quindi $f(\emptyset) = g(\emptyset) = k$.
		\item[$\boxed{f(s(n)) = h(n,f(n))}$] Per costruzione $f(s(n)) = g(s(n))$ per una (l'unica) $s(n)$-approssimazione $g$. D'altro canto $g(s(n)) = h(n,g(n))$ (per definizione di $s(n)$-approssimazione).
		Ora $g_{|s(n)}$ è una $n$-approssimazione, quindi $g_{|s(n)} (n) = g(n) \overset{\text{def.}}{=} f(n)$. Mettendo tutto insieme:
		\[ f(s(n)) \overset{\text{def.}}{=} g(s(n)) \overset{\text{def. $g$}}{=} h(n,g(n)) \overset{\text{def. + oss.}}{=} h(n, f(n))
			\]
	\end{itemize}
	Ciò dimostra che una $f$ ottenuta per separazione come abbiamo visto esiste e soddisfa la tesi del teorema di ricorsione numerabile.
	L'unicità di $f$ segue facilmente per induzione, Date $f'$ e $f''$ che soddisfano la ricorsione abbiamo:
	\[ f'(\emptyset) = k = f''(\emptyset) \qquad f'(s(n)) = h(n,f'(n)) \overset{\text{Hp. indutt.}}{=}\footnote{E usando l'estensionalità per funzioni su $h$.} h(n,f''(n)) = f''(s(n))
		\]
	e per estensionalità di funzioni si conclude che $f' = f''$.
\end{proof}

Procedendo come negli esempi all'inizio di questa sezione, il \hyperref[ric1]{teorema di ricorsione numerabile} ci consente di costruire le operazioni aritmetiche, le potenze, etc.
A titolo di esempio, vediamo nel dettaglio, il caso della somma.

\begin{example}
	[Costruzione di $+ : \omega \times \omega \rightarrow \omega$]
	Vogliamo formalizzare la definizione:
	\[ \begin{cases}
		a + \textcolor{red}{0} = 0\\
		a + \textcolor{red}{s(b)} = s(a+b)
	\end{cases}
		\]
	Per il \hyperref[ric1]{teorema di ricorsione numerabile} sappiamo che, per ogni $a \in \omega$ fissato, esiste un'unica $f : \omega \rightarrow \omega$ tale che:
	\[ f(\textcolor{red}{0}) = a \land \forall b \in \omega \; f(\textcolor{red}{s(b)}) = s(f(b))
		\]
	Scriviamo quindi:
	\[ a + x = y \Mydef \exists f \in {}^{\omega}\omega \;\; f(0) = a \; \land  \; f(x) = y \; \land \; \forall b \in \omega \; f(s(b)) = s(f(b))
		\]
\end{example}

L'applicazione che segue chiude il conto che abbiamo lasciato aperto con gli assiomi di Peano. Dimostriamo che essi identificano un'unica struttura a meno di isomorfismi, quindi $\omega$ è 
a buon diritto, l'insieme dei numeri naturali.

\begin{theorem}[Unicità dei numeri naturali]
	Supponiamo che $(\NN, 0, \text{succ})$ soddisfi gli assiomi di Peano, allora $(\NN, 0, \text{succ})$ \textbf{e} $(\omega,\emptyset,s)$ sono strutture isomorfe - \textbf{ossia, formalmente, esiste}:
	$ f : \omega \rightarrow \NN$ \textbf{bigettiva} tale che:
	\begin{enumerate}[(i)]
		\item $f(\emptyset) = 0$.
		\item $\forall n \in \omega \; f(s(n)) = \text{succ}(f(n))$.\footnote{Cioè è una bigezione tra insiemi, che rispetta lo 0 e la funzione successore che abbiamo definito.}
	\end{enumerate}
\end{theorem}

Fa comodo isolare la seguente osservazione.

\begin{remark}[Ogni numero in $\omega\setminus\emptyset$ è successore]
	$\forall x \in \omega \; x \ne 0 \rightarrow \exists y \in \omega \; x = s(y)$, ovvero ogni numero diverso da 0 è il successore di qualcos'altro.
\end{remark}

\begin{proof}
	Induzione su $x$. Il caso $x = 0$ è vero a vuoto (essendo la premessa sempre automaticamente falsa). Nel caso $x = s(m)$ basta prendere $y = m$ e si ha $x = s(y)$.
\end{proof}

Dimostriamo ora il teorema.

\begin{proof}
	Per il \hyperref[ric1]{teorema di ricorsione} (stiamo prendendo $A = \NN$, e $k = 0$ e $h = \text{succ}$) c'è un'unica $f$ che soddisfa le condizioni $f(\emptyset) = 0$ e $\forall n \in \omega \; f(s(n)) = \text{succ}(f(n))$.
	Resta da constatare che $f$ è bigettiva.
	\begin{itemize}
		\item[\boxed{\text{Surgettività}}] Per ipotesi $(\NN, 0, \text{succ})$ soddisfa il principio di induzione (poiché soddisfa gli assiomi di Peano). Dimostriamo quindi per induzione in $(\NN, 0, \text{succ})$ che $\forall y \in \NN \; \exists x \in \omega \; f(x) = y$.
		\begin{itemize}
			\item[$\boxed{\text{caso $y = 0$}}$] Basta osservare che $f(\emptyset) = 0$ per costruzione.
			\item[$\boxed{\text{caso $y = \text{succ}(n)$}}$] Per ipotesi induttiva esiste $x \in \omega$ tale che $f(x) = n$, da cui si ottiene, per definizione di $f$ che $f(s(x)) = \text{succ}(n)$.
		\end{itemize}
		\item[\boxed{\text{Iniettività}}] Consideriamo, per assurdo, il minimo $x \in \omega$ tale che, per qualche $y \in \omega$ con $y \ne x$, $f(x) = f(y)$.
		Osserviamo che, per la minimalità di $x$, $x<y$, quindi, in particolare $y \ne \emptyset$, e per l'osservazione possiamo scrivere $y = s(y')$. Procediamo quindi per induzione su $x$
		nel trovare un assurdo per ogni $x \in \omega$.
		\begin{itemize}
			\item[$\boxed{\text{caso $x = \emptyset$}}$] In questo caso si deve avere che:
			\[
				\text{succ}(f(y')) \overset{\text{def. $f$}}{=} f(s(y'))
								   \overset{y = s(y')}{=} f(y)
								   \overset{\text{Hp.}}{=} f(x) = 0
				\]
			che equivale a dire che 0 è successore di qualche numero contraddicendo l'osservazione (che vale anche per $(\NN, 0, \text{succ})$, in quanto soddisfa gli assiomi di Peano per ipotesi).
			\item[$\boxed{\text{caso $x \ne \emptyset$}}$] Per l'osservazione possiamo scrivere $x = s(x')$, da cui:
			\[ \text{succ}(f(x')) = f(s(x')) = f(x) = f(y) = f(s(y')) = \text{succ}(f(y'))
				\]
			e, per l'assioma \hyperref[a]{(a)} (iniettività del successore) in $(\NN, 0, \text{succ})$, segue che $f(x') = f(y')$. Allora, per la minimalità di $x$, siccome $x' < x$, dobbiamo avere $x' = y'$ (avevamo posto per ipotesi $x$ come minimo per cui c'è un elemento distinto $y$ che
			ha la stessa immagine, quindi qualsiasi cosa abbia la stessa immagine e sia più piccola di $x$ deve essere unica).
			Ma da questo seguirebbe $x = s(x') = s(y') = y$, contro l'ipotesi $\lightning$
		\end{itemize}
	\end{itemize}
\end{proof}

Se, infine, volgiamo la nostra attenzione all'esempio dei numeri di Fibonacci, vediamo che non è possibile definire questa sequenza applicando il \hyperref[ric1]{teorema di ricorsione}
in maniera diretta, perché $F_n$ non dispense solo dal termine precedente della sequenza, $F_{n-1}$, ma anche da $F_{n-2}$. Ce la si potrebbe cavare con un trucco, per esempio definendo la funzione
$n \mapsto (F_n,F_{n+1})$ da $\omega$ a $\omega \times \omega$. È comodo, però, disporre di una versione più versatile del teorema di ricorsione numerabile.

\begin{theorem}
	[Ricorsione numerabile - seconda forma]
	Dato un insieme $A$, denotiamo con $A^*$ l'insieme delle funzioni $g \subseteq \omega \times A$ con $\Dom(g) \in \omega$\footnote{Cioè è un numero di $\omega$.}. Sia $h : A^* \rightarrow A$, allora esiste un'unica funzione
	$f: \omega \rightarrow A$ tale che:
	\[ \forall n \in \omega \; f(n) = h(f_{|n}) \footnote{In altre parole, $f(n)$, può dipendere in maniera arbitraria dai valori assunti da $f$ sui numeri minori di $n$. Cioè $h$ è una funzione che manda funzioni che hanno come 
	dominio un $n \in \omega$ in $A$, in particolare $h(f_{|n})$ è una funzione di funzioni con dominio in $\omega$.}
		\]
\end{theorem}

\begin{example}
	[Esempio di applicazione]
	Per costruire la successione di Fibonacci, definiamo $h(g)$ in questo modo. Sia $n = \Dom(g)$. Se $n = \emptyset$ o $n = 1$, allora $h(g) = 1$.
	Altrimenti esistono $n-1, n-2 \in \omega$ tali che $s(n-1) = s(s(n-2)) = n$. Definiamo quindi $h(g) = g(n-1) + g(n-2)$\footnote{Abbiamo quindi ottenuto $h$ come funzione di funzioni con dominio in $\omega$ e in particolare più piccolo di $n$, dunque per il teorema tale $h$ definisce univocamente $f(n)$, a partire da $f_{|n} \in A^*$.}.
\end{example}

\begin{proof}
	L'idea è di definire, mediante la prima forma del \hyperref[ric1]{teorema di ricorsione}, la successione della troncata di $f$. Ossia la funzione $f' : n \mapsto f_{|n}$ (che manda $f$ nella sua restrizione al dominio $\{0,\ldots,n-1\}$) - un modo alternativo, sarebbe 
	ripetere la dimostrazione della prima forma -. Procediamo nel primo modo e costruiamo per ricorsione - prima forma - la funzione $f' : \omega \rightarrow A^*$ tale che:
	\[ f'(\emptyset) = \emptyset \qquad f'(s(n)) = f'(n) \cup \{(n,h(f'(n)))\}\,\footnote{Esiste ed è unica per il primo teorema di ricorsione}
		\]
	Ora poniamo $f(n) := f'(s(n))(n)$ ($f' \in A^*$, quindi è una funzione com dominio in $\omega$, quindi $f : \omega \rightarrow A$ è ben definita) e verifichiamo per induzione che effettivamente $f'$ sia la successione della troncata di $f$, cioè $\forall n \in \omega \; f_{|n} = f'(n)$.
	\begin{itemize}
		\item[$\boxed{\text{caso $n = 0$}}$] Si vede subito che $f_{|0} = f'(\emptyset)(n) = \emptyset$ (per come l'abbiamo costruita).
		\item[$\boxed{\text{caso $n = s(m)$}}$] In questo caso abbiamo:
		\[ \begin{split}
			f_{|s(m)} & = f_{|m} \cup \{(m,f(m))\}\\
					& = f'(m) \cup \{(m,f'(s(m))(m))\}\\
					& = f'(m) \cup \{(m,h(f'(m)))\} = f'(s(m))
		\end{split}
			\]
	\end{itemize}
	dove la prima uguaglianza segue per definizione di funzione (successione in questo caso specifico), la seconda per com'è definita $f$ in funzione di $f'$ e l'ultima per la definizione ricorsiva di $f'$. Infine, quindi, $f(n) \overset{\text{def.}}{=} f'(s(n))(n) = h(f'(n)) = h(f_{|n})$ (dove l'ultima uguaglianza segue per quanto abbiamo dimostrato).
\end{proof}

Abbiamo ora terminato di dimostrare le proprietà di base dei numeri naturali. Da qui, prende le mosse il corso di aritmetica. Nella prossima sezione, inizieremo
lo studio di un concetto squisitamente insiemistico: la cardinalità.

\begin{exercise}
	Dimostra commutatività, associatività, etc. di $+$ e $\cdot$.
\end{exercise}

\newpage
\section{Cardinalità}
Il concetto di cardinalità è, forse, il modo più semplice di contare gli elementi di un insieme: diciamo che due insiemi hanno un ugual numero di elementi se esiste
una corrispondenza biunivoca fra di essi.

\begin{definition}[Equipotenza/Cardinalità]
	Dati due insiemi $A$ e $B$:
	\[ |A| = |B| \Mydef \exists f \in {}^A B \; \text{``$f$ è bigettiva $A \rightarrow B$''}
		\]
	diciamo anche che ``$A$ ha la stessa \vocab{cardinalità} di $B$'' o che ``$A$ e $B$ sono \vocab{equipotenti}''. Poniamo inoltre:
	\[ |A| \leq |B| \Mydef \exists B' \subseteq B \; |A| = |B'| 
		\]
	ossia $\exists f \in {}^A B$ ``$f$ è iniettiva'' (la definizione ci dice proprio che esiste un sottoinsieme di $B$ che è in bigezione con $A$, e per definizione di iniettività, si ha proprio che $A \hookrightarrow B$)\footnote{Tale relazione sarà anche una relazione di ordine tra cardinalità quando queste ultime saranno singoli oggetti della teoria.}.
\end{definition}

\begin{note}[Sulla notazione per le cardinalità]
	Osserviamo che:
	\begin{itemize}
		\item La scrittura $|A| = |B|$ suggerisce che esistono insiemi - o oggetti di qualche genere - denotati $|A|$ e $|B|$ di cui si predica l'uguaglianza.
		Effettivamente costruiamo questi oggetti, ma, per ora, la scrittura $|A| = |B|$ è inscindibile, come \ding{168}$[A,B)$ (nel senso che per ora è solo un'abbreviazione per dire bigezione, pertanto non possiamo separare quei simboli o farci qualcosa).
		\item Potrebbe sorgere il sospetto che se $|A|\textcolor{red}{<}|B|$ quando $A \subsetneq B$, ma non è così, come mostra l'esempio di $A = \{x \in \omega | x > 0\}$ e $B = \omega$, infatti $A \subsetneq B$, ma $|A| = |B|$.
	\end{itemize}
\end{note}

\begin{remark}[Proprietà formali di una relazione di equivalenza]
	La relazione $|\cdot| = |\cdot|$ soddisfa le proprietà formali di una relazione di equivalenza (ma per ora NON lo è\footnote{Potrebbe tuttavia essere pensata come una relazione di equivalenza su $V$ (la classe di tutti gli insiemi).}):
	\begin{itemize}
		\item \textbf{riflessività}: $|A| = |A|$.
		\item \textbf{simmetria}: $|A| = |B| \rightarrow |B| = |A|$.
		\item \textbf{transitività}: $|A| = |B| \land |B| = |C| \rightarrow |A| = |C|$.
	\end{itemize}
\end{remark}

\begin{exercise}
	Dimostrare l'osservazione.
\end{exercise}

\begin{soln}
Per la riflessività basta osservare che $\id_A$ è una bigezione da $A$ in $A$. Per la simmetria, abbiamo visto che se $f : A \rightarrow B$ è iniettiva, allora ammette
inversa $g : \Imm(f) \rightarrow A$ a sua volta iniettiva (e surgettiva poiché ha necessariamente come immagine tutto $A$), inoltre, essendo $f$ bigettiva si ha che $\Imm(f) = B$, quindi $g : B \rightarrow A$, e 
per quanto detto è bigettiva, dunque nel linguaggio della cardinalità $|B| = |A|$.\\
Infine, $|A| = |B| \iff \exists f : A \rightarrow B$ bigettiva, $|B| = |C| \iff \exists g : B \rightarrow C$ bigettiva, ora è sufficiente osservare che $g \circ f : A \rightarrow C$ è bigettiva in quanto composizione di funzioni bigettive\footnote{È una semplice verifica.}, per avere $|A| = |C|$.
\end{soln}

\begin{remark}[Proprietà formali $\lbrack$parziali$\rbrack$ di una relazione di ordine $\lbrack$largo$\rbrack$]
	La relazione $|\cdot| \leq |\cdot|$ soddisfa\footnote{Tali proprietà, unite al teorema di Cantor-Bernstein, che stiamo per vedere, ci danno una relazione di ordine totale su $V$.}:
	\begin{itemize}
		\item \textbf{riflessività}: $|A| \leq |A|$.
		\item \textbf{transitività}: $|A| \leq |B| \land |B| \leq |C| \rightarrow |A| \leq |C|$.
	\end{itemize}
\end{remark}

\begin{exercise}
	Dimostrare l'osservazione.
\end{exercise}

\begin{soln}
Per la riflessività basta osservare che $\id_A$ è in particolare una mappa iniettiva (oppure che $A$ è un sottoinsieme [improprio] di se stesso e quindi l'identità è la bigezione richiesta dalla definizione).
Per la transitività $|A| \leq |B| \iff \exists A \hookrightarrow B$, $|B| \leq |C| \iff \exists g : B \hookrightarrow C$, e osservando che la composizione di funzioni iniettive è iniettiva, si ha che $g \circ f : A \rightarrow C$ è iniettiva $\iff |A| \leq |C|$.
\end{soln}

Per stabilire che le cardinalità sono, formalmente, ordinate dalla relazione $|\cdot| \leq |\cdot|$, ci manca l'antisimmetria, che è appunto enunciata dal teorema seguente.

\subsection{Teorema di Cantor-Bernstein}

\begin{theorem}
	[Cantor-Bernstein]
	\label{CB}
	Se c'è una funzione iniettiva $A \rightarrow B$ e una funzione iniettiva $B \rightarrow A$, allora esiste una bigezione fra $A$ e $B$.
	\[ \forall A,B (|A| \leq |B| \land |B| \leq |A|) \rightarrow |A| = |B|
		\]
\end{theorem}

\begin{proof}
	Per ipotesi abbiamo quindi $f: A \rightarrow B$ e $g: B \rightarrow A$ iniettive. Il nostro obiettivo è costruire una nuova funzione $h : A \rightarrow B$ bigettiva.\\
	L'idea è che ogni elemento, poniamo, di $A$, è una tappa di un percorso:
	\[ a \overset{f}{\longrightarrow} f(a) \overset{g}{\longrightarrow} g(f(a)) \overset{f}{\longrightarrow} f(g(f(a))) \overset{g}{\longrightarrow} \dots
		\]
	Siccome $f$ e $g$ sono iniettive, questo percorso ha altresì un'unica estensione all'indietro (abbiamo visto che se le funzioni sono iniettive, allora ammettono un'inversa iniettiva dalle rispettive immagini (che è anche surgettiva), dunque possiamo sempre tornare indietro in modo unico, estendendo quindi 
	il nostro percorso anche nell'altra direzione):
	\[ \textcolor{red}{f^{-1}(g^{-1}(a))} \overset{f}{\longrightarrow} \textcolor{red}{g^{-1}(a)} \overset{g}{\longrightarrow} a \overset{f}{\longrightarrow} f(a) \overset{g}{\longrightarrow} g(f(a)) \overset{f}{\longrightarrow} f(g(f(a))) \overset{g}{\longrightarrow} \ldots
		\]
	a patto che $a \in \Imm(g)$ (perché l'inversa $g^{-1}$ va da $\Imm(g)$ a $B$), $g^{-1}(a) \in \Imm(f)$, etc. Quando, e se, non possiamo più applicare la funzione inversa, il percorso (all'indietro) si interrompe. Con questa catena di composizioni ci sono quindi tre tipi di percorsi possibili:
 	\begin{center}
		\begin{figure}[H]
			\centering
			\includegraphics[width=12.5cm]{immagini/cantor-ber1a.png}
		\end{figure}
	\end{center}
	\begin{center}
		\begin{figure}[H]
			\centering
			\includegraphics[width=12.5cm]{immagini/cantor-ber1b.png}
		\end{figure}
	\end{center}
	
	Per gli elementi che si trovano su un percorso circolare, o su un percorso illimitato avanti e indietro, $f$ fornisce una bigezione, come la fornirebbe anche $g^{-1}$ - la scelta è arbitraria a patto di 
	usare la medesima funzione per l'intero percorso - nel modo seguente\footnote{Informalmente, se siamo in uno dei due casi, allora $f$ è per forza una mappa bigettiva, perché è iniettiva e ``prende'' tutti gli elementi in arrivo, idem $g^{-1}$.}:
	\begin{center}
		\begin{figure}[H]
			\centering
			\includegraphics[width=12.5cm]{immagini/cantor-ber2a.png}
		\end{figure}
	\end{center}
	\begin{center}
		\begin{figure}[H]
			\centering
			\includegraphics[width=11.5cm]{immagini/cantor-ber2b.png}
		\end{figure}
	\end{center}
	Per i percorsi, invece, illimitati solo a destra, occorre vedere in quale insieme sta l'elemento iniziale del percorso: se questo è in $A$, la bigezione è data da $f$,
	altrimenti se sta in $B$ la bigezione è data da $g^{-1}$.
	\begin{center}
		\begin{figure}[h!]
			\centering
			\includegraphics[width=12.5cm]{immagini/cantor-ber3.png}
		\end{figure}
	\end{center}
	Per comodità poniamo quindi [la bigezione $h$], $h(x) = f(x)$ in ogni caso, eccetto quando $x$ è lungo un percorso che parte da $B$, nel cui caso poniamo $h(x) = g^{-1}(x)$.\\
	Formalmente, definiamo per \hyperref[ric1]{ricorsione} (prima forma) le seguenti successioni di sottoinsiemi di $B$ e $A$ rispettivamente - ossia, tecnicamente, la funzione $\omega \rightarrow \ps(B) \times \ps(A) : i \mapsto (B_i,A_i)$, con:
	\[ B_0 = B \setminus\Imm(f) \qquad A_i = g[B_i] \qquad B_{s(i)} = f[A_i]
		\]
	(ovvero la successione dei $B_i$ è definita con la prima forma della ricorsione, mentre quella degli $A_i$ dipende semplicemente da quest'ultima, ma non direttamente per ricorsione). Definiamo quindi:
	\[ B_* = \bigcup_{i \in \omega}B_i \Mydef \bigcup\{B_i | i \in \omega\} \qquad A_* = \bigcup_{i \in \omega} A_i
		\]
	Questi sono i punti che appartengono a cammini che partono da $B$, definiamo quindi $h : A \rightarrow B$ e $k : B \rightarrow A$ come segue:
	\[ h(x) = \begin{cases}
		g^{-1}(x) &\text{se $x \in A_*$}\\
		f(x) &\text{altrimenti}
	\end{cases}
	\qquad
	k(y) = \begin{cases}
		g(y) &\text{se $y \in B_*$}\\
		f^{-1}(y) &\text{altrimenti}
	\end{cases}
		\]
	queste mappe coprono tutti i casi  possibili, infatti, i percorsi ciclici e illimitati da entrambe le parti sono coperti da $k = f^{-1}$ ed $h = g^{-1}$, mentre nel caso di percorsi che partono da $B$ e sono limitati a sinistra, ovvero con primo elemento in $B^*$ abbiamo che $k(y) = g(y)$, invece nel caso simmetrico, 
	in cui si parte da $A$ con percorso limitato a sinistra si ha $h(x) = f(x)$, in tal modo prendiamo tutti gli elementi di tutti i cicli possibili che si formano nei due insiemi usando i percorsi descritti sopra.\\
	Ci basta quindi dimostrare che $h$ e $k$ sono ben definite, $k \circ h = \id_A$ e $h \circ k = \id_B$, in tal modo avremo la nostra bigezione (e la sua inversa).
	\begin{itemize}
		\item[$\boxed{\text{$h$ e $k$ ben definite}}$] Occorre verificare che stiamo applicando $g^{-1}$ e $f^{-1}$ a elementi della immagine di $g$ e $f$ rispettivamente.
		Nella definizione di $h$, se $x \in A_*$, allora $x \in A_i$, per qualche $i \in \omega$, quindi $x \in g[B_i] \subseteq \Imm(g)$. Nella definizione di $k$, se $y \not \in B_*$, in particolare,
		$y \not \in B_0$, per cui $y \in \Imm(f)$.
		\item[$\boxed{k \circ h = \id_A}$] Se $x \in A_*$, allora $x \in A_i$, per qualche $i \in \omega$, quindi $x = g(y)$, con $y \in B_i$, per cui $k(h(y)) = k(g^{-1}(x)) = k(y) = g(y) = x$ (abbiamo usato che $y = g^{-1}(x) \in B_*$ per quanto supposto sopra).\\
		Per il caso $x \not \in A_*$, osserviamo, intanto, che $x \not \in A_* \implies f(x) \not \in B_*$. Infatti, se $f(x) \in B_i$, con $i \in \omega$, allora $i \ne 0$, perché $B_0 = B \setminus \Imm(f)$, quindi possiamo scrivere $i = s(j)$, 
		e $f(x) \in B_{s(j)} = f[A_j]$. Per l'iniettività di $f$, abbiamo allora $x \in A_j \;\lightning$\\
		Di conseguenza, se $x \not \in A_*$, $k(h(x)) = k(f(x)) \overset{f(x) \not \in B_*}{=} f^{-1}(f(x)) = x$.
		\item[$\boxed{h \circ k = \id_B}$] Se $y \in B_*$, allora $y \in B_i$, per qualche $i \in \omega$, quindi $g(y) \in A_i$. Di conseguenza $h(k(y)) = h(g(y)) = g^{-1}(g(y)) = y$. Altrimenti $y \not \in B_*$ e, se $f^{-1}(y)\in A_*$, avremmo una contraddizione,
		perché $f^{-1}(y) \in A_i \rightarrow y = f(f^{-1}(y)) \in A_{s(i)}$. Quindi $h(k(y)) = h(f^{-1}(y)) = f(f^{-1}(y)) = y$.
	\end{itemize}
\end{proof}

Visto che $|\cdot|\leq|\cdot|$ ha le proprietà formali di una relazione d'ordine fra le classi di equivalenza della relazione $|\cdot| = |\cdot|$, possiamo definire il corrispondente ordine stretto.

\begin{definition}[Ordinamento stretto fra cardinalità]
	Dati due insiemi $A$ e $B$ definiamo:
	\[ |A| < |B| \Mydef |A| \leq |B| \land |A| \ne |B| \, \footnote{Dove ricordiamo che $|A| \ne |B| \Mydef \neg(|A| = |B|)$.}
		\]
\end{definition}

\subsection{Teorema di Cantor}

\begin{theorem}[Cantor]
	\label{cantor}
	Dato un qualunque insieme $A$ vale:
	\[ |A| < |\ps(A)|
		\]
\end{theorem}

La dimostrazione di questo enunciato è, ancora una volta, il medesimo argomento del paradosso di Russell.

\begin{proof}
	La disuguaglianza $|A| \leq |\ps(A)|$ è facile: basta considerare la funzione iniettiva:
	\[ A \longrightarrow \ps(A) : x \longmapsto \{x\}
		\]
	(che è iniettiva per \hyperref[ax2]{estensionalità}). Consideriamo, ora, una qualunque funzione $f : A \rightarrow \ps(A)$ iniettiva.
	Dobbiamo dimostrare che $\Imm(f) \subsetneq \ps(A)$ (cioè che non è surgettiva). Consideriamo:
	\[ B = \{x \in A | x \not \in f(x)\} \, \footnote{$f(x) \in \ps(A)$, ovvero è un sottoinsieme di $A$, quindi stiamo considerando il sottoinsieme degli elementi di $A$ che non stanno nelle loro immagini (dei sottoinsiemi di $A$).}
		\]
	Ora $B \subseteq A$, supponendo per assurdo che $f$ sia bigettiva, ovvero che $B = f(a)$ per qualche $a \in A$, avremmo:
	\[ a \in f(a) \subseteq A \iff a \in B \iff a \not \in f(a) \; \lightning
		\]
\end{proof}

\subsection{Operazioni fra cardinalità}

\begin{definition}[Somma, prodotto e potenze di cardinalità]
	Dati $A$ e $B$ possiamo definire somma, prodotto e potenze di cardinalità come segue:
	\[ \begin{split}
		|A| + |B| &\Mydef |A \sqcup B| \Mydef |(A \times \{0\}) \cup (B \times \{1\})| \\
		|A|\cdot |B| &\Mydef |A \times B| \\
		|A|^{|B|} &\Mydef |{}^{B}A|
	\end{split}
		\]
	(nella definizione di unione disgiunta abbiamo fatto il prodotto per cose diverse, in modo che gli elementi comuni ai due insiemi sono comunque diversi per la seconda componente, e quindi siano contati due volte.)
\end{definition}

Osserviamo che le operazioni fra cardinalità così date sono ben definite.

\begin{proposition}[Buona definizione delle operazioni]
	Le operazioni di somma, prodotto e potenza fra cardinalità sono ben definite, ossia dati $A,B,A',B'$, con $|A| = |A'|$ e $|B| = |B'|$, vale:
	\[ |A| + |B| = |A'| + |B'| \qquad |A|\cdot|B| = |A'|\cdot|B'| \qquad |A|^{|B|} = |A'|^{|B'|}
		\]
\end{proposition}

\begin{proof}
	Date $f : A \rightarrow A'$ e $g : B \rightarrow B'$ bigettive, è immediato verificare che le seguenti sono bigezioni:
	\[  \begin{split}
		A \sqcup B \longrightarrow A' \sqcup B' :\,& (a,0) \longmapsto (f(a),0) \\
											 \,& (b,1) \longmapsto (g(b),1) \\
	 	A \times B \longrightarrow A' \times B' :\,& (a,b) \longmapsto (f(a),g(b)) \\
		{}^{B}A \longrightarrow {}^{B'}A' :\,& h \longmapsto f \circ h \circ g^{-1} \, \footnote{Deriva dal diagramma commutativo che si può disegnare.}
		\end{split}
		\]
	ed equivalgono alle uguaglianze di cardinalità nella tesi.
\end{proof}

\begin{notation}[Cardinalità finite]
Riferendoci alle cardinalità finite $|\emptyset|,|1|,|2|,\ldots$ se non c'è rischio di confusione, scriveremo semplicemente $0,1,2,\ldots$
\end{notation}

\begin{remark}[Teorema di Cantor rivisitato]
	$|\ps(A)| = 2^{|A|}$, per cui il \hyperref[cantor]{teorema di Cantor}, può essere enunciato dicendo che, dato un qualunque $A$, vale $|A| < 2^{|A|}$.
\end{remark}

Verifichiamo che effettivamente ci sia una bigezione tra l'insieme delle parti di $A$ e quello delle funzioni da $A$ in 2.

\begin{proof}
	La funzione che ad ogni $B \in \ps(A)$ associa la sua \vocab{funzione indicatrice} $\chi_B : A \rightarrow 2$ è definita da:
	\[ \chi_B(x) = \begin{cases}
		1 &\text{se $x \in B$}\\
		0 &\text{altrimenti}
	\end{cases}
		\]
	ed è una bigezione $\ps(A) \rightarrow {}^{A}2$ (ovvero $|\ps(A)| = |{}^{A}\{0,1\}| = |{}^{A}2|$ per la nostra codifica dei naturali, e per la definizione data prima la seconda cardinalità corrisponde proprio all'operazione $|2|^{|A|}$).
\end{proof}

\begin{proposition}[Proprietà delle operazioni fra cardinalità]
	Le operazioni fra cardinalità godono delle proprietà seguenti: denotando, per brevità, con $\alpha,\beta,\gamma$ i simboli: $|A|,|B|,|C|$:
	\begin{align*}
		&\alpha + 0 = \alpha 	 & \quad & \alpha + \beta = \beta + \alpha 		 	& \quad & \alpha + (\beta + \gamma) = (\alpha + \beta) + \gamma \\
		&\alpha \cdot 0 = 0 	 & \quad & \alpha \cdot \beta = \beta \cdot \alpha 	& \quad & \alpha \cdot (\beta \cdot \gamma) = (\alpha \cdot \beta) \cdot \gamma \\
		&\alpha \cdot 1 = \alpha & \quad & \alpha \cdot (\beta + \gamma) = \alpha \cdot \beta + \alpha \cdot \gamma \\
		&\alpha^0 = 1 			 & \quad & (\alpha^\beta)^\gamma = \alpha^{\gamma \cdot \beta} & \quad & (\alpha \cdot \beta)^\gamma = \alpha^\gamma \cdot \beta^\gamma \\
		&1^\alpha = 1			 & \quad & \alpha^{\beta + \gamma} = \alpha^\beta \cdot \alpha^\gamma
	\end{align*}
\end{proposition}

\begin{proof}
	In ciascun caso, si tratta semplicemente di esibire una bigezione esplicita fra il membro di sinistra e il membro di destra. Come esempio, vediamo uno dei casi più complicati, il resto 
	è lasciato come \textcolor{red}{esercizio}. \\
	Dimostriamo che $(|A|^{|D|})^{|C|} = |A|^{|C|\cdot|B|}$. Dobbiamo esibire una bigezione fra l'insieme ${}^C({}^BA)$ delle funzioni che ad ogni elemento di $C$ associano una funzione $B \rightarrow A$, e l'insieme ${}^{C \times B}A$, delle funzioni 
	che ad ogni coppia di elementi in $C \times B$ associano un elemento di $A$. Associamo a $f \in {}^C({}^BA)$ la funzione $\widetilde{f} \in {}^{C \times B}A$ definita da:
	\[ \widetilde{f}(c,b) = (\underbrace{f(c)}_{\in {}^{B}A})(\underbrace{b}_{\in B}) \, \footnote{Cioè la mappa $\sim$ prende una funzione da $C$ a ${}^{B}A$ e la manda in un'altra che prende coppie di elementi in $C \times B$, e valuta il primo elemento in $f$ 
	per ottenere una mappa da $B$ a $A$, che poi valuta in $b \in B$.}
		\]
	Dimostriamo che l'inversa di questa applicazione associa a $g \in {}^{C \times B}A$ la funzione $\overline{g} \in {}^{C}({}^B A)$ definita da:
	\[	\overline{g}(c) : B \longrightarrow A : b \longmapsto g(c,b) \, \footnote{Ovvero la mappa $-$ associa una mappa di ${}^{C\times B}A$ con la mappa $\overline{g} \in {}^{C}({}^B A)$, che valutata in $c \in C$, dà una funzione da $B$ in $A$, che ad ogni $b \in B$ associa $g(c,b)$.}
		\]
	La verifica è facilissima, presa $g \in {}^{C \times B}A$ si ha:
	\[ \forall (c,b) \in C \times B \quad \widetilde{\overline{g}}(c,d) = (\overline{g}(c))(b) = g(c,b) \implies \widetilde{\overline{g}} = g
		\]
	(quindi $\sim \circ -$ è l'identità). Presa $f \in {}^{C}({}^B A)$, e fissato un qualunque $c \in C$, si ha:
	\[ \forall b \in B \,(\overline{\widetilde{f}}(c)(b)) = \widetilde{f}(c,b) = (f(c))(b) \implies \overline{\widetilde{f}}(c) = f(c)
		\]
	da cui, per l'arbitrarietà di $c$, $\overline{\widetilde{f}} = f$ (e quindi $- \circ \sim$ è l'identità).
\end{proof}


\newpage
\section{Cardinalità finite}
Ora inizia una breve carrellata fra le cardinalità più facile da definire. Parliamo qui di cardinalità finite, poi introdurremo la cardinalità numerabile e la cardinalità del continuo.

\begin{definition}[Insieme finito/infinito]
	Diciamo che $A$ è \vocab{finito} se $\exists n \in \omega \, |A| = |n|$. Se $A$ non è finito, diciamo che $A$ è \vocab{infinito}.
\end{definition}

Storicamente, è riflessiva una definizione alternativa di finitezza, data originariamente da Dedekind.

\begin{definition}[Dedekind-finitezza]
	Diciamo che $A$ è \vocab{Dedekind-finito} se non può essere messo in corrispondenza biunivoca con un suo sottoinsieme proprio. Ossia $A$ è Dedekind-finito se:
	\[ \forall B \subsetneq A \, |B| < |A|
		\]
\end{definition}

\subsection{Principio dei cassetti}
Con gli assiomi introdotti fino ad ora, possiamo solo dimostrare che finito $\rightarrow$ Dedekind-finito, mentre l'implicazione inversa è conseguenza dell'assioma della scelta.

\begin{proposition}[Principio dei cassetti - ossia - finito $\rightarrow$ Dedekind-finito]
	\label{cassetti}
	Dato $A$ finito e $B$ un sottoinsieme proprio di $A$, $B \subsetneq A$, vale $|B| < |A|$. 
\end{proposition}

\begin{proof}
	Naturalmente $|B| \leq |A|$ vale perché l'identità $\id_B$ è una funzione iniettiva $B \rightarrow A$. Occorre quindi dimostrare che $|B| \ne |A|$.\\
	Supponiamo per assurdo che $|B| = |A|$. Osserviamo che, senza perdita di generalità, possiamo assume $A = n \in \omega$\footnote{Quello che faremo è proprio portare $B \subsetneq A$ in $f[B] \subsetneq f[A] = n$ e qui trovare l'assurdo, che è come assumere sempre che 
	$A = n$, perché possiamo sempre spostare il problema in $\omega$ con una bigezione.}. Per ipotesi, infatti esiste $f : A \rightarrow n$ bigettiva, per un opportuno $n \in \omega$.
	Quindi $f[B] \subsetneq n$ (volendo perché la restrizione di $f$ a $B$ è ancora iniettiva ma non surgettiva\footnote{È conseguenza del fatto che $B$ sia un sottoinsieme proprio e che $f$ è bigettiva.}, quindi non può
	avere in arrivo tutto $n$). D'altro canto, per l'iniettività di $f$, $|f[B]| = |B| \overset{\text{Hp. assurda}}{=} |A| = n$.
	Ci basta quindi dimostrare per induzione su $n$, che:
	\[ \forall n \in \omega \; \forall B \subseteq n (|B|=|n|\longrightarrow B = n)
		\]
	(cioè che ogni sottoinsieme di un numero naturale con la stessa cardinalità è il numero stesso) in questo modo avremmo $f[B] = f[A] = n$ (prima avevamo un sottoinsieme di $A$ non di $n$), che è assurdo in quanto abbiamo detto che $f[B] \subsetneq n$.
	\begin{itemize}
		\item[$\boxed{\text{caso $n = \emptyset$}}$] Necessariamente $B = \emptyset$, quindi $B = n$ come richiesto dalla tesi.
		\item[$\boxed{\text{caso $n = s(m)$}}$] L'ipotesi induttiva è $\forall C \subseteq m \; |C| = |m| \rightarrow C = m$, vogliamo dimostrare che $\forall B \subseteq s(m) \; |B| = |s(m)| \rightarrow B = s(m)$.\\
		Sia $f : s(m) \rightarrow B$ bigettiva (come ipotesi antecedente). Si danno due casi. Se $f(m) = m$, allora sia $C := \Imm(f_{|m})$, e, per l'iniettività di $f$, si ha $|C| = |m|$, quindi, per l'ipotesi induttiva (essendo $C \subseteq m$), vale $C = m$. Ma in questo modo $B = \Imm(f) = C \cup \{\underbrace{f(m)}_{= m}\} = m \cup \{m\} = s(m) = n$.
		\begin{figure}[H]
			\centering
			\includegraphics[width = 1.95cm]{immagini/cassetti1.png}
		\end{figure}
		Se $f(m) \ne m$, allora vediamo che esiste $a < m$ tale che $f(a) = m$. Se così non fosse, infatti, $f_{|m}$ sarebbe una bigettiva fra $m$ e $m \setminus \{f(m)\}$, contro l'ipotesi induttiva. Ora, però, possiamo costruire una nuova bigezione $f' : s(m) \rightarrow B$ che ricade nel caso precedente:
		\[ f'(x) = \begin{cases}
			m &\text{se $x = m$} \\
			f(m) &\text{se $x = a$} \\
			f(x) &\text{altrimenti} \\
		\end{cases}
			\]
			\begin{figure}[H]
				\centering
				\includegraphics[width = 1.95cm]{immagini/cassetti2.png}
			\end{figure}
		in altre parole stiamo ``aggiustando'' la bigezione $f$ in modo che venga di nuovo una bigezione $f'$, tale che $f'(m) = m$ e si ricade nel caso precedente (e lo possiamo sempre fare, come osservato).
	\end{itemize}
\end{proof}

\begin{corollary}[$A$ finito $\implies$ ha un'unica cardinalità]
	Se $A$ è un insieme finito, allora esiste ed è unico un elemento di $\omega$ con cui è in bigezione:
	\[ \exists \textcolor{red}{!} n \in \omega \; |A| = |n|
		\]
\end{corollary}

\begin{proof}
	Se $|m| = |A| = |n|$, possiamo assumere, senza perdita di generalità $m \leq n$, ossia $m \subseteq n$, quindi, usando il \hyperref[cassetti]{principio dei cassetti} $m = n$, abbiamo quindi l'unicità.
\end{proof}

Se adesso volessimo dimostrare il viceversa: [che formulato in versione contronominale è] che un insieme infinito non è Dedekind-finito, quale sarebbe l'ostacolo? Abbiamo già osservato che $\omega$ non è finito, perché la funziona successore 
stabilisce una corrispondenza biunivoca fra $\omega$ e $\omega \setminus\{0\} \subsetneq \omega$ (quindi non è Dedekind-finito, e per la contronominale del \hyperref[cassetti]{principio dei cassetti} non è finito). Ne segue la seguente osservazione.

\begin{remark}
	Se esiste $f : \omega \rightarrow A$ iniettiva, allora $A$ non è Dedekind-finito.
\end{remark}

\begin{proof}
	Basta considerare la funzione iniettiva:
	\[ g : A \longrightarrow A : a \longmapsto \begin{cases}
		f \circ s \circ f^{-1}(a) &\text{se $a \in f[\omega]$} \\
		\id_A(a) &\text{altrimenti}
	\end{cases}
		\]
	È immediato vedere che $\Imm(g) = A \setminus \{f(0)\} \subsetneq A$ (l'unico escluso è lo 0, perché non può esserci un elemento che ha come controimmagine un elemento di $\omega$ il cui successore sia 0, perché per quanto visto non esiste), dunque $A$ è in bigezione con un suo sottoinsieme proprio.
	\begin{figure}[H]
		\centering
		\includegraphics[width = 4.5cm]{immagini/cassetti3.png}
	\end{figure}
\end{proof}

Quindi ci basterebbe dimostrare che $\omega$ si immerge in ogni insieme infinito (e dal lemma appena visto avremmo che l'insieme non è Dedekind-finito, completando l'altra freccia del principio dei cassetti).
Un tentativo di dimostrazione potrebbe andare come segue.

\begin{proof}
	Sia $A$ infinito, costruiamo per ricorsione, seconda forma, una $f : \omega \rightarrow A$ iniettiva. Supponiamo di conoscere $f_{|n}$, il nostro scopo è definire il prossimo valore: $f(n)$.
	Siccome $A$ è infinito, $f_{|n}$, che è iniettiva per costruzione, non può essere surgettiva, quindi esiste $a \in A$ con $a \not \in \Imm(f_{|n})$. Pongo $f(n) = a$.
\end{proof}

Dov'è l'errore? Nell'ultima riga! Noi sappiamo che, data $f_{|n}$, esistono degli $a \in A$ con $a \not\in\Imm(f_{|n})$, questo è corretto. È anche corretto che ci basterebbe porre $f(n) =$``uno qualunque di questi $a$''.
Il guaio è che, \underline{per applicare il teorema di ricorsione}, ci serve una funzione che fissa (nel senso che la $h$ del teorema di ricorsione essendo un insieme deve avere tutti gli elementi già fissati, cosa che non può avvenire in questo caso) uno degli $a$. A patto di averne una, ne andrebbe bene una qualunque.\\
Purtroppo però, a partire dalla mera ipotesi che $A$ è infinito, non abbiamo modo di procurarci nessuna funzione del genere. Potremmo cavarcela se avessimo qualche struttura su $A$, sulla quale far leva - per esempio per dire ``prendo il minimo 
fra gli $a \not\in \Imm(f_{|n})$'', o ``prendo il più giallo'' - ma di $A$ non sappiamo nulla, e non abbiamo modo di indurre una struttura di questo genere.\\
Accettato che non possiamo dimostrare che $\omega$ si immerge in qualsiasi insieme infinito, possiamo però lambire questa soglia: dimostriamo che, in un insieme infinito, si immergono tutti i numeri naturali.

\begin{proposition}[Tutti i naturali si immergono in un insieme infinito]
	Sia $A$ infinito, allora $\forall n\in \omega \; |n| < |A|$.
\end{proposition}

\begin{proof}
	Basta dimostrare il $\leq$, infatti $|n| < |n+1| \leq |A|$. Dimostriamo per induzione su $n$ che c'è una funzione iniettiva da $n$ ad $A$.
	\begin{itemize}
		\item[$\boxed{\text{caso $n = 0$}}$] La funzione vuota, $f = \emptyset$.
		\item[$\boxed{\text{caso $n = m+1$}}$] Per ipotesi induttiva esiste $f : m \rightarrow A$ iniettiva. Siccome $A$ è infinito (e $m$ è finito), esiste $a \in A \setminus \Imm(f)$ (e non ci serve fissarlo poiché non stiamo usando il teorema di ricorsione).
		La funzione $f' = f \cup \{(n,a)\}$, che si ottiene estendendo $f$ col 
		mandare $n$ in $a$, è iniettiva $n \hookrightarrow A$.
	\end{itemize}
\end{proof}

\begin{corollary}[Ovvietà]
	Un sottoinsieme di un insieme finito è finito.
\end{corollary}

\begin{proof}
	Sia $A$ finito e $B \subseteq A$. Se, per assurdo $B$ fosse infinito, avremmo $|A| < |B| \leq |A| \, \lightning$ (poiché $|A| = |n|$ per definizione di finito e per la proposizione precedente tutti gli $n$ si immergono in un insieme infinito si ha $A \rightarrow n \hookrightarrow B$, dove la prima funzione è bigettiva e la seconda iniettiva, e per le proprietà di composizione delle 
	funzioni iniettive, la composizione di queste ultime due ci dà $A \hookrightarrow B$)
\end{proof}

\begin{exercise}
	Dimostrare che:
	\begin{itemize}
		\item se $|A| < |n|$ con $n \in \omega$, allora $|A| = |m|$ per qualche $m < n$.
		\item se $A$ è finito e $f : A \rightarrow B$, allora $f[A]$ è finito.
	\end{itemize}	
\end{exercise}

\subsection{Operazioni fra le cardinalità finite}

\begin{proposition}[Le operazioni tra cardinalità finite possono essere definite in funzione delle operazioni su $\omega$]
	Dati $m,n \in \omega$ vale che:
	\[ |m| + |n| = |m+n| \qquad |m|\cdot|n| = |m \cdot n| \qquad |m|^{|n|} = |m^n|
		\]
	ovvero, per gli elementi di $\omega$ le operazioni tra cardinalità corrispondo alla cardinalità delle operazioni tra gli elementi, già definite per ricorsione su $\omega$.
\end{proposition}

\begin{proof}
	Dimostriamo, intanto che $|m| + |1| = |s(m)|$. A sinistra abbiamo, infatti la cardinalità di $(m \times \{0\}) \cup \{(0,1)\}$\footnote{Typo del prof. Mamino sui suoi appunti in quanto $1 = \{0\}$.} e a destra abbiamo la cardinalità di $m \cup \{m\}$.
	Quest'ultimo insieme si mappa bigettivamente nel primo, mandando $x \in m$ in $(x,0)$ e $m$ in $(0,1)$. 
	Ora, le uguaglianze asserite seguono, per induzione su $n$, dalle proprietà delle operazioni sulle cardinalità e dalla definizione ricorsiva delle operazioni su $\omega$.\\
	\textcolor{purple}{$|m|+|n| = |m+n|$}
	\begin{itemize}
		\item[$\boxed{\text{caso $n = 0$}}$] $|m| + |0| =  |(m \times \{0\}) \cup \emptyset| = |m| = |m + 0|$.
		\item[$\boxed{\text{caso $n = s(a)$}}$] Per ipotesi induttiva abbiamo $|m| + |a| = |m + a|$, da cui possiamo verificare la tesi come segue:
		\[ \begin{split}
			|m| + |s(a)| \overset{\text{oss. iniziale}}{=}\quad& |m| + (|a| + |1|) \\
						 \overset{\text{propr. operaz. card.}}{=}& (|m| + |a|) + |1| \\
						 \overset{\text{Hp. indutt}}{=}\quad& |m+a| + |1| \\
						 \overset{\text{oss. iniziale}}{=}\quad& |s(m+a)| \\
						 \overset{\text{def. di $+$}}{=}\quad\;& |m + s(a)|
 		\end{split} 
			\]
	\end{itemize}
	\textcolor{purple}{$|m|\cdot|n| = |m \cdot n|$}
	\begin{itemize}
		\item[$\boxed{\text{caso $n = 0$}}$] $|m| \cdot |0| =  |m \times \emptyset| = |0| \overset{\text{def. di $\cdot$}}{=} |m \cdot 0|$.
		\item[$\boxed{\text{caso $n = s(a)$}}$] Per ipotesi induttiva abbiamo $|m|\cdot|a| = |m \cdot a|$, da cui possiamo verificare la tesi come segue:
		\[ \begin{split}
			|m| \cdot |s(a)| \overset{\text{oss. iniziale}}{=}\quad& |m| \cdot (|a| + |1|) \\
						 \overset{\text{propr. operaz. card.}}{=}& |m| \cdot |a| + \underbrace{|m| \cdot |1|}_{|m \times \{0\}| = |m|} \\
						 \overset{\text{Hp. indutt}}{=}\quad& |m \cdot a| + |m| \\
						 \overset{\text{propr. $+$ card.}}{=}\quad& |m \cdot a + m| \\
						 \overset{\text{def. di $\cdot$}}{=}\quad\;& |m \cdot s(a)|
 		\end{split} 
			\]
	\end{itemize}
	\textcolor{purple}{$|m|^{|n|} = |m^n|$}
	\begin{itemize}
		\item[$\boxed{\text{caso $n = 0$}}$] $|m|^{|0|} =  |{}^{0}m| = |1| = |m^0|$ (l'unica funzione possibile dal vuoto a $m$ è $f = \emptyset$\footnote{E quindi ${}^0m = \{f : \emptyset \rightarrow m\} = \{\emptyset\} = 1$, o in alternativa si può pensare che
		$f \subseteq \emptyset \times m = \emptyset \implies f \in \ps(\emptyset) = \{\emptyset\}$ e quindi $f = \emptyset \implies {}^0m = \{f\} = \{\emptyset\} = 1$.}).
		\item[$\boxed{\text{caso $n = s(a)$}}$] Per ipotesi induttiva abbiamo $|m|^|a| = |m^a|$, da cui possiamo verificare la tesi come segue:
		\[ \begin{split}
			|m|^{|s(a)|} \overset{\text{oss. iniziale}}{=}\quad& |m|^{|a| + |1|} \\
						 \overset{\text{propr. operaz. card.}}{=}& |m|^{|a|} \cdot \underbrace{|m|^{|1|}}_{=|m|} \\
						 \overset{\text{Hp. indutt}}{=}\quad& |m^a| \cdot |m| \\
						 \overset{\text{propr. del $\cdot$ card.}}{=}\quad& |m^a \cdot m| \\
						 \overset{\text{def. potenza}}{=}\quad\;& |m^{s(a)}|
 		\end{split} 
			\]
	\end{itemize}
\end{proof}

\begin{note}
	Questa proposizione ci fornisce una dimostra delle proprietà aritmetiche elementari delle operazioni su $\omega$ [sfruttando le proprietà delle operazione fra cardinalità], alternativa a quella per induzione (che è stata lasciata per esercizio).
	Basta, infatti, applicare le corrispondenti proprietà delle operazioni sulle cardinalità\footnote{E ciò non comporta problemi di circolarità poiché nella dimostrazione della proposizione precedente abbiamo usato solo la definizione delle tre operazione e nessuna delle  loro proprietà.}.
\end{note}

\begin{exercise}
	Dimostra che se $m,n \in \omega$ e $m \leq n$, esista un unico $n-m \in \omega$ tale che $m + (n-m) = n$. In due modi diversi.
\end{exercise}

\newpage
\section{La cardinalità del numerabile}

\begin{definition}[Numerabilità]
	Diciamo che $A$ è \vocab{al più numerabile} se $|A| \leq |\omega|$ ed è \vocab{numerabile} se $|A| = |\omega|$.
	Il simbolo $\aleph_0$ - aleph con zero - è semplicemente un'abbreviazione per $|\omega|$ (per cui $|A| \leq \aleph_0$ si può leggere ``$A$ è al più numerabile'' e $|A| = \aleph_0$ si 
	può leggere ``$A$ è numerabile'').
\end{definition}

\begin{remark}
	In altri termini, dire che $A$ è al più numerabile significa dire che c'è una funzione iniettiva $A \hookrightarrow \omega$. Dire che è numerabile significa dire che c'è una bigezione con $\omega$.
\end{remark}

\begin{proposition}
	Se $A$ è al più numerabile, allora o $A$ è finito o $A$ è numerabile.
\end{proposition}

\underline{Ossia}: $|A| < \aleph_0$ se e solo se $A$ è finito [non è altro che una formulazione equivalente della proposizione sopra].

Potremmo dimostrare la proposizione direttamente, ma ci conviene, invece, passare attraverso alcune considerazioni che saranno utili in seguito.\\
In generale, per costruire una bigezione fra due insiemi $A$ e $B$ - ossia per dimostrare $|A| = |B|$ - occorre appoggiarsi a qualche struttura definita sugli insiemi $A$
e $B$. Per esempio, una funzione successore. In questo corso, giocheranno un ruolo importante, in questa direzione, le relazioni d'ordine, e, in particolare - l'idea è di Cantor - i 
\vocab{buoni ordini}. Ricordiamo la definizione.

\begin{definition}[Buon ordinamento]
	Un insieme totalmente ordinato $(S,<)$ si dice \vocab{bene ordinato} se ogni suo sottoinsieme non vuoto ha un minimo.
	\[ \forall A \subseteq S \; A \ne \emptyset \longrightarrow \exists m \in A \; \forall a \in A \; m \leq x
		\]
\end{definition}

Il trucco è che un isomorfismo di ordini è, in particolare, una bigezione, e spesso, per costruire bigezioni, costruiamo isomorfismi di ordini.

\begin{definition}[Isomorfismo]
	Due insiemi (parzialmente\footnote{Dove parziale indica l'assenza della proprietà di totalità nella definizione di relazione d'ordine.}) ordinati
	$(A,<_A)$ e $(B,<_B)$ sono \vocab{isomorfi}, in simboli $(A,<_A) \sim (B,<_B)$ se esiste una bigezione $f : A \rightarrow B$ tale che:
	\[ \forall x,y \in A \; x <_A y \longleftrightarrow f(x) <_B f(y)
		\]
	(cioè se esiste una bigezione che rispetta le relazioni d'ordine).
\end{definition}

\begin{remark}[Funzioni strettamente crescenti]
	Due insiemi TOTALMENTE ordinati $(A,<_A)$ e $(B,<_B)$ sono isomorfismi se e solo se esiste una funzione $f : A \rightarrow B$ \underline{surgettiva} e \vocab{strettamente crescente} - cioè tale che:
	\[ \forall x,y \in A \; x <_A y \longleftrightarrow f(x) <_B f(y)
		\]
	(non è altro che la definizione in cui supponiamo gli insiemi totalmente ordinati e diamo un nome alla funzione che realizza l'isomorfismo in questo caso).
\end{remark}

\begin{exercise}
	Dimostrare la proposizione enunciata sopra.
\end{exercise}

\begin{remark}[Ogni insieme finito è isomorfo alla sua cardinalità]
	Sia $(A,<_A)$ totalmente ordinato con $|A| = n \in \omega$. Allora $(A,<_A) \sim (n,<)$, dove $<$ denota l'ordinamento indotto da $\omega$ (cioè l'ordine che abbiamo definito su $\omega$ ristretto a $n$).
\end{remark}

\begin{proof}
	Procediamo per induzione su $n$.
	\begin{itemize}
		\item[$\boxed{\text{caso $n = 0$}}$] $A = \emptyset$, quindi $(A,<_A) \sim (\emptyset,\emptyset)$.
		\item[$\boxed{\text{caso $n = s(m)$}}$] Se $m = 0$, allora $A = \{a\}$ e $(A,<_A) \sim (1,<)$, cioè la tesi è banalmente vera. Assumiamo quindi $m>0$. Dimostriamo intanto che $(A,<_A)$ ha un massimo elemento.
		Fissiamo una bigezione $f : s(m) \rightarrow A$. Allora $|f[m]| = m$, quindi $f[m]$ con l'ordinamento indotto da $<_A$ è isomorfo a $(m,<)$ per ipotesi induttiva e, in particolare, ha massimo $M$. Ora per la totalità di $<_A$,
		o $M < f(m)$ oppure $f(m)<M$. Si verifica immediata che, nel primo caso, $f(m)$ è il massimo di $A$, e nel secondo $M$ è il massimo di $A$.\\
		Stabilito che $A$ ha un massimo $N$, osserviamo che, detto $A' := A \setminus\{N\}$, siccome $|A'| = m$, usando nuovamente l'ipotesi induttiva abbiamo un isomorfismo $f : A' \rightarrow m$ fra $A'$, con l'ordinamento indotto da $<_A$ e $(m,<)$.
		Si verifica facilmente che:
		\[ f' : A \longrightarrow s(m) : x \longmapsto \begin{cases}
			f(x) &\text{se $x \in A'$} \\
			m &\text{se $x = N$}
		\end{cases}
			\]
		è l'isomorfismo cercato.
	\end{itemize}
\end{proof}

Possiamo caratterizzare $\omega$ in termini delle proprietà del suo ordinamento naturale. Quelle che servono sono le seguenti.

\begin{proposition}[Proprietà di $(\omega,<)$]
	Dato $(\omega,<)$ ordine totale allora valgono le seguenti:
	\begin{enumerate}[(1)]
		\item $(\omega,<)$ è un buon ordine.
		\item $(\omega,<)$ è \vocab{illimitato} - ossia $\forall x \in \omega \; \exists y \in \omega \; x < y$.
		\item Ogni $A \subseteq \omega$ superiormente limitato e non vuoto ha un massimo, ossia:
		\[ \forall A \subseteq \omega (A \ne \emptyset \land (\exists L \in \omega \; \forall x \in A \; x \leq L)) \longrightarrow (\exists M \in A \; \forall x \in A \; x \leq M)
			\]
	\end{enumerate}
\end{proposition}

\begin{proof}
	Abbiamo che (1) è il principio del minimo che abbiamo già dimostrato su $\omega$, per (2) basta prendere $y = s(x)$ (e $x \in s(x) \implies x < y$). Per (3) se $A$ è superiormente limitato da $L \in \omega$, allora $A \subseteq s(L)$, quindi $A$ è finito.
	Siccome $A$ è finito, l'ordinamento totale su $A$ definito da:
	\[ x \prec y \Mydef y < x
		\]
	è buono, quindi, in particolare, c'è il minimo di $A$ secondo l'ordinamento $\prec$. Questo è il massimo di $A$ (secondo l'ordinamento $<$).
\end{proof}

\begin{proposition}[Caratterizzazione di $\omega$ come ordine]
	Sia $(A,\prec)$, con $A \ne \emptyset$, un ordinamento:
	\begin{enumerate}
		\item buono
		\item illimitato
		\item tale che ogni sottoinsieme superiormente limitato e non vuoto di $A$ ha un massimo secondo $\prec$.
	\end{enumerate}
	Allora $(A,\prec) \sim (\omega,<)$.
\end{proposition}

Dimostriamo prima un facile lemma.

\begin{lemma}[Stretta crescenza col successore $\implies$ stretta crescenza]
	Sia $(A,\prec)$ un ordine, e sia $f : \omega \rightarrow A$ tale che:
	\[ \forall n \in \omega \; f(n) \prec f(s(n)) \, \footnote{Typo di Mamino nelle dispense.}
		\]
	allora $f$ è strettamente crescente, cioè $\forall m,n \in \omega \; m < n \rightarrow f(m) \prec f(n)$, e in particolare è iniettiva.
\end{lemma}

\begin{proof}
	Considero, per assurdo, $m < n$ tali che $f(m) \not \prec f(n)$, con $n$ minimo [tale per cui accade ciò]. Siccome $0 \leq m < n$, esiste $n'$ tale che $n = s(n')$.
	Ora, da un'\hyperref[succ2]{osservazione precedente}, essendo $m < s(n')$, si ha $m = n' \lor m < n'$. Nel primo caso, dall'ipotesi segue:
	\[ f(m) \prec f(s(m)) = f(s(n')) = f(n)
		\]
	contraddicendo $f(m) \not\prec f(n)$. Nel secondo caso, per la minimalità di $n$, deve accadere per forza $f(m) \prec f(n')$, ma $f(n') \prec f(s(n')) = f(n)$ per ipotesi, quindi abbiamo di nuovo una contraddizione,
	pertanto deve essere necessariamente $f(m) \prec f(n)$.
\end{proof}

Possiamo ora dimostrare la proposizione.

\begin{proof}
	Costruiamo per ricorsione un isomorfismo $f$ da $(\omega, <)$ a $(A,\prec)$:
	\[ f(0) = \min_\prec A \qquad f(s(n)) = \min_\prec\{a \in A | f(n) \prec a\} \, \footnote{Cioè la funzione manda il successore nel più piccolo termine in $(A,\prec)$ che sta ``sopra'' a $f(n)$ (in pratica la stiamo costruendo apposta affinché sia strettamente crescente).}
		\]
	dove $\min_\prec$ denota il minimo secondo la relazione d'ordine (buona) $\prec$. Occorre dimostrare intanto che $f$ è ben definita. $f(0)$ è ben definita, perché $A \ne \emptyset$, dunque (essendo ben ordinato per ipotesi ha un minimo\footnote{Per essere precisi essendo $A \in \ps(A)$ tecnicamente il principio del buon ordinamento si applica anche ad $A$ stesso (oltre che ai suoi sottoinsiemi propri), che quindi ha minimo.}).
	Per dire che $f(s(n))$ è ben definita, occorre dire che la funzione $h : A \rightarrow A$, $h(x) = \min_\prec\{a \in A|f(n) \prec a\}$ è ben definita (sarebbe la funzione che definisce la ricorsione - prima forma -), 
	ossia che $\{a \in A | x \prec a\}$ è non vuoto (e quindi di nuovo esiste il minimo perché stiamo supponendo $A$ ben ordinato). Ma questo avviene, qualsiasi sia $x\in A$, perché altrimenti $A$ sarebbe limitato [superiormente] da $x$ (e non illimitato superiormente come abbiamo supposto nelle ipotesi).\\
	Per come è costruita, e per il lemma, $f$ è [strettamente] crescente (cioè l'abbiamo costruita in modo che sia una funzione da $\omega$ in $A$ crescente rispetto al successore, per cui vale il lemma sopra, dunque è sempre crescente), quindi iniettiva.
	Di conseguenza, ci basta dimostrare la surgettività.\\
	Prendiamo $y \in A$ e cerchiamo $x \in \omega$ tale che $y = f(x)$. Se, per ogni $x \in \omega$, avessi $f(x) \prec y$, allora $f[\omega]$ sarebbe superiormente limitato da $y$, e tuttavia non avrebbe massimo perché ogni $f(x)$ è $\prec$ di $f(s(x))$, il che è assurdo.
	Quindi c'è il minimo $x \in \omega$ tale che $y \preceq f(x)$. Dimostriamo che $f(x) \preceq y$, da cui l'uguaglianza (e quindi la surgettività).
	\begin{itemize}
		\item[$\boxed{\text{$x = 0$}}$] in tal caso $f(x)$ è il minimo di $A$, quindi $f(x) \preceq y \in A$.
		\item[$\boxed{x = s(x')}$] in questo caso $f(x') \prec y$ per la minimalità di $x$ (avendo preso $x$ come il minimo in $\omega$ tale che $f(x) \preceq y$, tutto ciò che sta sotto non può rispettare l'ultima condizione), ma allora, $y \in \{a \in A | f(x') \prec a\}$, quindi $f(x) = f(s(x')) = \min_\prec\{a \in A | f(x) \prec a\} \preceq y$ (dove l'ultima disuguaglianza deriva dal fatto che $y$
		appartiene all'insieme di cui stiamo facendo il minimo, mentre la seconda uguaglianza è la definizione di $f$).
	\end{itemize}
\end{proof}

Tornando alla proposizione iniziale.

\begin{proposition}[Caratterizzazione insiemi al più numerabili]
	Se $A$ è al più numerabile, allora o $A$ è finito o $A$ è numerabile.
\end{proposition}

\begin{proof}
	Per ipotesi esiste $f : A \rightarrow \omega$ iniettiva, per cui abbiamo $|A| = |f[A]|$, e siccome $f[A] \subseteq \omega$,
	ci basta dimostrare che dato $B \subseteq \omega$, o $B$ è finito o è numerabile.\\
	Sia $B \subseteq \omega$ infinito, dimostriamo che $B$, con l'ordinamento indotto dall'ordine naturale di $\omega$ soddisfa le ipotesi della proposizione precedente. 1 e 3 \footnote{Typo di Mamino.} valgono
	in quanto ogni sottoinsieme di $B$ è in particolare, sottoinsieme di $\omega$ (dunque abbiamo buon ordinamento ed esistenza del massimo). Per ottenere 2 dobbiamo dire che $B$
	non ha un massimo elemento (cioè è illimitato). Se, infatti, ci fosse un $M \in B$ tale che $\forall b \in B \; b \leq M$, allora avremmo che $B \subseteq s(M)$, $B$ sarebbe dunque finito, contro l'ipotesi.
	Pertanto $(B,<_{|B}) \sim (\omega,<) \implies |B| = \aleph_0$, dunque se un sottoinsieme di $\omega$ è infinito, allora è necessariamente numerabile.\\
	Il caso di un sottoinsieme non infinito coincide col caso di un elemento di $\omega$ (che sappiamo essere un sottoinsieme per le proprietà di $\omega$), che è dunque banalmente in bigezione con se stesso (via identità) e quindi finito per definizione.
\end{proof}

\begin{exercise}
	\label{ex7.13}
	Dimostra che se $|A| \leq \aleph_0$ e $f : A \twoheadrightarrow B$ è surgettiva, allora $|B| \leq \aleph_0$.
\end{exercise}

\begin{soln}
	Mostriamo che sotto queste ipotesi esiste $h : B \hookrightarrow \omega$ (iniettiva), sia $g : A \hookrightarrow \omega$ e poniamo:
	\[ h(b) = \min(g[\underbrace{\{a \in A | f(a) = b\}}_{= ``f^{-1}(b)''}]) \, \footnote{Si noti che, essendo $f$ non necessariamente iniettiva, $f^{-1}$ denota la controimmagine, non la funzione inversa, da cui la scelta delle parentesi quadre quando si applica $g$, per evidenziare che stiamo facendo l'immagine di un'insieme.}
		\]
	l'insieme tra graffe è non vuoto per surgettività di $f$, dunque il minimo è ben definito.
	Inoltre, se $h(b) = h(b')$, allora i minimi [che chiamiamo] $g(a)$ e $g(a')$ sono uguali, ma $a$ e $a'$ sono elementi nelle controimmagini rispettivamente di $b$ e $b'$, cioè tali che $f(a) = b$ e $f(a') = b'$.
	Sappiamo quindi per ipotesi che $g(a) = g(a')$ e per l'iniettività di $g$ segue $a = a'$, da cui $f(a) = f(a')$ (ovviamente sono lo stesso elemento), da cui $b = f(a) = f(a') = b'$.
\end{soln}

\subsection{Insiemi numerabili in pratica}
Sapere che, se $|A| \leq \aleph_0$, allora o $A$ è finito o è numerabile, ci fornisce lo strumento essendo per dimostrare la numerabilità di molti insiemi concreti. Spesso, infatti,
è facile dimostrare che un insieme infinito è tale. Rimane poi da gestire un discorso di disuguaglianze per dire che esso è al più numerabile.\\
Cominciamo quindi con qualche considerazione generale a proposito delle disuguaglianze fra cardinalità.

\begin{remark}[Compatibilità tra operazioni e ``ordinamentro'' fra cardinalità]
	Dati gli insiemi $A,B,C$ con $|B| \leq |C|$ allora vale:
	\begin{align*}
		|A| + |B| \leq |A| + |C| \qquad & |A|^{|B|} \leq |A|^{|C|} \\
		|A| \cdot |B| \leq |A| \cdot |C| \qquad & |B|^{|A|} \leq |C|^{|A|}
	\end{align*}
\end{remark}

Vale a dire che le operazioni sulle cardinalità sono monotone, nel senso delle disuguaglianze larghe. \underline{Attenzione però} che, in generale, NON sono strettamente monotone!

\begin{proof}
	Detta $f : B \rightarrow C$ la funzione iniettiva che testimonia che $|B| \leq |C|$ e detto $B' = f[B]$ abbiamo che $|B| = |B'|$ (come al solito per definizione di disuguaglianza
	tra cardinalità), quindi basta dimostrare le disuguaglianze asserite con $B'$ al posto di $B$. Ora, giocando sul fatto che $B' \subseteq C$ (abbiamo fatto apposta lo scambio tra $B$ e $B'$ per poter usare i contenimenti),
	si vede che queste disuguaglianze rappresentano, in realtà, relazioni di contenimento fra RHS e LHS. Per esempio:
	\[ \begin{split}
		B' \subseteq C &\overset{\text{ovvio}}{\implies} (A \times \{0\}) \cup (B' \times \{1\}) \subseteq (A \times \{0\}) \cup (C \times \{1\}) \; \textcolor{red}{= A \sqcup B' \subseteq A \sqcup C}\\
					   &\overset{\id_A \times \id_{B'}}{\implies} |(A \times \{0\}) \cup (B' \times \{1\})| \leq |(A \times \{0\}) \cup (C \times \{1\})| \\
					   &\overset{\text{def.}}{\iff}|A| + |B'| \leq |A| + |C|
	\end{split}
		\]
	Le altre si ottengono allo stesso modo.
\end{proof}

\begin{remark}[Disuguaglianza di inclusione-esclusione]
	$|A \cup B| \leq |A| + |B|$.
\end{remark}

\begin{proof}
	Basta osservare che la seguente funzione è iniettiva:
	\[ f : A \cup B \longrightarrow (A \times \{0\}) \cup (B \times \{1\}) : x \longmapsto \begin{cases}
		(x,0) &\text{se $x \in A$} \\
		(x,1) &\text{altrimenti}
	\end{cases}
		\]
\end{proof}

Veniamo roa a calcolare le operazioni aritmetiche. Già sappiamo, per il \hyperref[cantor]{teorema di cantor}, 
che $2^{\aleph_0} > \aleph_0$, per cui mettere un $\aleph_0$ a esponente di qualunque cosa non sia uno 0 o un 1 conduce fuori dal numerabile.
Tutto il resto invece no.

\begin{proposition}[Operazioni aritmetiche con $\aleph_0$]
	$\aleph_0 + \aleph_0 = \aleph_0 \cdot \aleph_0 = \aleph_0^{n} = \aleph_0$, con $n \in \omega\setminus\{0\}$.
\end{proposition}

\begin{proof}
	Supponiamo di sapere già che $\aleph_0 \cdot \aleph_0 = \aleph_0$, allora possiamo formare la catena di disuguaglianze:
	\[ \aleph_0 \overset{\text{op. card.}}{=} \aleph_0 + 0 \overset{\text{oss. sopra}}{=} \aleph_0 + \aleph_0 \overset{\text{op. card.}}{=} \aleph_0 \cdot 2 \overset{\text{oss. sopra}}{=} \aleph_0 \cdot \aleph_0 \overset{\text{ipotesi}}{=} \aleph_0
		\]
	Da cui per il \hyperref[CB]{Cantor-Bernstein}:
	\[ \aleph_0 + \aleph_0 = \aleph_0 \cdot \aleph_0 = \aleph_0
		\]
	Ora è facile vedere per induzione che $n \in \omega \setminus\{0\} \rightarrow \aleph_0^n = \aleph_0 $, infatti $\aleph_0^1 = \aleph_0$ e $\aleph_0^{n+1} = \aleph_0^n \cdot \aleph_0 \overset{\text{Hp. indutt.}}{=} \aleph_0 \cdot \aleph_ 0 = \aleph_0$.
\end{proof}

Per concludere la dimostrazione precedente, resta da dimostrare il lemma seguente.

\subsection{Prodotto di numerabili è numerabile}

\begin{lemma}[$\aleph_0 \cdot \aleph_0 = \aleph_0$]
	$\aleph_0 \cdot \aleph_0 = \aleph_0$, ossia esiste una bigezione fra $\omega \times \omega$ e $\omega$.
\end{lemma}

\begin{wrapfigure}[15]{r}{2.82cm}
	\includegraphics[width=2.82cm]{immagini/aleph2.png}
\end{wrapfigure}
Ci sono diverse vie per illustrare questo risultato. Per esempio, possiamo rappresentare le coppie $(x,y) \in \omega \times \omega$ sotto la specie di una griglia a maglie quadrate.
Poi disegnare un percorso che pare visitare tutte le maglie della griglia, con sufficiente apparenza di regolarità, possibilmente, da convincere il lettore che vi debba essere un metodo.
Infine numeriamo le maglie secondo l'ordine in cui sono visitate dal percorso. Avremo così numerato tutte le coppie di numeri naturali del disegno.\\
Altrimenti, è possiamo esibire delle bigezioni esplicite, per esempio:
\[ f(x,y) = 2^x \cdot (2y + 1) - 1 \qquad g(x,y) = \frac{(x+y)^2 + 3x + y}{2}
	\]
È possibile scrivere i due numeri della coppia in base 10 a cifre alternate, tipo: $(\textcolor{blue}{64},\textcolor{red}{4096}) \mapsto \textcolor{red}{4}\textcolor{cyan}{0}\textcolor{red}{0}\textcolor{red}{9}\textcolor{cyan}{0}\textcolor{blue}{6}\textcolor{red}{6}\textcolor{red}{4}\textcolor{blue}{4}$.\\

\emph{Dimostrazione.} Consideriamo l'ordinamento su $\omega \times \omega$ definito come segue:
	\begin{wrapfigure}[8]{l}{2.5cm}
		\includegraphics[width=2.5cm]{immagini/ordine_omega.png}
	\end{wrapfigure}
	\[ \begin{split}
		(a,b) \prec (a',b') &\Mydef \max(a,b) < \max(a',b') \\
							&\lor (\max(a,b) = \max(a',b') \land a < a') \\
							&\lor (\max(a,b) = \max(a',b') \land a = a' \land b < b')
	\end{split}
		\]
	(dove per max sulla coppia si intende il max tra $a$ e $b$) ossia per confrontare $(a,b)$ con $(a',b')$, si confrontano prima $\max(a,b)$ e $\max(a',b')$; a parità si confrontano $a$ ed $a'$ (cioè se hanno una delle due componenti con lo stesso modulo massimo, si passa a confrontare il valore delle prime componenti);
	se anche queste coincidono, allora si confrontano $b$ e $b'$.\footnote{Come si vede nella figura a lato, nel primo caso, avendo $b$ modulo massimo, ci sono anche punti più a destra, che in quest'ordinamento sono più piccoli (perché hanno un valore più piccolo come massima componente).}\\
	L'idea è che, in questo modo, le coppie $\prec$ di una certa $(a,b)$ fissata sono tutte contenute nel quadrato $\{0,\ldots,\max(a,b)\} \times \{0,\ldots,\max(a,b)\}$, quindi sono in numero finito, e questo 
	implica che $(\omega \times \omega, \prec)$ è isomorfo $(\omega,<)$.\\
	Formalmente, iniziamo col verificare che $\prec$ sia effettivamente un ordine stretto. La proprietà irriflessiva è immediata (perché in tutti gli OR nella definizione stiamo usando l'ordinamento stretto di $\omega$, dunque
	$\neg (a,b) \prec (a,b)$). Per verificare la proprietà transitiva, prendiamo $(a,b) \prec (a',b) \prec (a'',b'')$ (vorremo vedere che questo implica $(a,b) \prec (a'',b'')$). Dalle disuguaglianze precedenti segue $\max(a,b) \leq \max(a',b') \leq \max(a'',b'')$. Se una di queste disuguaglianze è stretta allora $(a,b) \prec (a'',b'')$ (e avremmo concluso),
	altrimenti $\max(a,b) = \max(a',b') = \max(a'',b'')$, segue dalla definizione che $a \leq a' \leq a''$.
	Nuovamente, se una disuguaglianza è stretta abbiamo concluso, altrimenti $a = a' = a''$, quindi, affinché la scrittura iniziale sia ancora vera deve essere necessariamente che $b < b' < b''$, da cui $b < b''$, e quindi anche in questo caso vale $(a,b) \prec (a'',b'')$.\\
	Per dire che l'ordine è totale osserviamo che se $(a,b)$ e $(a',b')$ non sono né $\prec$ ne $\succ$ allora dobbiamo avere $\max(a,b) = \max(a',b')$, $a = a'$, $b = b'$, ovvero $(a,b) = (a',b')$, dunque l'ordine stretto è anche totale.
	Ora vogliamo dire che $(\omega \times \omega, \prec) \sim (\omega, <)$ (in questo modo, avendo un'isomorfismo di ordini, avremmo in particolare una bigezione tra $\omega$ e $\omega \times \omega$, dunque il prodotto di cardinalità numerabili è numerabile).\\
	Partiamo dall'osservazione che se $(a,b) \in \omega \times \omega$ allora:
	\[ (\omega \times \omega)_{(a,b)} \Mydef \{(x,y) \in \omega \times \omega | (x,y) \prec (a,b)\}
		\]
	detto il ``\vocab{segmento iniziale} determinato da $(a,b)$ su $(\omega \times \omega,<)$'', è finito\footnote{È quello che abbiamo detto sopra sulla definizione di $\prec$, pensando alle due figure.}. Infatti $(\omega \times \omega)_{(a,b)} \subseteq s(\max(a,b)) \times s(\max(a,b))$.
	Ci serve dire \textcolor{red}{1.} $(\omega \times \omega, \prec)$ è bene ordinato \textcolor{red}{2.} $(\omega \times \omega, \prec)$ è illimitato \textcolor{red}{3.} ogni sottoinsieme non vuoto e superiormente limitato di $\omega \times \omega$ ha un massimo.
	\begin{enumerate}[1.]
		\item Dato $A \subseteq \omega \times \omega$ con $A \ne \emptyset$, considero $a \in A$. Se $(\omega \times \omega)_a \cap A = \emptyset$ (stiamo considerando il segmento iniziale rispetto a un generico elemento $a\in\omega \times \omega$), allora $a$ è il minimo di $A$ (sta in $a$ e
		non c'è nulla più piccolo nell'insieme perché l'intersezione col segmento iniziale di $a$ (= cose strettamente più piccole in $(\omega \times \omega,\prec)$) è vuota). Altrimenti $A' = (\omega \times \omega)_{(a,a)} \cap A$ è non vuoto e finito,
		quindi ha minimo $m$ (perché $\prec$ è un ordine totale). Questo deve essere anche il minimo di $A$, perché se $x \in A \setminus A'$, allora $m \prec a \preceq x$ (dove la seconda disuguaglianza segue per quanto detto nel caso dell'intersezione vuota, mentre la prima perché sia $a$ che $m$ stanno nell'intersezione in cui abbiamo preso il minimo).
		\item Dato $(a,b) \in \omega \times \omega$, $(a,b) \prec (s(a),s(b))$, dunque $\omega \times \omega$ è illimitato.
		\item Dato $A \subseteq \omega \times \omega$ non vuoto e superiormente limitato da $(a,b) \in \omega \times \omega$, abbiamo che $A \subseteq (\omega \times \omega)_{(a+1,b+1)}$ è finito (per quanto osservato sopra), quindi ammette massimo perché $\prec$ è totale (abbiamo un numero finito di elementi da confrontare).
	\end{enumerate}
$\hfill\square$

\subsection{Numeri interi e razionali}
Usando la proposizione appena dimostrata, potremmo dimostrare, per esempio, che $\ZZ$ e $\QQ$ sono numerabili, se non fosse che non abbiamo ancora definito questi oggetti. Allo scopo, ricordiamo che - \hyperref[3.73]{esercizio 3.73} - una relazione di equivalenza
induce un insieme di classi di equivalenza.

\begin{definition}[$\ZZ$]
	Definiamo $\ZZ$ come l'insieme delle classi di equivalenza su $\omega \times \omega$ indotte dalla relazione:
	\[ (a,b) \sim_\ZZ (a',b') \Mydef a + b' = b + a' \, \footnote{Morale: ``$(a,b) = a - b$''.}
		\]
\end{definition}

\begin{exercise}
	Dimostrare che $\sim_\ZZ$ è una relazione di equivalenza.
\end{exercise}

\begin{example}[Operazioni su $\ZZ$]
	Definiamo $+,-,\cdot$ su $\ZZ$ mediante:
	\begin{align*}
		[(a,b)]_\ZZ + [(a',b')]_\ZZ &\Mydef [(a+a',b+b')]_\ZZ \\
		-[(a,b)]_\ZZ &\Mydef [(b,a)]_\ZZ \\
		[(a,b)]_\ZZ \cdot [(a',b')]_\ZZ &\Mydef [(a\cdot a' + b \cdot b', a \cdot b' + a' \cdot b)]_{\ZZ}
	\end{align*}
	dimostra che $\ZZ$, con queste operazioni, è un anello commutativo con identità: $1 \Mydef [(1,0)]_\ZZ$. 
\end{example}

\begin{definition}[$\QQ$]
	Definiamo $\QQ$ come l'insieme delle classi di equivalenza su $\ZZ \times (\omega\setminus\{0\})$ indotte dalla relazione:
	\[ (n,d) \sim_\QQ (n',d') \Mydef n \cdot d' = n' \cdot d \, \footnote{Morale: ``$(n,d) = \frac nd$''.}
		\]
\end{definition}

\begin{exercise}
	Dimostrare che $\sim_\QQ$ è una relazione di equivalenza.
\end{exercise}

\begin{exercise}[Operazioni su $\QQ$]
	Definisci $+,-,\cdot$ e $\square^{-1}$ su $\QQ$ nella maniera ragionevole e dimostra che $\QQ$ è un campo.
\end{exercise}

\begin{exercise}[Ordinamento su $\QQ$]
	Definisci la relazione $<$ su $\QQ \times \QQ$ dicendo che $q \in \QQ$ è positivo se $q = [(n,d)]_\QQ$, con $n,d \in \omega\setminus\{0\}$, e dicendo che $a < b$ se e solo se $b - a$ è positivo.
	Dimostra che questo è un ordine totale e \vocab{denso}, cioè:
	\[ \forall a,b \in \QQ \; a < b \rightarrow \exists c \in \QQ \; a< c <b \, \footnote{Typo di Mamino.}
		\]
\end{exercise}

\begin{note}
	Gli esercizi precedenti sono tediosi, ma non sono difficili. Nel resto del corso daremo per scontate le proprietà aritmetiche elementari di $\ZZ$ e $\QQ$.
	D'ora innanzi scriveremo:
	\[ a - b \Mydef [(a,b)]_{\ZZ} \qquad \frac{n}{d} \Mydef [(n,d)]_\QQ
		\]
\end{note}

Per dimostrare la numerabilità di $\ZZ$ e $\QQ$, è comodo richiamare ancora un \hyperref[ex7.13]{esercizio}, però, questa volta, lo risolviamo\footnote{La soluzione riportata è quella di Mamino.}.

\begin{corollary}[Definizione di al più numerabile al contrario]
	Un insieme $A \ne \emptyset$ è al più numerabile se e solo se esiste $f : \omega \rightarrow A$ surgettiva.
\end{corollary}

\begin{proof}
	La freccia $\Longleftarrow$ deriva dall'esercizio citato prima con $A = \omega$ e $B = A$\footnote{Quelli al LHS sono quelli nell'enunciato dell'esercizio, quelli al RHS sono quelli presi dalle ipotesi del corollario.}. Per
	l'inverso, supponiamo $A$ al più numerabile e mostriamo che c'è sempre una mappa surgettiva tra $\omega$ ed $A$. Sappiamo che se un insieme è al più numerabile, o è finito o è numerabile, se $|A| = \aleph_0$ allora c'è $f$
	bigettiva (e quindi in particolare surgettiva), se $|A| < \aleph_0$ allora c'è [per definizione] $g : n \rightarrow A$ bigettiva per qualche $n \in \omega\setminus\{0\}$, da questa definiamo:
	\[ f(x) = \begin{cases}
		g(x) &\text{se $x < n$} \\
		g(0) &\text{altrimenti}
	\end{cases}
		\]
	come mappa surgettiva da $\omega$ in $A$ (cioè estendiamo la funzione che già c'è con $n$ a tutti i naturali maggiori o uguali ponendola come g(0)).
\end{proof}

\begin{notation}[Successione]
	Con \vocab{successione} (numerabile) intendiamo semplicemente una funzione con dominio $\omega$, per cui:
	\[ \alpha = \{\alpha_i\}_{i \in \omega} \Mydef \alpha : \omega \longrightarrow : i \longmapsto \alpha_i
		\]
	una \vocab{enumerazione} di $A$ è una successione $a = \{a_i\}_{i \in \omega}$ tale che $A = \Imm(a)$, ossia, informalmente, $A = \{a_i | i \in \omega\}$.
\end{notation}

Il corollario dice che $A \ne \emptyset$ è al più numerabile se e solo se ha un'enumerazione.

\begin{example}
	$\ZZ$ è numerabile.
\end{example}

\begin{proof}
	
\end{proof}

\begin{example}
	$\QQ$ è numerabile.
\end{example}

\begin{proof}
	
\end{proof}

Adesso, ci piacerebbe poter dire che, se abbiamo un insieme $A$ al più numerabile, e tutti i suoi elementi sono, a loro volta, insiemi al più numerabili, allora
$\bigcup A$ è al più numerabile. D'altro canto è ragionevole: se esiste una enumerazione $\{a_i\}_{i \in \omega}$ di $A$, e, per ogni $i \in \omega$, esista una enumerazione $\alpha_i = \{a_{i,j}\}_{j \in \omega}$
di $a_i$, allora possiamo mandare surgettivamente $\omega \times \omega$ in $A$: $(i,j) \mapsto \alpha_{i,j}$, e, siccome $\omega \times \omega$ è al più numerabile, lo è anche $A$.
L'ereditariamente è credere di poter fissare una $\alpha_i$ per ogni $i \in \omega$.\\
Usando l'assioma della scelta potremo farlo, ma, per ora, non abbiamo modo, in generale, di procurarci la funzione $i \mapsto \alpha_i$. Possiamo però 
assumere di averla, così si corregge il ragionamento impreciso di prima.


\begin{proposition}
	Sia $A = \{a_i | i \in \omega\}$ e sia $\{\alpha_i\}_{i \in \omega}$ una successione di funzioni tali che, per ogni $i \in \omega$, $\alpha_i : \omega \rightarrow a_i$ è 
	una enumerazione di $a_i$. Allora $|\bigcup A| \leq \aleph_0$.
\end{proposition}

\begin{proof}
	Basta osservare che:
	\[ f : \omega \times \omega \longrightarrow \bigcup A : (i,j) \longmapsto \alpha_i(j)
		\]
	è surgettiva.
\end{proof}

\begin{notation}

\end{notation}


\end{document}