\documentclass[11pt]{scrartcl}
\usepackage[italian]{babel}
\usepackage[sexy]{evan}
\usepackage{float}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% BOOST SOFTWARE LICENSE - VERSION 1.0 - 17 AUGUST 2003
%
% Copyright (c) 2022 Evan Chen [evan at evanchen.cc]
% https://web.evanchen.cc/ || github.com/vEnhance
%  
% Available for download at:
% https://github.com/vEnhance/dotfiles/blob/main/texmf/tex/latex/evan/evan.sty
%
% Permission is hereby granted, free of charge, to any person or organization
% obtaining a copy of the software and accompanying documentation covered by
% this license (the "Software") to use, reproduce, display, distribute,
% execute, and transmit the Software, and to prepare derivative works of the
% Software, and to permit third-parties to whom the Software is furnished to
% do so, all subject to the following:
%
% The copyright notices in the Software and this entire statement, including
% the above license grant, this restriction and the following disclaimer,
% must be included in all copies of the Software, in whole or in part, and
% all derivative works of the Software, unless such copies or derivative
% works are solely in the form of machine-executable object code generated by
% a source language processor.
%
% THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
% IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
% FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
% SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
% FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
% ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
% DEALINGS IN THE SOFTWARE.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\title{Elementi Di Teoria Degli Insiemi}
\subtitle{\large\normalfont\rmfamily\scshape APPUNTI DEL CORSO DI ELEMENTI DI TEORIA DEGLI INSIEMI \\ TENUTO DAL PROF. MARCELLO MAMINO}
\author{Diego Monaco \\ \textnormal{\href{d.monaco2@studenti.unipi.it}{d.monaco2@studenti.unipi.it}} \\ Università di Pisa}
\date{Anno Accademico 2022-23}
\maketitle
\newpage

\tableofcontents
\eject
\newpage

\section*{Premessa}
Queste dispense sono la quasi esatta trascrizione in \LaTeX\,delle dispense del corso di Elementi di teoria degli insiemi, tenuto dal prof. Marcello Mamino nell'anno accademico 2022-23 presso l'Università di Pisa.

\section*{Ringraziamenti}
Francesco Sorce, Rubens Martino, Lorenzo Picinelli.

\mbox{}
\vfill
\begin{wrapfigure}{R}{0.2\textwidth}
	\centering
	\href{https://creativecommons.org/licenses/by-nc/4.0/deed.it}{\includegraphics[width=0.2\textwidth]{licenza.png}}
\end{wrapfigure}

Quest'opera è stata rilasciata con licenza Creative Commons Attribuzione - Condividi allo stesso modo 4.0 Internazionale. Per leggere
una copia della licenza visita il sito web \href{http://creativecommons.org/licenses/by-sa/4.0/deed.it}{\textcolor{blue}{https://creativecommons.org/licenses/by-nc/4.0/deed.it}}.\\

\newpage
\section{Prologo nel XIX secolo}
La nascita della teoria degli insiemi è una storia complicata di cui so pochissimo. Però, persone che ne sanno molto più di me hanno sostenuto l'opinione che il problema seguente
abbia avuto un ruolo. Come che sia, è almeno un'introduzione possibile.

\begin{problem}
Data una serie trigonometrica:
\[ S(x) = c_0 + \sum_{i=1}^{+\infty}a_i\sin{(ix)}+b_i\cos{(ix)}
	\]
se, per ogni $x \in \RR$, sappiamo che $S(x)$ converge a 0, possiamo dire che i coefficienti $c_0,a_i,b_i$ sono tutti 0?
\end{problem}

Risolto positivamente da \href{https://it.wikipedia.org/wiki/Georg_Cantor}{\textcolor{purple}{Georg Cantor}} nel 1870.

\begin{definition}
Diciamo che $X \subseteq \RR$ è un \vocab{insieme di unicità} se, per ogni serie trigonometrica:
\[ S(x) = c_0 + \sum_{i=1}^{+\infty}a_i\sin{(ix)}+b_i\cos{(ix)}
	\]
vale la seguente implicazione:
\[ \text{$S(x)$ converge a 0 per tutti gli $x\not\in X$} \implies \text{tutti i coefficienti $c_0,a_i,b_i$ sono nulli}
	\]
\end{definition}

\begin{example}
	Per il risultato di Cantor, $\emptyset$ è di unicità.
\end{example}

\begin{problem}
	Quali sottoinsiemi di $\RR$ sono di unicità?
\end{problem}

\begin{fact}
\label{unicità}
$X \subseteq \RR$ è di unicità se (ma non solo se) ogni funzione continua $f : \RR \longrightarrow \RR$ che soddisfi le ipotesi seguenti è necessariamente lineare\footnote{$f(x) = \alpha x + \beta$.}:
\begin{itemize}
	\item per ogni intervallo aperto $\left]a,b\right[$ con $]a,b[ \cap X = \emptyset$, $f_{|\left]a,b\right[}$ è lineare;
	\item per ogni $x \in \RR$, se $f$ ha derivate destre e sinistre in $x$, allora queste coincidono\footnote{Ovvero $f$ non ha punti angolosi.}.
\end{itemize}
\end{fact}

\begin{example}
	$X = \{\ldots,a_{-2},a_{-1},a_0,a_1,a_2,\ldots\} = \{a_i | i \in \ZZ\}$ con $\ldots < a_{-2} < a_{-1} < a_0 < a_1 < a_2 <\ldots$, $\displaystyle\lim_{i \to +\infty} a_i = +\infty$, $\displaystyle\lim_{i \to -\infty} a_i = -\infty$ ha la 
	proprietà data dal \hyperref[unicità]{Fatto 1.5}, quindi è di unicità.
\end{example}

\begin{notexample}
L'intervallo $[0,1]$ o $\RR$ non hanno la proprietà espressa dall'\hyperref[unicità]{Fatto 1.5}.
\end{notexample}

\begin{notexampleb}
Per l'\vocab{insieme di Cantor} non vale il \hyperref[unicità]{Fatto 1.5}.
\end{notexampleb}

Possiamo costruire l'insieme di Cantor a partire dall'intervallo $C_0 = [0,1]$ nel seguente modo:

\begin{center}
	\begin{figure}[h]
		\centering
		\includegraphics[width=12.5cm]{immagini/cantor.png}
	\end{figure}
\end{center}

ovvero, preso l'intervallo $[0,1]$ possiamo dividerlo in tre parti e rimuovere la parte centrale $\displaystyle\left]\frac 13, \frac 23\right[$, chiamiamo gli intervalli rimanenti $C_1$, possiamo iterare il procedimento sui due segmenti di $C_1$ ed ottenere $C_2,C_3,\ldots$, a questo punto 
definiamo l'insieme di Cantor $C$ come:
\[ C := \bigcap_{i \in \NN}C_i
	\]
Esiste una funzione continua (e crescente) $f : \RR \longrightarrow \RR$ detta \vocab{scala di Cantor} (o \vocab{scala del diavolo}), tale che $f^{\prime}(x) = 0$ per $x \not\in C$ e non è 
derivabile in $x \in C$.

\begin{center}
	\begin{figure}[h]
		\centering
		\includegraphics[width=13.5cm]{immagini/scalacantor.png}
	\end{figure}
\end{center}

tale funzione si costruisce aggiungendo tratti costanti (prima $\displaystyle\frac 12$, poi $\displaystyle\frac 14$, $\displaystyle\frac 34$ e così via, dividendo l'intervallo $[0,1]$ sull'asse delle ordinate in parti uguali) alle parti eliminate sull'intervallo
$[0,1]$ sull'asse delle ascisse per costruire l'insieme di Cantor.

\begin{note}
Per $\QQ$ e $C$ non vale il \hyperref[unicità]{Fatto 1.5} ma, in realtà, sono di unicità.
\end{note}

\begin{exampleb}
L'insieme degli elementi di una successione crescente col suo limite è un esempio di insieme di unicità.
\end{exampleb}

\begin{center}
	\begin{figure}[h]
		\centering
		\includegraphics[width=10.5cm]{immagini/succunic.png}
	\end{figure}
\end{center}

Dimostriamo quindi che $X$ è un insieme di unicità.
\begin{proof}
La funzione $f$ è lineare in $]-\infty, a_0[, ]a_0,a_1[, ]a_1,a_2[, \ldots$. Quindi nei punti $a_0,a_1,a_2,\ldots$ ammette derivata destra e sinistra. 
Siccome questi punti non possono essere angolosi, $f_{|]-\infty, a_0[}$, $f_{|]a_0,a_1[}$, etc. hanno lo stesso coefficiente angolare, quindi, sfruttando la cardinalità, $f_{|]-\infty, a_0[}$
è lineare. Siccome $f_{|]-\infty, a_0[}$ è lineare, usando nuovamente l'assenza di punti angolosi abbiamo la tesi.
\end{proof}

\begin{examplebb}
L'insieme degli elementi di una successione crescente di successioni crescenti è un insieme di unicità.
\end{examplebb}

\begin{center}
	\begin{figure}[h]
		\centering
		\includegraphics[width=12.5cm]{immagini/succunic2.png}
	\end{figure}
\end{center}

Dimostriamo che $X$ è di unicità.
\begin{proof}
In ciascuno degli intervalli $]a_{i0}, a_{(i+1)0}[$, $f$ è lineare, ragionando come nell'esempio precedente, ci siamo ridotti alla situazione
- di nuovo - dell'esempio precedente con $a_i^{\prime} = a_{i0}$.
\end{proof}

\subsection{Digressione: insiemi numerabili}
\begin{definition}
	Un insieme $X$ è \vocab{numerabile} se è il supporto di una successione, $X = \{a_0,a_1,a_2,\ldots\} = \{a_i | i \in \NN\}$, con $a_i \ne a_j$ per ogni $i \ne j$.\footnote{O in altre parole se esiste $f : \NN \longrightarrow X$ biunivoca.}
\end{definition}

\begin{example}
	Alcuni esempi di insiemi numerabili sono:
	\begin{itemize}
		\item $\NN$, l'insieme dei numeri naturali, infatti, la successione $a_i = i$ realizza la bigezione.
		\item I numeri dispari, con la bigezione data da $a_i = 2i + 1$.
		\item I numeri primi, $a_i = p_i$, con $p_i$ $i$-esimo numero primo.
		\item $\ZZ$ l'insieme dei numeri interi, con la bigezione data da $a_i = \displaystyle (-1)^i \left\lceil\frac{i}{2}\right\rceil$.
	\end{itemize}
\end{example}

\begin{examplem}
L'insieme $\NN \times \NN = \{(x,y) | x,y \in \NN\}$ è numerabile.
\end{examplem}

\begin{proof}
La funzione $f : \NN \times \NN \longrightarrow \NN : (x,y) \longmapsto 2^x(1+2y) - 1$ è biunivoca (perché?), quindi $a_i = f^{-1}(i)$ enumera $\NN \times \NN$.
\end{proof}

\begin{proposition}
Un sottoinsieme infinito di un insieme numerabile è, a sua volta, numerabile.
\end{proposition}

\begin{proof}
Sia $Y \subseteq X$ con $Y$ infinito e $X = \{a_i | i \in \NN\}$. La sottosuccessione $b_j = a_{i_j}$ degli $a_*$ che appartengono a $Y$ enumera $Y$. A essere precisi 
bisognerebbe dire esattamente chi sono gli indici $i_j$. Per ricorsione:
\[ i_0 = \min\{i | a_i \in Y\} \qquad i_{j+1} = \min\{i > i_j | a_i \in Y\}
	\]
dove i minimi esistono perché $Y$ non è finito.
\end{proof}

\begin{proposition}
Se $X$ e $Y$ sono numerabili $X \times Y = \{(a,b) | a \in X, b \in Y\}$ è anch'esso numerabile.
\end{proposition}

\begin{proof}
Fissiamo $X = \{a_i | i \in \NN\}$, $Y = \{b_j | j \in \NN\}$. Siccome $\NN \times \NN$ è numerabile, $\NN \times \NN = \{(i_t,j_t)|t \in \NN\}$.
Quindi $X \times Y = \{(a_{i_t}, a_{j_t}) | t \in \NN\}$.
\end{proof}

\begin{example}
$\QQ$ è numerabile.
\end{example}

\begin{proof}
$\QQ$ è in corrispondenza biunivoca con:
\[F = \{(\text{num.},\text{den.})\footnote{num. = numeratore, den. = denominatore.} | \text{num. $\in \ZZ$} \wedge \text{den. $\in\NN_{>0}$} \wedge \text{M.C.D.(num.,den.) = 1}\} \subseteq \ZZ \times \NN\]
\end{proof}

\begin{notexample}
$\RR$ non è numerabile.
\end{notexample}

\begin{proof}
Supponendo, per assurdo, che $\RR = \{a_i | i \in \NN\}$, cerchiamo un $x \in \RR$ che non compare fra gli $a_i$. Allo scopo, costruiamo la sottosuccessione $a_{i_j}$
definita per ricorrenza da:
\[ i_0 = 0 \qquad i_1 = \min\{i | a_i > a_0\} \qquad i_{j+1} = \min\{i | \, \text{$a_i$ è compreso tra $a_{j-1}$ e $a_j$}\}
	\]
graficamente:

\begin{center}
	\begin{figure}[h]
		\centering
		\includegraphics[width=10.5cm]{immagini/RRnum.png}
	\end{figure}
\end{center}

Si vede facilmente (esercizio!) che la successione $\{a_{i_{2k}}\}_k$ è crescente, $\{a_{i_{2k+1}}\}_k$ è decrescente 
e $\displaystyle \lim_{k \to +\infty} a_{i_{2k}} \leq \lim_{k \to +\infty}a_{i_{2k+1}}$. Fissiamo $x$ tale che $\displaystyle \lim_{k \to +\infty} a_{i_{2k}} \leq x \leq \lim_{k \to +\infty} a_{i_{2k+1}}$.
Chiaramente $x$ non è nessuno degli $a_{i_j}$, perché $a_{i_2k} < x < a_{i_{2k+1}}$. Supponiamo $x = a_n$, allora ci sarà $j$ tale che $i_j < n < i_{j+1}$, ma 
questo è assurdo perché allora $x = a_n$ è compreso fra $a_{i_{j-1}}$ e $a_{i_j}$, però $n < i_{j+1}$ contro la minimalità di quest'ultimo.

\begin{exercise}
Completare la dimostrazione nel caso $n < i$.
\end{exercise}

\begin{exercise}
Dimostrare che l'insieme di Cantor $C$ non è numerabile.
\end{exercise}
\end{proof}

\pagebreak
\subsection{Tornando agli insiemi di unicità}

\begin{theorem}
[Cantor-Lebesgue]
\label{CL}
Se $X \subseteq \RR$ è chiuso e numerabile, allora $X$ soddisfa il \hyperref[unicità]{Fatto 1.5}, ed è, quindi, di unicità.
\end{theorem}

La strategia di dimostrazione passa attraverso una definizione.

\begin{definition}
Dato $X \subseteq \RR$, il \vocab{derivato di Cantor-Bendixson} di $X$ è:
\[ X^{\prime} = X \setminus\{\text{punti isolati di $X$}\}
	\]
(dove $a \in X$ è un \vocab{punto di accumulazione} se $\exists \varepsilon > 0 : ]a - \varepsilon, a + \varepsilon[ \cap X = \{a\}$).
\end{definition}

\begin{remark}
Se $X$ è chiuso e per $X^{\prime}$ vale il \hyperref[unicità]{Fatto 1.5}, allora anche per $X$ vale il \hyperref[unicità]{Fatto 1.5}.
\end{remark}

Dimostriamo questo fatto.

\begin{proof}
Occorre dimostrare che se $f$ è continua, lineare, ristretta agli intervalli aperti che non intersecano $X$, e non ha punti angolosi, allora $f$ è
lineare ristretta agli intervalli aperti che non intersecano $X^{\prime}$. Fatto questo, usando l'ipotesi su $X^{\prime}$, $f$ è lineare - abbiamo quindi
mostrato che per $X$ vale \hyperref[unicità]{Fatto 1.5}.\\
Sia $]a,b[ \cap X^{\prime} = \emptyset$, dobbiamo dire che $f_{|]a,b[}$ è lineare. Ci basta dire che per ogni $\varepsilon > 0$, $f_{|[a+\varepsilon, b-\varepsilon]}$ è lineare.
Siccome $]a,b[ \cap X^{\prime} = \emptyset$, $]a,b[ \cap X = \{\text{punti isolati di $X$}\}$. Quindi $[a+\varepsilon, b-\varepsilon] \cap X$ è finito - se così non fosse, avrebbe un punto di accumulazione 
$\alpha$ che non può essere un punto isolato di $X$ (altrimenti si avrebbe un assurdo). Per cui $f_{|[a+\varepsilon, b-\varepsilon]}$ è lineare a tratti, e, siccome non ha punti angolosi, è lineare.
\end{proof}

\begin{corollary}
Sia $X^{(n)} = X^{\prime\prime\ldots\footnote{$n$ volte.}}$. Se $X^{(n)} = \emptyset$ per qualche $n \in \NN$, allora per $X$ vale il \hyperref[unicità]{Fatto 1.5}.
\end{corollary}

\begin{proof}
Induzione su $n$.
\end{proof}

Il guaio è che ci sono chiusi numerabili per cui $X^{(n)} \ne \emptyset$, qualunque sia $n$.

\begin{example}
Vogliamo costruire $X$ chiuso e numerabile tale che $X^{(n)} \ne \emptyset$ per ogni $n \in \NN$. Cominciamo col rivedere alcuni esempi già visti.
\end{example}

\begin{center}
	\begin{figure}[h]
		\centering
		\includegraphics[width=14.5cm]{immagini/es1.png}
	\end{figure}
\end{center}

Tutti i punti sono isolati, $X^{\prime} = \emptyset$.

\pagebreak

\begin{center}
	\begin{figure}[h]
		\centering
		\includegraphics[width=14.5cm]{immagini/es2.png}
	\end{figure}
\end{center}

``Successione con punto limite". Tutti i punti sono isolati salvo $l$, quindi $X^{\prime} = \{l\}$ e $X^{\prime\prime} = \emptyset$.

\begin{center}
	\begin{figure}[h]
		\centering
		\includegraphics[width=14.5cm]{immagini/es3.png}
	\end{figure}
\end{center}

``Successione di successioni", $X^{\prime} = \{a_{10}, a_{20}, \ldots, l\}$, $X^{\prime\prime} = \{l\}$ e $X^{\prime\prime\prime} = \emptyset$.\\
Si vede che possiamo proseguire, in qualche modo, costruendo una successione di successioni di successioni, etc. $n$ volte, $X_n$. Avremo $X_n^{(n)} \ne \emptyset$, $X_n^{(n+1)} = \emptyset$. Ora costruiamo 
$X_{\omega}$ fatto così:

\begin{center}
	\begin{figure}[h]
		\centering
		\includegraphics[width=14.5cm]{immagini/esomega.png}
	\end{figure}
\end{center}

È chiaro che, per ogni $n$, $X_\omega^{(n)} \ne \emptyset$. D'altro canto, $X_\omega$ soddisfa il \hyperref[unicità]{Fatto 1.5}, perché $f$ deve essere lineare in ciascuno degli intervalli
$[a_n,a_{n+1}]$, perché $X_{n+1}$ soddisfa il \hyperref[unicità]{Fatto 1.5}, quindi ci si riduce al caso della successione.

\begin{exercise}
Perché $X_\omega$ è numerabile?
\end{exercise}

Ora potremmo pensare che, pazienza se $X_\omega$ non si smonta a furia di derivati, sarà un caso particolare. Però adesso, possiamo fare una successione di insiemi come $X_\omega$, chiamiamola $X_{\omega+1}$, e 
una successione di questi $X_{\omega+2}$, etc.\\
Al diavolo, serve un nuovo corollario!

\begin{corollary}
Se $X^{(n)}$ è di ``tipo $X_\omega$", allora per $X$ vale il \hyperref[unicità]{Fatto 1.5}.
\end{corollary}

Ok, questo corollario copre $X_\omega$, $X_{\omega + 1}$, $X_{\omega + 2}$, ma copre anche $X_{\omega \cdot 2}$?
\pagebreak
\begin{center}
	\begin{figure}[h]
		\centering
		\includegraphics[width=14.5cm]{immagini/2omega.png}
	\end{figure}
\end{center}

No: occorre un nuovo corollario.

\begin{corollary}
Se $X^{(n)}$ è di ``tipo $X_{\omega \cdot 2}$", allora per $X$ vale il \hyperref[unicità]{Fatto 1.5}.
\end{corollary}

E poi un altro per $X_{\omega \cdot 3}$, e un altro per $X_{\omega \cdot 4}$, etc.\\
E ora abbiamo finito? No, perché possiamo costruire una nuova successione con $X_{\omega},X_{\omega \cdot 2},X_{\omega \cdot 3}$, etc.\\
Se chiamiamo questa follia $X_{\omega \cdot \omega}$, ecco che si riparte a fare successioni di $X_{\omega \cdot \omega}$. Ora si sarà capito che definiremo
una serie aritmetica di queste cose, per cui potremo fare anche $\omega^\omega$, $\omega^{\omega^{\omega}}$, etc. È questa la soluzione allora?\\
No, ogni sforzo di trovare l'induzione a capo delle induzioni è vano. Se ho $X_{\omega}$, $X_{\omega^\omega}$, $X_{\omega^{\omega^{\omega}}}$, etc., allora,
ecco che faccio una successione con queste cose, la battezzo in qualche modo - ad esempio, $X_{\varepsilon_0}$ - e si riparte!\\
Per smontare ogni possibile insieme chiuso e numerabile occorre un \textbf{nuovo tipo di induzione}, l'\vocab{induzione transfinita}, che è strettamente più potente dell'induzione aritmetica.
Questa tecnica è stata sviluppata da Cantor, forse prendendo le mosse dal problema degli insiemi di unicità, e sarà uno degli argomenti centrali del corso.

\begin{exercise}[per la fine del corso]
Dimostrare il teorema di \hyperref[CL]{Cantor-Lebesgue}.
\end{exercise}

\subsection{Giochi di parole}
Descrivere un oggetto matematico non basta per crearlo. Se bastasse, si incorrerebbe in contraddizioni come queste.
\paragraph*{Paradosso di Russell}\mbox{}\\
Tipicamente le collezioni - uso questa parola perché daremo, al termine ``insieme", un senso tecnico preciso - non sono membro di se stesse: la collezione di 
tutti i numeri primi non è un numero primo. Però ci sono anche collezioni che sono membri di se stessi: per esempio la collezione di tutte le collezioni. Consideriamo:
\[ N = \{\text{collezioni $X$}\, | X \not\in X \}
	\]
la collezione delle collezioni che non sono membri di se stessi - la $N$ sta per collezioni normali. Quindi ci chiediamo se $N \in N$ oppure no? $N \in N$ se e solo se per definizione $N \not \in N$, che è assurdo.\\
Il paradosso di Russell ci dice che, del principio di collezione - ossia l'idea che data una proprietà ben definita $P$ si possa costruire la collezione $\{X | P(X)\}$ - non ci si può fidare.

\paragraph*{Paradosso di Berry}\mbox{}\\
L'italiano annovera un numero finito di parole, è quindi possibile formare solo un numero finito di frasi di meno di cento parole. Alcune di queste descrivono un numero naturale, altre no. Comunque, solo un numero 
finito di numeri naturali può essere descritto con meno di cento parole. Per il principio del minimo, esiste:
\begin{align*}
	h = \text{``il più piccolo numero naturale che l'italiano non può} \\ 
 \text{descrivere con meno di cento parole"}
\end{align*}
Il guaio chiaramente, è che lo abbiamo appena descritto con sedici parole.\\
Quindi non ci si può fidare troppo neppure dell'italiano, o meglio, non è possibile descrivere precisamente cosa sia una descrizione precisa.\\
In conclusione, occorre fissare un linguaggio formale in cui si esprimano le proposizioni della teoria degli insiemi, e occorre fissare un sistema di assiomi, espressi in questo linguaggio, che 
dicano quali costruzioni sono lecite: quali insiemi esistono. Il ruolo della teoria degli insiemi è, poi, di fondare l'edificio della matematica. L'ambizione, quindi, è che il linguaggio e gli assiomi della teoria degli insiemi, 
siano in realtà, il linguaggio e gli assiomi della matematica.

\subsection{Scopi del corso}
Questo corso persegue due obiettivi:
\begin{enumerate}[(1)]
	\item Studiare i \textbf{fondamenti della matematica}, nella forma più comunemente accettata nel XX secolo e fino ad ora, la teoria degli insiemi di 
	\href{https://it.wikipedia.org/wiki/Ernst_Zermelo}{\textcolor{purple}{Zermelo}}-\href{https://it.wikipedia.org/wiki/Adolf_Abraham_Halevi_Fraenkel}{\textcolor{purple}{Fraenkel}} con l'assioma della scelta (ZFC).
	\item Studiare tecniche e strumenti che sono stati sviluppati grazie alla teoria degli insiemi, per esempio: la teoria delle cardinalità, la teoria dei numeri ordinali, l'induzione e la ricorsione transfinita.
\end{enumerate}

In questo corso non ci occupiamo dei modelli della teoria degli insiemi. Mi spiego. Per esempio, in teoria dei gruppi si assiomatizza cosa sia un gruppo, e poi si studia come possano essere fatti i diversi gruppi. In 
teoria degli insiemi si assiomatizza l'universo di tutti gli insiemi, però, per il teorema di incompletezza di \href{https://it.wikipedia.org/wiki/Kurt_G%C3%B6del}{\textcolor{purple}{Gödel}}, questa assiomatizzazione non 
può essere completa. Quindi esistono tanti universi insiemistici possibili. Indagare queste possibilità - i modelli della teoria degli insiemi - è argomento di corsi più avanzati.

\newpage
\section{Il linguaggio della teoria degli insiemi}
Per non incorrere in contraddizione, accettiamo che le sole proposizioni ad avere senso siano quelle esprimibili mediante \vocab{formule insiemistiche}. Le formule si costruiscono ricorsivamente.
\begin{itemize}
	\item Le lettere $a,b,c,\ldots,A,B,C,\ldots,\alpha,\beta,\gamma,\ldots$ rappresentano \vocab{variabili}. I valori delle variabili sono sempre insiemi, e non ci sono altri oggetti salvo gli insiemi.
	\item Le \vocab{formule atomiche} sono:
	\[ \text{variabile = variabile} \qquad \qquad \text{variabile $\in$ variabile}\footnote{\,``appartiene a".}
		\]
	sono formule atomiche $x=y$, $x=x$, $\alpha = C$, e anche $x \in y$, $x \in x$, $\alpha \in C$.
	\item Le formule atomiche si combinano tra loro mediante:
	\begin{itemize}
		\item \vocab{connettivi logici} ovvero il ``non'' la ``e'' e la ``o'' (inclusiva):
		\[ \text{$\neg$ formula} \qquad \text{formula $\land$ formula} \qquad \text{formula $\lor$ formula}
			\]
		quindi ad esempio:
		\begin{flalign*}
			&\neg\Phi \equiv \text{``$\Phi$ è falsa''} &\\
			&\Phi \land \psi \equiv \text{``$\Phi$ e $\psi$ sono entrambe vere''} &\\
			&\Phi \lor \psi \equiv \text{``almeno una fra $\Phi$ e $\psi$ è vera''}
		\end{flalign*}
		\item \vocab{quantificatori} ovvero quello universale ``per ogni'' e quello esistenziale ``esiste'':
		\[ \forall x \, \text{formula} \qquad \exists x \, \text{formula}
			\]
		ad esempio:
		\begin{flalign*}
			&\forall x \, \Phi \equiv \text{``$\Phi$ è vera qualunque sia l'insieme $x$''} &\\
			&\exists x \, \Phi \equiv \text{``c'è un insieme $x$ che fa si che $\Phi$ sia vera''}
		\end{flalign*}
		\begin{exercise}
			Chiaramente varranno $\forall x \, x = x,$ $ \forall x \, \exists y \, x = y,$ $ \neg (\exists x \, \forall y \, x = y)$.
		\end{exercise}
	\end{itemize}
\end{itemize}

\textbf{\underline{L'intuizione}} è che l'universo insiemistico sia un gigantesco \href{https://it.wikipedia.org/wiki/Digrafo_aciclico}{\textcolor{purple}{grafo diretto aciclico}} i cui vertici sono gli insiemi,
ed in cui le frecce rappresentano la relazione di appartenenza.

\begin{center}
	\begin{figure}[h]
		\centering
		\includegraphics[width=10.5cm]{immagini/graf.png}
	\end{figure}
\end{center}

Possiamo solo fare affermazioni a proposito di vertici e frecce di questo grafo. Per esempio:
\[ \text{``$a$ è un elemento di un certo $b$''} \equiv \text{``c'è un percorso di due frecce fra $a$ e $b$''} 
	\]
che corrisponde mediante formule insiemistiche a $ \exists x (a \in x \land x \in b)$. E ancora:
\[\text{``$a$ è un sottoinsieme di $b$''} \equiv \text{``ogni elemento di $a$ è elemento di $b$''} \equiv \]\[
		\equiv\text{``non c'è un insieme che è elemento di $a$ e non di $b$''}\equiv\]\[
	 \equiv \text{``non c'è un vertice con una freccia verso $a$ e non una verso $b$''}
	\]
che corrisponde mediante formule insiemistiche a $\neg\exists x (x \in a \land \neg x \in b)$ (tutto ciò che raggiunge $a$ deve raggiungere anche $b$).\\
\textbf{\underline{Parentesi}} Ad essere precisi, avremmo dovuto definire le formule includendo un mucchio di parentesi, allo scopo di eliminare ogni possibilità
di formare una combinazione di simboli ambigua. Per esempio $\textcolor{red}{\Phi_1 \land \Phi_2 \lor \Phi_3}$ è ambigua, perché si potrebbe leggere $(\Phi_1 \land \Phi_2) \lor \Phi_3$
o $\Phi_1 \land (\Phi_2 \lor \Phi_3)$. In una notazione completamente parentesizzata, per esempio, la formula per ``$a$ è un sottoinsieme di $b$'' sarebbe:
\[ \neg(\exists x((x \in a)\land(\neg(x \in b))))
	\]
Non useremo, in generale, questa notazione, ma useremo le parentesi selettivamente per evitare ambiguità. \footnote{Mi riservo in queste dispense di modificare un pochino questa regola, qualora alcune formule risultassero più leggibili con le parentesi.}\\
\textbf{\underline{Abbreviazioni}} Le formule appena descritte costituiscono il linguaggio della teoria degli insiemi \textbf{puro}. Durante il corso estenderemo
più volte questo linguaggio mediante abbreviazioni, che semplicemente rimpiazzano formule più lunghe con scritture convenzionali più compatte, e quindi non alterano 
la potenza espressiva del linguaggio. Vediamo le prime abbreviazioni:
\[ x \ne y \Mydef \neg x = y \footnote{Cioè ``non è vero che $x$ è uguale a $y$''.} \qquad x \not\in y \Mydef \neg x \in y \qquad \not\exists x \,\Phi \Mydef \neg \exists x \, \Phi
	\]\[ \Phi \rightarrow \psi \Mydef \psi \lor \neg \Phi \qquad \Phi \leftrightarrow \psi \Mydef (\Phi \rightarrow \psi) \land (\psi \rightarrow \Phi)
		\]\[ \exists x \in y \; \Phi \Mydef \exists x (x \in y \land \Phi) \qquad \forall x \in A \; \Phi \Mydef \forall x (x \in A \rightarrow \Phi)
			\]\[ \exists !\, x\, \Phi(x) \Mydef \exists x (\Phi(x) \land \forall y(\Phi(y) \rightarrow y = a))
				\]\[ \exists !\, x \in A \,\Phi(x) \Mydef \exists! \, x(x \in A \land \Phi(x))
					\]\[ A \subseteq B \Mydef \forall x (x \in A \rightarrow x \in B) \qquad A \subsetneq B \Mydef( A \subseteq B) \land (A \ne B)
						\]\[ C = A \cup B \Mydef \forall x \, x \in C \leftrightarrow (x \in A \lor x \in B)
							\]\[ C = A \cap B \Mydef \forall x \, x \in C \leftrightarrow (x \in A \land x \in B)
								\]
\begin{note}
	Il fatto che possiamo dire $C = A \cup B$ o $C = A \cap B$ non significa né che questi oggetti esistano né che siano unici. Dimostreremo fra poco l'esistenza e unicità 
	di unione e intersezione.
\end{note}

\begin{exercise}
Esprimi queste proposizioni mediante formule insiemistiche pure:
\begin{itemize}
	\item gli elementi degli elementi di $A$ sono elementi di $A$;
	\item $B$ è l'insieme dei sottoinsiemi di $A$;
	\item l'unione degli elementi di $A$ è l'intersezione di quelli di $B$\footnote{Qui assumi che l'unione e intersezione esistano e siano uniche.}
\end{itemize}
\end{exercise}

\subsection{Le regole di inferenza}
La teoria assiomatica degli insiemi si compone di tre parti: il linguaggio formale che abbiamo appena descritto, gli assiomi della teoria che studieremo durante il corso, 
ed un sistema di regole che specificano precisamente quali passaggi sono leciti nelle dimostrazioni. Possiamo immaginare questa ultima componente come una specie di algebra dei ragionamenti,
che permette di verificare i passaggi di una dimostrazione in maniera puramente meccanica, come se fossero semplici manipolazioni algebrica. Noi non vedremo le regole di inferenza, e voglio spiegare qui il perché.
\begin{enumerate}[1]
	\item Sono argomento del corso di logica.
	\item In realtà, scrivere le dimostrazioni in maniera formale, le renderebbe lunghissime e particolarmente incomprensibili.
	\item In pratica, non si sbaglia facendo ragionamenti che non reggono, si sbaglia dicendo cose fumose che non possono essere espresse nel linguaggio della teoria. Per esempio, le parole ``e così via'' sono pericolose.
	\item Conoscere le regole - fidatevi - non aiuta né a trovare né a capire le dimostrazioni.
\end{enumerate}
Pur senza dare un sistema completo di regole, vediamo qualche manipolazione formale che potrebbe servire.\\
\textbf{\underline{Tavole di verità}} Due combinazioni mediante connettivi logici ($\neg$, $\land$, $\lor$, $\rightarrow$, $\leftrightarrow$)
delle stesse formule - ``\vocab{combinazioni booleane}'' - alle volte, dicono la stessa cosa. Per esempio, $\neg \Phi \lor \neg \psi \equiv\footnote{\,``equivale a''.} \neg (\Phi \land \psi)$.
Per verificare questo fatto basta considerare tutte le possibili combinazioni di valori di verità che possono assumere le formule combinate - nell'esempio $\Phi$ e $\psi$ - compilando una ``\vocab{tabella di verità}''.
\begin{center}
	\begin{tabular}{>{$}l<{$}>{$}l<{$}|*{7}{>{$}l<{$}}}
	\Phi & \psi & \neg\Phi   & \neg\psi   & \neg\Phi \lor \neg\psi   & \Phi \land \psi & \neg(\Phi \land \psi)    \\
	\hline\vrule height 14pt width 0pt
	V & V & F & F & \textcolor{red}{F} & V & \textcolor{red}{F}\\
	V & F & F & V & \textcolor{red}{V} & F & \textcolor{red}{V}\\
	F & V & V & F & \textcolor{red}{V} & F & \textcolor{red}{V}\\
	F & F & V & V & \textcolor{red}{V} & F & \textcolor{red}{V}
	\end{tabular} 
\end{center}
Come si osserva le due colonne corrispondenti ai valori di verità delle nostre formule iniziali hanno gli stessi valori di verità in ogni caso.\\
Conviene tenere a mente alcune delle equivalenze elementari:
\[ \neg\neg \Phi \equiv \Phi \qquad \Phi \land (\psi \lor \Theta) \equiv (\Phi \land \psi) \lor (\Phi \land \Theta) \qquad \Phi \lor (\psi \land \Theta) \equiv (\Phi \lor \psi) \land (\Phi \lor \Theta)
	\]\[ \neg(\Phi \land \psi) \equiv \neg \Phi \lor \neg \psi \qquad \neg(\Phi \lor \psi) = \neg \Phi \land \neg \psi \, \footnote{\href{https://it.wikipedia.org/wiki/Leggi_di_De_Morgan}{\textcolor{purple}{Leggi di De Morgan}}.}
		\]\[ \Phi \rightarrow \neg \psi \equiv \psi \rightarrow \neg \Phi \qquad \Phi \rightarrow \psi \equiv \neg \psi \rightarrow \neg \Phi
			\]

\begin{exercise}
Dimostrare le equivalenze delle formule elencate sopra.
\end{exercise}

Per quanto riguarda i quantificatori ricordiamo le regole seguenti, che tuttavia non sono esaustive.
\[ \neg\forall x \, \Phi \equiv \exists x \, \neg\Phi \qquad \neg\forall x \, \neg \Phi \equiv \exists x \, \Phi
	\]\[ \neg\exists x \, \Phi \equiv \forall x \, \neg \Phi \qquad \neg \exists x \, \neg \Phi \equiv \forall x \, \Phi
		\]

\begin{exercise}
Convinciti della validità delle equivalenze precedenti.
\end{exercise}

\begin{exercise}
Dimostra che:
\[ \neg \forall x \in A \, \Phi \equiv \exists x \in A \, \neg \Phi \qquad \neg \exists x \in A \, \Phi \equiv \forall x \in A \, \Phi
	\]
\end{exercise}

\begin{exercise}
Dimostra che:
\[ \forall x (x \in A \rightarrow x \in B) \equiv \neg \exists x (x \in A \land \neg x \in B)
	\]
\end{exercise}

\begin{exercise}
Secondo te, la seguente formula è vera?
\[ \forall A ((\exists x \, x \in A) \rightarrow \exists x \in A (x \in B \rightarrow \forall y \in A \, y \in B))
	\]
\end{exercise}

Infine vi sono regole per la relazione di uguaglianza, che dicono, in sostanza, che se $x = y$ allora $x$ e $y$ non sono distinguibili, ossia vale $\Phi(x) \leftrightarrow \Phi(y)$ qualunque sia $\Phi$.
Per quanto ci riguarda, \textbf{se $x = y$ allora $x$ e $y$ sono nomi della stessa cosa}.

\newpage
\section{I primi assiomi}
\subsection{Assiomi dell'insieme vuoto e di estensionalità}
\begin{axiom}
[Assioma dell'insieme vuoto]
\label{ax1}
Esiste un insieme vuoto.
\[ \exists x \; \forall y \; y \not\in x
		\]
\end{axiom}

\begin{note}
Questo assioma non sarebbe strettamente necessario, in quanto potremmo ottenere un insieme vuoto anche come sottoprodotto, per esempio, dell'assioma dell'infinito che vedremo in seguito.
Tuttavia è bello poter partire avendo per le mani almeno un insieme.
\end{note}

\begin{axiom}
[Assioma di estensionalità]
\label{ax2}
Un insieme è determinato dalla collezione dei suoi elementi. Due insiemi coincidono se e solo se hanno i medesimi elementi.
\[ \forall a \; \forall b \; a = b \leftrightarrow \forall x (x \in a \leftrightarrow x \in b)
	\]
\end{axiom}

\begin{exercise}
Dimostra che la freccia $a = b \rightarrow \forall x (x \in a \leftrightarrow x \in b)$, in realtà, segue dal fatto che se $a = b$ allora $a$ e $b$ sono indistinguibili\footnote{Nel senso che abbiamo descritto in precedenza, cioè sono nomi della stessa cosa.}.
\end{exercise}

\textbf{\underline{Convenzione}} Le variabili libere (= non quantificate), se non specificato altrimenti, si intendono quantificate universalmente all'inizio della formula. Per cui possiamo scrivere
l'assioma di estensionalità semplicemente nella forma:
\[ a = b \leftrightarrow \forall x (x \in a \leftrightarrow x \in b)
	\]

\begin{proposition}[Unicità dell'insieme vuoto]
C'è un unico insieme vuoto.
\[ \exists ! \, x \; \forall y \; y \not \in x
	\]
\end{proposition}

\begin{proof}
Consideriamo due insiemi vuoti $x_1$ e $x_2$, ossia supponiamo $\forall y \, y \not\in x_1$, e $\forall y \, y \not \in x_2$. Allora:
\[ \forall y (y \in x_1 \leftrightarrow y \in x_2)
	\]
[sono coimplicate logicamente] perché $y \in x_1$ e $y \in x_2$ sono entrambe necessariamente false (quindi la proposizione così com'è scritta è sempre vera). Per \hyperref[ax2]{estensionalità}, la proposizione sopra (sempre vera) è equivalente a $x_1 = x_2$ (che quindi a sua volta sarà sempre vera), e quindi abbiamo la tesi.
\end{proof}

\emph{Dimostrazione formale.} Questo livello di pedanteria non è necessario, ma, per una volta, proviamo a dimostrare in ogni dettaglio la formula $\exists ! x (\forall y (y \not \in x))$. Per definizione di $\exists !$, ciò equivale a:
\[ \exists x_1 ((\forall y \, y \not \in x_1) \land \forall x_2 ((\forall y \, y \not \in x_2) \rightarrow x_2 = x_1))
	\]
Per l'\hyperref[ax1]{assioma del vuoto}, $\exists x_1 \, \forall y \, y \not \in x_1$: fissiamo questo $x_1$. Resta da dimostrare che:
\[ (\forall y \, y \not \in x_1) \land \forall x_2(\forall y \, y \not \in x_2) \rightarrow x_2 = x_1
	\]
Per costruzione, $\forall y \, y \not\in x_1$, è vera (avendo fissato $x_1$), quindi resta:
\[ \forall x_2 (\forall y \, y \not \in x_2) \rightarrow x_2 = x_1
	\]
Ora prendiamo un $x_2$ qualunque, dobbiamo dimostrare:
\[ \forall y (y \not \in x_2) \rightarrow x_2 = x_1
	\]
Si danno due casi: o $\forall y (y \not \in x_2)$ è vera o è falsa. Nel secondo caso, l'implicazione è vera per via della tabella di verità. Nel primo abbiamo sia $\forall y \, y \not \in x_1$, [vera] per
costruzione, sia $\forall y \, y \not \in x_2$, [vera] per ipotesi. Quindi, preso un qualunque $y$, $y \in x_1$ e $y \in x_2$ sono entrambe false. La tabella di verità di $\leftrightarrow$ ci dice quindi che vale $y \in x_1 \leftrightarrow y \in x_2$, e, per 
l'arbitrarietà di $y$:
\[ \forall y (y \in x_1 \leftrightarrow y \in x_2)
	\]
Dall'\hyperref[ax2]{assioma di estensionalità}:
\[ \forall y (y \in x_1 \leftrightarrow y \in x_2) \rightarrow x_1 = x_2
	\]
Abbiamo quindi $x_1 = x_2$, da cui segue la verità dell'implicazione iniziale. $\hfill\square$


Chiaramente, ho voluto scrivere questa dimostrazione delirante per convincervi che NON È UNA BUONA IDEA.

\begin{notation}
L'unicità dell'insieme vuoto ci giustifica ad introdurre delle nuove abbreviazioni:
\[ x = \emptyset \Mydef \forall y \, y \not\in x \qquad \emptyset \in x \Mydef \exists z (z = \emptyset \land z \in x)
	\]
\end{notation}

\subsection{Assioma di separazione}
\begin{axiom}
[Assioma di separazione]
\label{ax3}
Se $A$ è un insieme, e $\psi(x)$ una formula insiemistica qualunque, allora $\{x \in A | \psi (x)\}$\footnote{Stiamo usando già questa notazione, ma la definiremo a breve.} è un insieme.
\[ \forall A \; \exists B \; \forall x \; x \in B \leftrightarrow (x \in A \land \psi (x))
	\]
\end{axiom}

\begin{note}
Tecnicamente l'assioma di separazione è uno \vocab{schema di assiomi}, ossia una regola che, per ogni possibile formula $\psi$, ci permette di scrivere un assioma.
\end{note}

\begin{proposition}
Fissati $A$ e $\psi(x)$, l'insieme $\{x \in A | \psi(x)\}$ è univocamente definito. Ossia:
\[ \forall A \; \exists \textcolor{red}{!} B \; \forall x \; x \in B \leftrightarrow (x \in A \land \psi(x))
	\]
\end{proposition}

\begin{proof}
Come per l'unicità dell'insieme vuoto, supponiamo di avere $B_1$ e $B_2$ tali che:
\[ \forall x \, x \in B_1 \leftrightarrow (x \in A \land \psi(x)) \qquad \forall x \, x \in B_2 \leftrightarrow (x \in A \land \psi(x))
	\]
Allora, $\forall x \, x \in B_1 \leftrightarrow (x \in A \land \psi(x)) \leftrightarrow x \in B_2$, quindi ciò coimplica, per \hyperref[ax2]{estensionalità}, che $B_1 = B_2$.
\end{proof}

\begin{exercise}[Transitività della coimplicazione]
Verificare che se $\psi \leftrightarrow \Phi$ e $\Phi \leftrightarrow \Theta$, allora $\psi \leftrightarrow \Theta$.
\end{exercise}

\begin{notation}
Vista l'unicità, possiamo introdurre una nuova abbreviazione:
\[ B = \{x \in A | \psi(x)\} \Mydef \forall x \, x \in B \leftrightarrow (x \in A \land \psi(x))
	\]
\end{notation}

Osserviamo che l'assioma di separazione è una forma indebolita del principio di collezione\footnote{Quel principio che definisce gli insiemi come tutte le cose che soddisfano una certa formula.}. Rimpiazzando il principio con questo assioma, il Paradosso di Russell diventa una proposizione.

\begin{proposition}[Insieme di tutti gli inisemi]
Non esiste l'insieme di tutti gli insiemi.
\[ \not\exists V \; \forall x \; x \in V
	\]
\end{proposition}

\begin{proof}
Supponiamo, per assurdo, che esista questo $V$. Allora, per \hyperref[ax3]{separazione} con la formula $\psi (x) \equiv x \not \in x$, esiste l'insieme:
\[ N = \{x \in V | x \not\in x\}
	\]
che, per definizione (via separazione), ha la proprietà:
\[ \forall x \, x \in N \leftrightarrow (x \in V \land x \not \in x)
	\]
Per ipotesi assurda, $x \in V$ è sempre vera (stiamo considerando l'insieme di tutti gli insiemi), quindi quanto scritto si riduce a:
\[ \forall x \, x \in N \leftrightarrow x \not\in x
	\]
prendendo ora come insieme $N$: $x = N$, abbiamo $N \in N \leftrightarrow N \not\in N$, assurdo.
\end{proof}

\subsection{Classi e classi proprie}
Sebbene, abbiamo detto che gli unici oggetti della teoria degli insiemi sono gli insiemi, usualmente ci si riferisce alla collezione di tutti gli insiemi 
che soddisfano una certa formula come ad una specie di insieme: una \vocab{classe}. Più precisamente, data una formula $\psi(x)$, se diciamo: ``sia $C$ la classe degli insiemi $x$ tali che $\psi(x)$''
intendiamo dire che useremo la scrittura $x \in C$ come una semplice abbreviazione per la formula $\psi(x)$.\footnote{Ovvero per tutti gli oggetti (solo gli insiemi in questo caso) che soddisfano una tale formula $\psi(x)$.} \\
Non avrebbe senso scrivere \textcolor{red}{$C \in$ qualcosa}, perché il simbolo $\in$ in $x \in C$ non ha senso (ha senso solo tra oggetti di tipo insieme), se non nel tutt'uno $\in C$. In altri termini, se scriviamo $x \in C$ in luogo di $\psi(x)$ è solo come ausilio dell'intuizione (per comodità insomma, senza intendere qualcosa di formale all'interno della teoria degli insiemi):
avremmo potuto decidere di scrivere $x$\ding{168}, o nient'altro che $\psi(x)$.

\begin{definition}[Classe universale]
La classe $V$ si dice \vocab{classe universale} ed è la classe di tutti gli insiemi.
\[ x \in V \Mydef x = x \footnote{Cioè la classe degli insiemi che soddisfano il predicato $\psi(x): x = x$ (ovvero tutti gli insiemi per quanto assunto all'inizio della teoria), $V = \{x | \psi(x)\} = \{x | x = x\}$ (dove naturalmente non sto usando separazione ma il principio di collezione perché stiamo definendo una classe).}
	\]
\end{definition}

Insomma, scrivere $x \in V$ non dice molto: è una formula sempre vera.

\begin{notation}[Uguaglianza tra classi]
Date due classi $C$ e $D$, che, ricordiamo, non significa altro che ``date due formule$\ldots$'', definiamo l'abbreviazione:
\[ C = D \Mydef \forall x ((x \in C) \leftrightarrow (x \in D)) \footnote{Non è altro che un abbreviazione per dire che le formule che definiscono le classi $C$ e $D$ sono soddisfatte dagli stessi insiemi $x$.}
	\]
\end{notation}

Ora, dato un qualunque insieme $A$, possiamo definire la classe $\hat{A}$ degli $x$ tali che $x \in A$ (cioè la classe degli $x$ che soddisfano $\psi(x) : x \in A$). Se $\hat{A} = \hat{B}$, per l'abbreviazione data non stiamo dicendo altro che:
\[ \forall x ((x \in A) \leftrightarrow (x \in B))
	\]
che equivale $A = B$ per \hyperref[ax2]{estensionalità}. Ha quindi senso, con un leggero abuso di notazione, omettere il cappelletto $\hat{}$ e ``identificare'' la classe $\hat{A}$ semplicemente con $A$. In questo senso,
abbiamo classi che sono insiemi - formalmente $C$ è un insieme se $C = \hat{A}$ per qualche insieme $A$ - e classi che non sono insiemi. Chiamiamo \vocab{classe propria} una classe che non è un insieme.\footnote{Essere un insieme per una classe significa quindi moralmente identificarvisi nel senso riportato sopra, se ciò non fosse possibile parliamo di classi proprie.}

\begin{example}
$V$ è una classe propria.
\end{example}

\textbf{\underline{L'intuizione}}, che sarà più chiara via via che procediamo nel corso, è che le classi proprie sono troppo grandi per essere insiemi.

\subsection{Assioma del paio e coppia di Kuratowski}
I primi tre assiomi ci dicono, a grandi linee, che, entro i limiti di quanto si può fare rinunciando al principio di collezione - che esiste $\{x | \, \text{una qualunque proprietà}\}$ -, gli insiemi sono delle specie di collezioni.
Sono determinati dai loro elementi, e li si può dividere in collezioni più piccole in maniera arbitraria. \\ Ci troviamo, però, adesso, nella necessità di procurarci qualche insieme con cui lavorare. I prossimi assiomi serviranno per giustificare le costruzioni con cui,
usualmente, si definiscono nuovi insiemi. Per esempio, abbiamo bisogno di costruire certi insiemi di base, tipo l'insieme dei numeri interi o insiemi finiti i cui elementi sono elencati esplicitamente, fare prodotti di insiemi esistenti, 
considerare le funzioni fra insiemi esistenti, etc.

\begin{axiom}
[Assioma del paio]
\label{ax4}
Dati $a$ e $b$ esiste l'insieme $\{a,b\}$.
\[ \forall a \; \forall b \; \exists P \; \forall x \; x \in P \leftrightarrow (x = a \lor x = b)
	\]
\end{axiom}

\begin{proposition}
[Unicità del paio]
Fissati $a$ e $b$, l'insieme $\{a,b\}$ è univocamente determinato.
\[\forall a \; \forall b \; \exists\textcolor{red}{!} P \; \forall x \; x \in P \leftrightarrow (x = a \lor x = b)
	\]
\end{proposition}

\begin{exercise}
	Dimostra la proposizione precedente.
\end{exercise}

\begin{soln}
	Supponiamo che esistano $P_1$ e $P_2$ tali che:
	\[ \forall x (x \in P_1 \leftrightarrow (x = a \lor x = b)) \qquad \text e \qquad \forall x (x \in P_2 \leftrightarrow (x = a \lor x = b))
		\]
	da ciò segue che:
	\[ \forall x (x \in P_1 \leftrightarrow x \in P_2)
		\]
	dunque per \hyperref[ax2]{estensionalità} l'espressione sopra equivale a $P_1 = P_2$.
\end{soln}

\begin{proposition}[Esistenza dei singoletti]
	Dato $a$, esiste ed è unico $\{a\}$.
	\[ \forall a \; \exists ! S \; \forall x \; x \in S \leftrightarrow x = a
		\]
\end{proposition}

\begin{proof}
	Ponendo $b = a$ nella proposizione precedente, si ha che:
	\[ \forall a \; \exists ! S \; \forall x \; x \in S \leftrightarrow (x = a \lor x= a)
		\]
	ora $x = a \lor x = a$ equivale a $x = a$\footnote{Stiamo dicendo che in generale $\{a,a\} = \{a\}$ poiché $a \lor a = a$ (in base alle regole dei connettivi logici).}.
\end{proof}

\begin{notation}[Paio (o coppia) e singoletto]
	Possiamo ora introdurre delle abbreviazioni per il paio (o coppia) ed i singoletti:
	\[ P = \{a,b\} \Mydef \forall x \, x \in P \leftrightarrow (x = a \lor x = b)
		\]\[ S = \{a\} \Mydef \forall x \, x \in S \leftrightarrow x = a
			\]
\end{notation}

\begin{remark}
	Osserviamo che $\{a,b\} = \{b,a\}$.
\end{remark}

\begin{proof}
	Segue dal fatto che $\lor$ è commutativo:
	\[ x \in \{a,b\} \leftrightarrow (x = a \lor x = b) \leftrightarrow (x = b \lor x = a) \leftrightarrow x \in \{b,a\}
		\]
	quindi per \hyperref[ax2]{estensionalità} $\{a,b\} = \{b,a\}$.
\end{proof}

Il paio $\{a,b\}$ è, quindi, una coppia non ordinata. È possibile codificare le coppie ordinate con il seguente trucco.

\begin{definition}
	[Coppia di \href{https://it.wikipedia.org/wiki/Kazimierz_Kuratowski}{\textcolor{purple}{Kuratowski}}]
	Definiamo la \vocab{coppia di Kuratowski}:
	\[(a,b) \Mydef \{a,\{a,b\}\}
		\]
\end{definition}

\begin{proposition}[Proprietà di coppia ordinata]
	La coppia di Kuratowski $(a,b)$ rappresenta la coppia ordinata di $a$ e $b$, ossia vale che:
	\[ (a,b) = (a^{\prime},b^{\prime}) \leftrightarrow (a = a^{\prime} \land b = b^{\prime})
		\]
\end{proposition}

\begin{proof}
	Detto $c = (a,b)$, vogliamo determinare univocamente $a$ e $b$. Osserviamo che $a$ è determinata da:
	\[ x = a \leftrightarrow \forall y \in c (x \in y) \, \footnote{Sostanzialmente stiamo dicendo che preso un elemento $x$, $x = a$ se e solo se, preso un elemento di $(a,b) = \{\{a\},\{a,b\}\}$, $x$ appartiene sempre a tale elemento (dovendo appartenere sia ad $\{a\}$ che ad $\{a,b\}$ sarà per forza $a$).}
		\]
	la freccia $\rightarrow$ segue da come è definita la coppia $(a,b)$, mentre $\leftarrow$ segue dal fatto che, sempre per definizione di coppia di Kuratowski, $\{a\} \in c = (a,b)$, per cui:
	\[ \forall y \in c (x \in y) \overset{\text{ipotesi}}{\implies} x \in \{a\} \overset{\text{singoletto}}{\implies} x = a
		\]
	Determiniamo ora $b$, studiamo prima il caso in cui $\exists ! x (x \in c)$\footnote{Cioè sto dicendo la coppia è in realtà un insieme fatto da un solo insieme.}:
	\[ \begin{split}
		\exists ! x (x \in c) &\iff \{a\} = \{a,b\} \\
							&\iff b = a
	\end{split}
		\]
	ovvero se e solo se i due insiemi che formano $c = (a,b)$ sono il singoletto $\{a\}$ (per \hyperref[ax2]{estensionalità}). In questo caso $b$ è determinato, se non fosse così allora $\{a,b\}$ (che corrisponde a $b$ nella coppia ordinata) sarebbe univocamente determinato da:
	\[ x = \{a,b\} \leftrightarrow (x \in c \land x \ne \{a\})
		\]
	in tal modo abbiamo che:
	\[ x = b \leftrightarrow (x \in \{a,b\} \land x \ne a)
		\]
	Possiamo quindi ricavare la tesi come segue:
	\[ \begin{split}
		(a = a' \land b = b') & \leftrightarrow (\forall y \in c (a' \in y)) \land (b' \in \{a,b\} \land b' \ne a) \\
							  & \leftrightarrow \{a\} = \{a'\} \land \{a,b\} = \{a,b'\} \\
							  & \leftrightarrow (a,b) = (a',b')
	\end{split}
		\]
	(dove nel secondo passaggio abbiamo usato \hyperref[ax2]{estensionalità} per giustificare le uguaglianze).
\end{proof}

\begin{definition}[$n$-upla ordinata]
	Possiamo estendere la definizione di coppia ordinata con il seguente trucco:
	\[ \begin{split}
	   (a,b,c) &\Mydef ((a,b),c) \\
	   (a,b,c,d) &\Mydef (((a,b),c),d) \\
	   (a_1,a_2,\ldots,a_n) &\Mydef ((a_1,a_2,\ldots,a_{n-1}),a_n)
	\end{split}
	\]
\end{definition}

\begin{note}
	Quest'ultima definizione è, in realtà, uno schema di definizioni: una per ogni $n$. Per ora, \textcolor{red}{NON} siamo in grado di scrivere, per esempio,
	una formula insiemistica che dica ``Esiste un $n$ ed una $n$-upla $(a_1,\ldots,a_n)$ tale che…''. Però, per ogni $n$ dato, chessò 92, possiamo scrivere esplicitamente una formula che dice $x = (a_1,a_2,a_3,\ldots,a_{92})$.
\end{note}

\begin{proposition}[Proprietà di $n$-upla ordinata]
	Si ha che:
	\[ (a,b,c) = (a',b',c') \leftrightarrow a = a' \land b = b' \land c = c'
		\]\[ (a_1,\ldots,a_n) = (a_1',\ldots,a_n') \leftrightarrow a_1 = a_1' \land \ldots \land a_n = a_n'
			\]
\end{proposition}

\begin{exercise}
	Dimostra la prima e convinciti che, dato un qualunque $n$ esplicito, potresti dimostrare la seconda.
\end{exercise}

\subsection{Assioma dell'unione e operazioni booleane}

\begin{axiom}[Assioma dell'unione]
	\label{ax5}
	Dato un insieme $A$ esiste un insieme $B$ i cui elementi sono gli elementi degli elementi di $A$. Ovvero, dato un insieme $A$ esiste l'unione degli elementi di $A$.
	\[ \forall A \; \exists B \; \forall x \; x \in B \leftrightarrow \exists y \in A \; x \in y\footnote{Cioè $x$ è un elemento di $B$ se e solo se è un elemento di un elemento di $A$.}
		\]
\end{axiom}

\begin{proposition}
	[Unicità dell'unione]
	Vale l'unicità dell'unione:
	\[ \forall A \; \exists \textcolor{red}{!} B \; \forall x \; x \in B \leftrightarrow \exists y \in A \; x \in y
		\]
\end{proposition}

\begin{proof}
	Supponiamo di avere $B_1$ e $B_2$ tali che:
	\[ \forall x \, x \in B_1 \leftrightarrow \exists y \in A \, x \in y
		\]\[ \forall x \, x \in B_2 \leftrightarrow \exists y \in A \, x \in y
			\]
	quindi $\forall x (x \in B_1 \leftrightarrow x \in B_2)$, e per \hyperref[ax2]{estensionalità} $B_1 = B_2$.
\end{proof}

\begin{notation}[Unione di un insieme]
	Possiamo introdurre l'abbreviazione:
	\[ B = \bigcup A\footnote{\,``Unione di $A$''.} \Mydef \forall x \,( x \in B \leftrightarrow \exists y (x \in y))
		\]
\end{notation}

\begin{exercise}
	Dimostra che l'assioma dell'unione segue che:
	\[ \forall A \; \exists B \; (\forall y \in A \; \forall x \in y \; x \in B)\footnote{Cioè per ogni insieme esiste l'insieme di tutti gli elementi degli elementi di $A$.}
		\]
\end{exercise}

Combinando l'assioma dell'unione e del paio possiamo definire $a \cup b$.

\begin{definition}[Unione di insiemi]
	Poniamo:
	\[ a \cup b \Mydef \bigcup\{a,b\}
		\]
\end{definition}

\begin{proposition}[Caratterizzazione unione di insiemi]
	Dati $a,b$ e $a \cup b$ vale che:
	\[ x \in a \cup b \leftrightarrow (x \in a \lor x \in b)
		\]
\end{proposition}

\begin{proof}
	Dire che $x$ è un elemento di $a \cup b$ equivale a dire che $x$ è un elemento di un elemento di $\{a,b\}$, ossia
	che $x$ è un elemento di uno tra $a$ e $b$ ($x \in a \lor x \in b$).
\end{proof}

Ora definiamo le intersezioni: \emph{riesci a vedere perché, a differenza delle unioni, non servirà un nuovo assioma?}

\begin{definition}[Intersezione di un insieme]
	Sia $C$ una \textcolor{red}{classe}\footnote{Quindi, in particolare, $C$ può essere un insieme (in questo caso la definizione è comunque lecita in generale con le classi, i cui elementi sono  appunto insiemi).} non vuota.
	L'\textcolor{red}{insieme} $B$ è l'\vocab{intersezione} di $C$ se:
	\[ B = \bigcap C \Mydef \forall x (x \in B \leftrightarrow \forall y \in C (x \in y))
		\]
	cioè $x$ sta in $b$ se è elemento di ogni elemento di $C$.
\end{definition}

\begin{proposition}[Esistenza e unicità dell'intersezione]
	Data una classe non vuota $C$, l'intersezione $\bigcap C$ esiste ed è unica. In particolare, nel caso dell'intersezione di un insieme vale:
	\[ \forall A (A \ne \emptyset \rightarrow \exists ! B \; \forall x(x \in B \leftrightarrow \forall y \in A (x \in y)))
		\]
\end{proposition}

\begin{note}
	L'ipotesi $C \ne \emptyset$ è necessaria perché altrimenti si avrebbe che $\bigcap \emptyset$ è la classe universale $V$ ($x \in \bigcap \emptyset \leftrightarrow \forall y \in \emptyset(x \in y)$ (dove il RHS è sempre falso per costruzione, quindi gli $x$ che soddisfano l'enunciato sono tutti)), che non è un insieme.
\end{note}

\begin{proof}
	L'unicità segue per \hyperref[ax2]{estensionalità} al solito modo. Veniamo all'esistenza. Dal momento che $C$ non è vuota [per ipotesi], possiamo prendere $z \in C$. 
	Ora consideriamo (un sottoinsieme di $B$ ottenuto per \hyperref[ax3]{separazione} nel modo seguente):
	\[ B = \{x \in z | \forall y \in C (x \in y)\}
		\]
	ovvero il sottoinsieme di $z$ di tutti gli elementi che appartengono a tutti gli elementi di $C$.
	Chiaramente (per definizione) $x \in B \rightarrow \forall y \in C (x \in y)$, d'altro canto, $\forall y \in C (x \in y)$ implica, in particolare (un tale $x$ appartiene a tutti gli elementi della classe e quindi anche a $z$), $x \in z$, quindi in automatico $x \in B$.\\
	Abbiamo così verificato che $x \in B \leftrightarrow \forall y \in C (x \in y)$, ossia $B = \bigcap C$ (moralmente abbiamo costruito l'intersezione di un insieme per separazione su un elemento della classe $C$ (o insieme se lo è), come il sottoinsieme di tutti gli elementi che stanno in tutti gli elementi della classe). L'ultimo ragionamento può essere pensato anche nel seguente modo:
	\[ \begin{split}
		\forall x \, x \in B & \leftrightarrow (x \in z \land (\forall y \in C(x \in C)))\\
							 & \overset{\text{def.}}{\leftrightarrow} (x \in z) \land x \in \bigcap C \\
							 & \leftrightarrow x \in \bigcap C
	\end{split}
		\]
	dove l'ultima equivalenza è giustificata dal fatto che se $x$ sta in tutti gli elementi degli elementi di $C$ allora $x$ sta in particolare anche in $z$ e quindi il primo termine dell'$\land$ può essere rimosso.
\end{proof}

\begin{notation}[Intersezione e differenza di insiemi]
	Poniamo:
	\[ a \cap b \Mydef \bigcap\{a,b\} \qquad \text e \qquad a\setminus b \Mydef \{x \in a | x \not\in b\}
		\]
\end{notation}

\begin{proposition}[Caratterizzazione intersezione e differenza di insiemi]
	Vale che:
	\[ x \in a \cap b \leftrightarrow (x \in a \land x \in b)
		\]\[ x \in a \setminus b \leftrightarrow (x \in a \land x \not\in b)
			\]
\end{proposition}

\begin{exercise}
	Dimostrare la proposizione precedente (la seconda è semplicemente la definizione).
\end{exercise}

\begin{proposition}[Proprietà di unione, intersezione e differenza di insiemi]
	Alcune proprietà delle operazioni $\cup$, $\cap$, $\setminus$:
	\[ \begin{split}
		\text{\textcolor{red}{commutatività:}} \qquad & a \cup b = b \cup a \qquad \text e \qquad a \cap b = b \cap a \\
		\text{\textcolor{red}{associatività:}} \qquad & a \cup (b \cup c) = (a \cup b) \cup c \Mydef a \cup b \cup c \\
	                                        	      & a \cap (b \cap c) = (a \cap b) \cap c \Mydef a \cap b \cap c \\
		\text{\textcolor{red}{distributività:}} \qquad & a \cup (b \cap c) = (a \cup b) \cap (a \cup c) \\
													   & a \cap (b \cup c) = (a \cap b) \cup (a \cap c) \\
	    \text{\textcolor{red}{leggi di \href{https://it.wikipedia.org/wiki/Augustus_De_Morgan}{\textcolor{red}{De Morgan}}:}} \qquad & a \setminus (b \cup c) = (a \setminus b) \cap (a \setminus c) \\
																													& a \setminus (b \cap c) = (a \setminus b) \cup (a \setminus c)
	   \end{split}
	\]
\end{proposition}

\begin{proof}
	Tutte queste proprietà su deducono immediatamente dalle corrispondenti proprietà dei connettivi logici, le quali, a loro volta, si vedono con le tabelle di verità. Per esempio, dimostriamo 
	la prima delle leggi di De Morgan (facendo uso della corrispondente legge per i connettivi logici):
	\[ \begin{split}
		x \in a \setminus (b \cup c) \iff & x \in a \land x \not\in (b \cup c)\\
		\iff & x \in a \land \neg(x \in b \lor x \in c)\\
		\overset{\text{De Morgan}}{\iff} & x \in a \land x \not\in b \land x \not\in c\\
		\iff & x \in a \land x \not\in b \land \underbrace{x \in a}_{\text {non cambia nulla}} \land x \not\in c\\
		\iff & x \in (a \setminus b) \land x \in (a \setminus c)\\
		\iff & x \in (a \setminus b) \cap (a \setminus c)
	\end{split}
		\]
\end{proof}

Ora possiamo costruire insiemi finiti elencandone gli elementi, come si fa di solito, con la notazione $\{\ldots\}$\footnote{Paradossalmente prima di aggiungere l'assioma dell'unione
alla teoria potevamo costruire $n$-uple ordinate di lunghezza arbitraria, ma non un insieme con più di due elementi.}.

\begin{notation}[Insiemi di $n$ elementi]
	Possiamo ora introdurre un'abbreviazione per indicare insiemi con più di due elementi (costruiti usando l'\hyperref[ax5]{assioma dell'unione}):
	\[ \begin{split}
	   \{a,b,c\} &\Mydef \{a\} \cup \{b\} \cup \{c\} \\
	   \{a,b,c,d\} &\Mydef \{a\} \cup \{b\} \cup \{c\} \cup \{d\} \\
	   \{a_1,\ldots,a_n\} &\Mydef \{a_1\} \cup \ldots \cup \{a_n\}
	\end{split}
	\]
\end{notation}

\begin{proposition}[Caratterizzazione di insieme con $n$ elementi]
	Vale che:
	\[  \begin{split}
		x \in \{a,b,c\} & \leftrightarrow (x = a \lor x = b \lor x = c) \\
	    x \in \{a_1,\ldots,a_n\} & \leftrightarrow (x = a_1 \lor \ldots \lor x = a_n) 
		\end{split}
			\]
\end{proposition}

\begin{exercise}
	Dimostrare la proposizione precedente.
\end{exercise}

\subsection{Assioma delle parti e prodotto cartesiano}
Abbiamo definito le coppie $(x,y)$, però, per esempio, ancora nulla ci assicura che dati $A$ e $B$ esista:
\[ A \times B = \{(x,y) | x \in A \land y \in B\}
	\]
Le funzioni $A \rightarrow B$ saranno poi sottoinsiemi di $A \times B$, e vorremo parlare dell'insieme ${}^{A}B$
delle funzioni $A \rightarrow B$. Per tutto questo ci manca un solo ingrediente: l'insieme delle parti.

\begin{axiom}
	[Assioma delle parti]
	\label{ax6}
	Dato un insieme $A$ esiste l'insieme $\ps(A)$ i cui elementi sono i sottoinsiemi di $A$.
	\[ \forall A \; \exists B \; \forall x \; x \in B \leftrightarrow x \subseteq A 
		\]
\end{axiom}

\begin{proposition}
	[Unicità delle parti]
	Vale che:
	\[\forall A \; \exists ! B \; \forall x \; x \in B \leftrightarrow x \subseteq A 
		\]
\end{proposition}

\begin{proof}
	Segue come sempre per \hyperref[ax2]{estensionalità}, in quanto, se avessimo $B_1$, $B_2$, allora:
	\[ \forall x (x \in B_1 \leftrightarrow x \subseteq A) \qquad \text e \qquad \forall x(x \in B_2 \leftrightarrow x \subseteq A)
		\]
	quindi $\forall x((x \in B_1) \leftrightarrow (x \subseteq A) \leftrightarrow (x \in B_2)) \leftrightarrow \forall x (x \in B_1 \leftrightarrow x \in B_2) \leftrightarrow B_1 = B_2$.
\end{proof}

\begin{notation}[Insieme delle parti (o insieme potenza)]
	Data l'unicità possiamo porre:
	\[ B = \ps(A) \Mydef \forall x \; x \in B \leftrightarrow x \subseteq A
		\]
\end{notation}

\begin{proposition}[Esistenza ed unicità del prodotto cartesiano]
	Dati $A$ e $B$  esiste un unico insieme $A \times B$ tale che:
	\[ \forall z (z \in A \times B) \leftrightarrow \exists x \in A \, \exists y \in B \, z = (x,y)\footnote{Ossia, informalmente, $ z \in A \times B$ se e solo se si può scrivere come coppia ordinata di un elemento di $A$ ed uno di $B$.}
		\]
\end{proposition}

\begin{proof}
	L'unicità è conseguenza immediata della definizione e dell'\hyperref[ax2]{assioma di estensionalità} (stessa dimostrazione di sempre). Per l'esistenza, definiamo per \hyperref[ax3]{separazione}:
	\[ A \times B \Mydef \{z \in \ps(\ps(A \cup B)) | \exists x \in A \ \exists y \in B \ z = (x,y)\}
		\]
	così come scritto, siamo sicuri che è un insieme che contiene coppie ordinate di elementi di $A$ e $B$, tuttavia dobbiamo dimostrare anche che ogni coppia $(x,y)$ con $x \in A$ e $y \in B$ appartiene a questo insieme. Per fare ciò bisogna dimostrare che tutte queste coppie
	appartengono a $\ps(\ps(A \cup B))$:\footnote{Poniamo $a,b,\ldots \in z \Mydef a \in z \land b \in z \land \ldots$ e $a,b,\ldots \subseteq z \Mydef a \subseteq z \land b \subseteq z \land \ldots$}\,\footnote{Tutte le implicazioni si basano sul fatto che se un oggetto è sottoinsieme di un qualche insieme allora è un elemento del corrispondente insieme delle parti per definizione.}
	\[\begin{split}
		a \in A \land b \in B &\implies \{a\},\{a,b\} \subseteq A \cup B \\
		& \implies \{a\},\{a,b\} \in \ps(A \cup B) \\
		& \overset{\text{\hyperref[ax4]{paio}}}{\implies} (a,b) = \{\{a\},\{a,b\}\} \subseteq \ps(A \cup B)\\
		& \implies (a,b) \in \ps(\ps(A \cup B))
	\end{split}
		\]
	pertanto tutte le coppie ordinate di elementi di $A$ e $B$ appartengono a $\ps(\ps(A \cup B))$ e per separazione possiamo costruire il prodotto cartesiano $A \times B$ come l'insieme di \underline{tutte} le coppie ordinate.
\end{proof}

\begin{note}
	Avremmo potuto costruire $A \times B$ usando, anziché l'assioma delle parti, l'assioma del rimpiazzamento, che vedremo più avanti.
\end{note}

\subsection{Relazioni di equivalenza e di ordine, funzioni}
Ora rivedremo alcuni concetti ben noti dai primi corsi del primo anno (\emph{o dalla scuola superiore?}). Lo facciamo molto rapidamente, essenzialmente per completezza, e per fissare le notazioni.

\begin{definition}[Relazione binaria]
	Si dice \vocab{relazione binaria} fra $A$ e $B$ un sottoinsieme di $A \times B$.
\end{definition}

\begin{notation}[Relazione binaria]
	Data una relazione $\rel \subseteq A \times B$, definiamo l'abbreviazione:
	\[ a \rel b \Mydef (a,b) \in \rel
		\]
\end{notation}

\begin{example}
	Per esempio scriviamo $a < b$ per indicare che $(a,b) \in\, <$.
\end{example}

Considerando il caso di $A \times A$ possiamo definire le seguenti relazioni.

\begin{definition}
	Una relazione $\sim \,\subseteq A \times A$ è una \vocab{relazione di equivalenza} se è:
	\begin{enumerate}[(i)]
		\item \textbf{\underline{riflessiva}}: $\forall x \in A \; x \sim x$.
		\item \textbf{\underline{simmetrica}}: $\forall x,y \in A\footnote{$\forall x_1,\ldots,x_n \Mydef \forall x_1 \ldots \forall x_n$, e lo stesso con $\exists$ e con i quantificatori limitati.} x \sim y \leftrightarrow y \sim x$.
		\item \textbf{\underline{transitiva}}: $\forall x,y,z \in A \; (x \sim y \land y \sim z) \rightarrow x \sim z$.
	\end{enumerate}
\end{definition}

\begin{definition}
	$\leq \, \in A \times A$ è una \vocab{relazione di ordine (largo)} se è:
	\begin{enumerate}[(i)]
		\item \textbf{\underline{riflessiva}}: $\forall x \in A \; x \leq x$.
		\item \textbf{\underline{antisimmetrica}}: $\forall x,y \in A \; (x \leq y \land y \leq x) \rightarrow x = y$.
		\item \textbf{\underline{transitiva}}: $\forall x,y,z \in A \; (x \leq y \land y \leq z) \rightarrow x \leq z$.
	\end{enumerate}
\end{definition}

\begin{definition}
	$< \, \in A \times A$ è una \vocab{relazione di ordine stretto} se è:
	\begin{enumerate}[(i)]
		\item \textbf{\underline{irriflessiva}}: $\forall x \in A \; \neg(x < x)$.
		\item \textbf{\underline{transitiva}}: $\forall x,y,z \in A \; (x < y \land y < z) \rightarrow x < z$.
	\end{enumerate}
\end{definition}

\begin{exercise}
	Dimostra che una relazione di ordine stretto $<$ su $A$ è automaticamente asimmetrica:
	\[ \forall x,y \in A \; x < y \rightarrow \neg (y < x)
		\]
\end{exercise}

\begin{soln}
Se valesse che $\forall x,y \in A \; x < y \rightarrow y < x$, allora sarebbero contemporaneamente vere $x < y$ e $y < x$, da cui, per transitività si avrebbe $x < x$ che è falso.
\end{soln}

\begin{proposition}[Corrispondenza tra ordini stretti e larghi]
	Data una relazione di ordine stretto $<$ su $A$, la relazione:
	\[ \leq\, = \{(x,y) \in A \times A | x < y \lor x = y\}\footnote{Formalmente: $\{z \in A \times A | \exists x,y \in A \; z = (x,y) \land \ldots\}$.}
		\]
	è una relazione di ordine largo. Viceversa, se $\leq$ è una relazione di ordine largo, la seguente relazione è dei ordine stretto:
	\[ <\, = \{(x,y) \in A \times A | x \leq y \land x \ne y\}\footnote{Come la nota sopra.}
		\]
	Inoltre, in questo modo, le relazioni di ordine stretto e di ordine largo sono poste in corrispondenza una - a - uno.
\end{proposition}

\begin{proof}
	Definiamo la \vocab{diagonale di una relazione} di $A \times A$ come:
	\[ \Delta_A \Mydef \{(x,y) \in A \times A | x = y\}
		\]
	Allora è facile verificare che, se $<$ è una relazione di ordine stretto, allora $< \cap \,\Delta_A = \emptyset$ e $< \cup \,\Delta_A$ è una relazione di ordine largo corrispondente.
	Viceversa, se $\leq$ è una relazione di ordine largo, allora $\Delta_A \subseteq \, \leq$ e $\leq \setminus \Delta_A$ è la relazione di ordine stretto corrispondente.
\end{proof}

\begin{notation}[Relazioni d'ordine strette e larghe]
	Fissata una relazione di ordine largo $\textcolor{red}{\leq}$ su $A$, ci sentiremo liberi di usare la corrispondente relazione di ordine stretto $\textcolor{red}{<}$ fintanto che la scelta del simbolo sia indizio sufficiente dell'operazione.
	Inoltre scriveremo $x > y$ per $y < x$ e $x \geq y$ per $y \leq x$.
\end{notation}

\begin{definition}[Relazione di ordine totale]
	Una \vocab{relazione di ordine totale} su $A$ è una relazione di ordine $\leq$ tale che:
	\[ \forall x,y \in A \, (x \leq y) \lor (x = y) \lor (y \leq x)
		\]
\end{definition}

\begin{exercise}
	Formula la definizione precedente per ordini stretti.
\end{exercise}

\begin{soln}
Diciamo che $<$ è un ordinamento totale (stretto) su $A$ se:
\[ \forall x \in A \, \forall y \in A (x \ne y \land ((x < y) \lor (x > y))) \lor (x = y)
	\]
o anche semplicemente:
\[ \forall x \in A \, \forall y \in A (x = y) \lor (x < y) \lor (x > y)
	\]
E per quanto detto possiamo anche pensare che:
\[ \text{$\leq$ ordine totale} \iff \text{$< \cup \Delta_A$ ordine totale}
	\]
(infatti nella prima definizione non è strettamente necessario che compaia l'uguaglianza, la si può ottenere quanto entrambe le disuguaglianze sono vere per antisimmetria, mentre per ordini stretti è necessario aggiungere la diagonale nella definizione di totalità).
\end{soln}

\begin{definition}[Restrizione di una relazione]
	Data una relazione $\rel \subseteq A \times B$, e dati $A' \subseteq A$, $B' \subseteq B$, possiamo definire la \vocab{restrizione} di $\rel$ a $A'\times B'$:
	\[ \rel_{|A' \times B'} \Mydef \rel \cap (A' \times B')
		\]
	``restrizione di $\mathcal R$ a $A' \times B'$\,''.
\end{definition}

\begin{exercise}
	Data $\rel$ relazione di equivalenza/ordine su $A$ e $A' \subseteq A$, dimostra che $\rel_{|A' \times A'}$ è una relazione di equivalenza/ordine su $A'$.
\end{exercise}

\begin{soln}
Vediamolo per le relazioni di equivalenza. È facile osservare che $\forall a' \in A'$, vale che $(a',a') \in \mathcal{R}_{|A' \times A'}$ (sta in $A' \times A'$ per definizione di prodotto cartesiano e sta in $\mathcal{R}$ essendo una relazione di equivalenza per ipotesi (vale il per ogni)),
analogamente valgono simmetria e riflessività. 
\end{soln}

\begin{definition}[Dominio e immagine di una relazione]
	Data una relazione $\rel \subseteq A \times B$, definiamo:
	\[  \begin{split}
		\Dom(\rel) &\Mydef \{x \in A | \exists y \in B \; x\rel y\} \qquad \text{\vocab{dominio} di $\rel$} \\
	    \Imm(\rel) &\Mydef \{y \in B | \exists x \in A \; x \rel y \} \qquad \text{\vocab{immagine} di $\rel$}
	\end{split}
			\]
	(notare che $\Dom(\rel)$ e $\Imm(\rel)$ non coincidono necessariamente con $A$ e $B$).
\end{definition}

\begin{definition}[Funzione]
	Chiamiamo \vocab{funzione} $f:A \rightarrow B$ una relazione $f \subseteq A \times B$ tale che:
	\[ \forall x \in A \; \exists ! \, y \in B \; (x,y) \in f
		\]
		(Intuitivamente $f$ è l'insieme delle coppie $(x,f(x))$ per $x \in A$).
\end{definition}

\begin{notation}[Immagine e immagine di un sottoinsieme]
	Data una funzione $f$ possiamo indicare la coppia $(x,y) \in f$ con la seguente abbreviazione:
	\[ y = f(x) \Mydef (x,y) \in f
		\]
	Dato $S \subseteq \Dom(f)$, indichiamo l'immagine di un sottoinsieme (ovvero l'insieme delle immagini del sottoinsieme) come:
	\[ f[S] \Mydef \{y \in \Imm(f)| \exists x \in S \; \underbrace{y = f(x)}_{= (x,y) \in f}\} = \underbrace{\{f(x) | x \in S\}}_{\text{informalmente}}
		\]
\end{notation}

\begin{definition}[Iniettività, suriettività e bigettività]
	Una funzione $f: A \rightarrow B$ è:
	\[ \begin{split}
		\text{\vocab{iniettiva} se:}\; & \forall y \in \Imm(f)\; \exists! \, x \in \Dom(f) \; f(x) = y\\
		\text{\vocab{suriettiva} se:}\; & B = \Imm(f)\; \text{ossia $\forall y \in B \; \exists x \in A \; f(x) = y$.}\\
		\text{\vocab{bigettiva} se:}\; &\text{è sia iniettiva sia surgettiva.}
	\end{split}
		\]
\end{definition}

\begin{definition}[Funzione inversa]
	Data $f$ iniettiva:
	\[ f^{-1} \Mydef \{(y,x) \in B \times A | f(x) = y\} \subseteq B \times A
		\]
\end{definition}

\begin{remark}
	Se $f$ iniettiva, $f^{-1} : \Imm(f) \rightarrow \Dom(f)$ è una funzione\footnote{Altrimenti è la semplice controimmagine di un sottoinsieme dell'immagine (che non è una funzione).} a
	sua volta iniettiva (basta pensare alla definizione di $f^{-1}$ iniettiva e usare che per l'iniettività di $f$ c'è un'unica $x \in \Dom(f)$ tale che $y = f(x)$).
	In particolare se $f : A \rightarrow B$ è bigettiva, allora $f^{-1}$ è bigettiva.
\end{remark}

\begin{definition}[Restrizione di una funzione]
	Data $f: A \rightarrow B$ e $A' \subseteq A$ definiamo:
	\[ f_{|A'} \Mydef \{(x,y) \in A' \times B | f(x) = y\}
		\]
	 ``$f$ \vocab{ristretta} ad $A'$\,'' è una funzione: $A' \rightarrow B$.
\end{definition}

\begin{definition}[Composizione di funzioni]
	Date $g : A \rightarrow B$ e $f : B \rightarrow C$:
	\[ f \circ g \Mydef \{(x,z) \in A \times C | z = f(g(x))\}\footnote{O più formalmente $\exists y(y = g(x) \land z = f(y))$.}
		\]
	``$f$ \vocab{composta} con $g$'' è una funzione: $A \rightarrow C$.
\end{definition}

\begin{notation}[Funzione identità]
	Indichiamo con $\id_A$ la \vocab{funzione identità} su $A$:
	\[ \id_A \Mydef \{(x,y) \in A \times A | x = y\} = \Delta_A
		\]
\end{notation}

\begin{remark}[Caratterizzazione funzione inversa]
	Data $f : A \rightarrow B$ bigettiva e $g : B \rightarrow A$ è equivalente scrivere:
	\[ g = f^{-1} \qquad g \circ f = \id_A \qquad f \circ g = \id_B
		\]
\end{remark}

\begin{exercise}[Composizione di funzioni iniettive/surgettive/bigettive]
	Data $f : A \rightarrow B$ e $g: B \rightarrow C$, sotto quali condizioni $g \circ f$ è iniettiva, suriettiva, bigettiva?
\end{exercise}

\begin{soln}
	Indaghiamo il problema partendo prima dalle singole funzioni con delle proprietà e componendole. Se $f$ e $g$ sono iniettive, allora $g \circ f$ è iniettiva, infatti:
	\[ g(f(x)) = g(f(y)) \overset{\text{$g$ iniett.}}{\iff} f(x) = f(y) \overset{\text{$f$ iniett.}}{\iff} x = y \qquad \forall x,y \in A
		\]
		che è equivalente alla definizione di $g \circ f : A \rightarrow C$ iniettiva. Se $f$ e $g$ sono surgettive, allora $g \circ f$ è surgettiva:
	\begin{align*}
		\text{$g$ surgettiva} &\iff \forall z \in C \; \exists y \in B \; g(y) = z \\
		\text{$f$ surgettiva} &\iff \forall y \in B \; \exists x \in A \; f(x) = y
	\end{align*}
	che messe assieme ci danno che $g(f(x)) = z$, cioè per ogni $z \in C$ esiste $x \in A$ tale che $(g \circ f)(x) = z$, che è equivalente alla definizione di $g \circ f$ surgettiva.
	Naturalmente, mettendo assieme i risultati precedenti, otteniamo che $f$ e $g$ bigettive implica $g \circ f$ bigettiva. Viceversa, osserviamo che se $g \circ f$ è iniettiva, allora $f$ è iniettiva,
	infatti, se per assurdo $f(x) = f(y)$, con $x \ne y$, allora, applicando $g$, si ha $g(f(x)) = g(f(y))$ (perché immagini di cose uguali), ma per iniettività di $g \circ f$, ciò equivale a $x = y$,
	che è assurdo, pertanto $x = y$\footnote{Abbiamo dimostrato per assurdo che $f(x) = f(y) \implies x = y$ (sotto l'ipotesi che $g \circ f$ iniettiva), il viceversa è banale e con questo si ha l'equivalenza con la definizione di $f$ iniettiva}.
	Se $g \circ f$ è surgettiva, allora $g$ è surgettiva, infatti, per ipotesi, $\forall z \in C \; \exists x \in A \; g(f(x)) = z$, e, dato che $f(x) \in B$, abbiamo trovato che per ogni $z \in C$ esiste $y = f(x) \in B$ tale che $g(y) = z$, ovvero $g$ surgettiva.\\
	Infine, verrebbe da chiedersi, se date $f$ iniettiva e $g$ surgettiva, $g \circ f$ sia necessariamente bigettiva (così da avere magari un'equivalenza tra la bigettività della composizione e le proprietà delle funzioni in partenza), sfortunatamente ciò è falso: presa
	$f : \{0,1\} \hookrightarrow \{0,1,2,3\}$ e $g : \{0,1,2,3\} \twoheadrightarrow \{0,1,2\}$, con:
	\begin{align*}
		& g(0) = 0 \qquad f(0) = 0 \\
		& g(1) = 0 \qquad f(1) = 1 \\
		& g(2) = 2 \\
		& g(3) = 3
	\end{align*}
	abbiamo $f$ iniettiva, $g$ surgettiva, ma $g \circ f$ non è né iniettiva ($g(f(0)) = g(f(1))$) né surgettiva ($\Imm(g \circ f) = \{0\}$).
\end{soln}

\begin{exercise}[Insieme quoziente e proiezione]
	\label{3.73}
	Data una relazione di equivalenza $\sim$ su $A$, dimostra che esiste un insieme $\faktor{A}{\sim}$ ed una funzione surgettiva $i_\sim$ da $A$ a $\faktor{A}{\sim}$
	tale che:
	\[ \forall x,y \in A \; x \sim y \leftrightarrow i_\sim(x) = i_\sim(y)
		\]
\end{exercise}

\begin{soln}
	Possiamo definire l'insieme $\faktor A \sim$ per separazione nelle parti di $A$ come segue:
	\[ \faktor{A}{\sim} \Mydef \{B \in \ps(A) | \forall x,y \in B \; x \sim y\}
		\]
	Osserviamo che per ogni $B, C \in \faktor A \sim$, vale che $B \cap C \ne \emptyset \iff B = C$, infatti, se esiste $x \in B \cap C$, allora $x \sim y$, $\forall y \in B$, e $x \sim z$, $\forall z \in C$.
	Da cui $w \in B \iff w \sim x \iff w \in C$ e quindi per l'arbitrarietà di $x$, vale $B = C$.\footnote{Essendo che ogni elemento, per quanto detto è in una classe di equivalenza di $\faktor{A}{\sim}$, si ha anche che $\bigcup \faktor{A}{\sim} = A$, dunque le classi di equivalenza sono disgiunte
	e la loro unione dà proprio l'insieme, pertanto si dirà che formano una \vocab{partizione} dell'insieme $A$.}\\
	Da quanto appena osservato segue quindi che ogni $x \in A$ appartiene ad una e una sola \vocab{classe di equivalenza} (gli elementi di $\faktor A\sim$), in quanto è sempre almeno in relazione con se stesso per riflessività, possiamo quindi definire $i_\sim$ come 
	la funzione da $A$ a $\faktor A \sim$ che manda $x$ nella sua classe di equivalenza. Naturalmente $i_\sim(x) = i_\sim(y)$ equivale al dire che le due classi di equivalenza sono la stessa, dunque per definizione
	si ottiene proprio che $x \sim y$. Inoltre $i_\sim$ è surgettiva in quanto in ogni classe di equivalenza di $\faktor A \sim$ c'è almeno un elemento (per la riflessività delle relazioni di equivalenza), la cui immagine via $i_\sim$ dà appunto la classe.
\end{soln}

\begin{exercise}[Primo teorema di ``omomorfismo'', per insiemi]
	Data una relazione di equivalenza $\sim$ su $A$ e $f : A \rightarrow B$, affinché esista la funzione $\widetilde{f}: \faktor{A}{\sim} \rightarrow B$ tale che $f = \widetilde{f} \circ i_\sim$,
	è necessario e sufficiente che $\forall x,y \in A \; x \sim y \rightarrow f(x) = f(y)$.
\end{exercise}

\begin{soln}
	Osserviamo che\footnote{Per essere formalissimi, staremmo usando che $f = \widetilde{f} \circ i_\sim \iff f(x) = (\widetilde{f} \circ i_\sim)(x)$, $\forall x \in A$, ovvero
	l'estensionalità per funzioni vista in un'osservazione precedente.} $f(x) = (\widetilde{f} \circ i_\sim)(x)$, $\forall x \in A$ se e solo se $f(x) = \widetilde{f}(i_\sim(x))$, ora ciò equivale al fatto che 
	l'immagine dell'elemento $x \in A$ al LHS è uguale a quella della classe di equivalenza (che è un sottoinsieme di $A$) $i_\sim(x)$ tramite $\widetilde{f}$ al RHS. Per rispettare la relazione richiesta (che sarebbe poi la commutatività di un diagramma)
	possiamo definire $\widetilde{f}(C)$, $C \in \faktor{A}{\sim}$, come $f(z)$ per un qualunque $z \in C$.\\ Ora ci basta osservare che questa è una buona definizione, e lo è in quanto tutti gli elementi in $C$ sono in relazione $\sim$ tra loro e per ipotesi tale relazione 
	è che la loro immagine via $f$ sia la stessa, pertanto $f(x) = f(y)$, $\forall x,y \in C$. Infine, poiché $\forall x \in A \; x \in i_\sim(x)$, si ha proprio che $\widetilde{f}(i_\sim(x)) = f(x)$. Abbiamo quindi dimostrato che l'uguaglianza iniziale è vera 
	se $\sim$ è definita come nelle ipotesi, osserviamo che se tale uguaglianza funziona, allora due elementi sono in relazione via $\sim$ se e solo se hanno la stessa immagine. Infatti, si avrebbe che:
	\[ \begin{split}
		f(x) = f(y) &\iff \widetilde{f}(i_\sim(x)) = \widetilde{f}(i_\sim(y)) \\
					&\iff i_\sim(x) = i_\sim(y) \\
					&\iff x \sim y
	\end{split}
		\]
	dove la prima equivalenza è l'assunto, la seconda è la definizione di $\widetilde{f}$ (che è una bigezione tra $\faktor A\sim$ e $\Imm(f)$, per questo abbiamo usato l'iniettività), mentre l'ultima equivalenza è la definizione di classi di equivalenza.
\end{soln}

\newpage
\section{Assioma dell'infinito e numeri naturali}
Il nostro prossimo obiettivo è definire i numeri naturali. I soli oggetti della teoria degli insiemi sono gl insiemi, per cui va da sé che 
i numeri saranno determinati insiemi. Il nostro scopo non è quindi tanto definire, quanto codificare i numeri naturali per mezzo di insiemi opportuni.
La scelta della codifica non è obbligata: per esempio potremmo decidere che:
\[ \text{``codifica buffa di $n$''} = \underbrace{\{\{\{\ldots \emptyset\ldots\}\}\}}_{\text{$n$ parentesi}}
	\]
Sceglieremo, invece, quest'altra codifica:
\[ n = \{0,1,\ldots,n-1\} = \{x \in \NN | x < n\}
	\]\[ 0 = \emptyset \qquad 1 = \{0\} \qquad 2 = \{0,1\} \qquad 3 = \{0,1,2\} \qquad \text{etc.}
		\]
che presenta alcuni vantaggi: per esempio $n$ è rappresentato da un insieme di $n$ elementi, e dire $m < n$ equivale semplicemente a dire $m \in n$.\\
L'ostacolo è ora parlare di questi oggetti in maniera precisa nel linguaggio della teoria degli insiemi. A dire il vero, potremmo già scrivere una formula $\Phi(n)$ che dice 
``$n$ è un numero naturale'' si tratta di un \textcolor{red}{esercizio} difficile, che sarà reso più facile da idee che vedremo più avanti. Noi non scriviamo questa formula, ma, anche a farlo,
non potremmo comunque dimostrare che esiste un insieme i cui elementi sono i numeri naturali, questo perché gli assiomi visti finora non permettono di uscire dalla classe degli insiemi finiti (degli insiemi ``ereditariamente finiti'',
ad essere precisi: definiremo questi concetto a tempo debito).\\
Servirà un nuovo assioma. E l'idea da sfruttare è che, siccome $n = \{0,\ldots,n-1\}$, per ottenere il successore di $n$, ossia $n+1 = \{0,\ldots,n-1,n\}$ dobbiamo aggiungere a $n$ l'elemento $n$ stesso: $n+1 = n \cup \{n\}$.
Avendo una formula per denotare il successore, possiamo postulare l'esistenza di un insieme chiuso per successori, e questo ci darà $\NN$.

\begin{definition}
	[Successore]
	Definiamo il \vocab{successore} di $x$:
	\[ s(x) \Mydef x \cup \{x\}
		\]
\end{definition}

\begin{definition}
	[Insiemi induttivi]
	Diciamo che $A$ è un \vocab{insieme induttivo} se contiene $\emptyset$ ed è chiuso per successori \footnote{Ciò non esclude che ci possano essere altri elementi oltre a $\emptyset$ che non siano successori (questa cosa è sempre falsa in $\omega$).}, ossia:
	\[ \text{$A$ è induttivo} \iff \emptyset \in A \land \forall x \in A \; s(x) \in A
		\]
\end{definition}

\begin{axiom}
	[Assioma dell'infinito]
	\label{ax7}
	Esiste un insieme induttivo.
	\[ \exists A (\emptyset \in A \land (\forall x \in A \; s(x) \in A))
		\]
\end{axiom}

Finalmente definiamo l'insieme dei numeri naturali - che, per qualche buffa ragione, chiamiamo $\omega$ - come l'intersezione della classe, non vuota per l'assioma dell'infinito,
di tutti gli insiemi induttivi.\footnote{Aver introdotto l'assioma dell'infinito
ci assicura che tale intersezione è non vuota, e ciò basta affinché $\omega$ sia un insieme (in caso contrario avremmo avuto l'intersezione del vuoto, che, come visto, non è un insieme).}

\begin{definition}[Numeri naturali]
	L'insieme $\omega$ è l'intersezione di tutti gli insiemi induttivi, ossia $\omega$ è l'unico insieme tale che:
	\[ \forall x (x \in \omega \leftrightarrow(\forall A \; \text{``$A$ è induttivo''}\rightarrow x \in A)) \footnote{Cioè $x$ è in $\omega$ se e solo se è elemento
	di qualsiasi insieme induttivo (nella \underline{classe} degli insiemi induttivi), e, inoltre, essendo l'intersezione di una classe, è in particolare un insieme (perché per definizione
	stiamo intersecando gli elementi di una classe, che sono insiemi).}
		\]
\end{definition}

Adesso che abbiamo $\omega$, possiamo facilmente dimostrare che ogni dato numero naturale vi appartiene.

\begin{definition}[Codifica dei numeri naturali]
	Definiamo:
	\[ 0 \Mydef \emptyset \qquad 1 \Mydef s(0) \qquad 2 \Mydef s(1) \qquad 3 \Mydef s(2) \qquad \text{etc.}
		\]
\end{definition}

\begin{exercise}
	Dimostra che $0,1,2,3 \in \omega$.
\end{exercise}

\begin{soln}
	Avendo definito $\omega$ come:
	\[ \omega = \bigcap_{\text{$A$ induttivo}} A
		\]
	sappiamo che $\emptyset \in A$, per ogni insieme induttivo (per definizione), dunque $0 \in \omega$. Inoltre vale che l'intersezione di insiemi induttivi è chiusa per successore (e quindi per quanto appena mostrato è a sua volta un insieme induttivo), infatti:
	\[ \forall x \in \bigcap_{\text{$A$ induttivo}} A \leftrightarrow \forall A \, \text{$A$ induttivo}\, (x \in A)
		\]
	ed essendo tutti gli $A$ chiusi per successore (in quanto induttivi) segue che:
	\[ s(x) \in \bigcap_{\text{$A$ induttivo}} A \implies s(x) \in \omega
		\]
	Pertanto, avendo osservato che $0 \in \omega$, si avrà anche che $1 = s(0) \in \omega$, $2 = s(1) \in \omega$, $3 = s(2) \in \omega$ e così via.
\end{soln}

Un esercizio un po' più difficile è esibire insiemi che non appartengono a $\omega$.

\begin{exercise}
	Dimostra che $\{\{\emptyset\}\} \not \in \omega$.\footnote{\textbf{\underline{Idea}}: Esibisci un insieme induttivo che non contiene $\{\{\emptyset\}\}$.}
\end{exercise}

\begin{soln}
	Osserviamo che $\{\{\emptyset\}\}$ non è un successore, se fosse che $s(x) = x \cup \{x\} = \{\{\emptyset\}\}$, dato che $x$ è elemento di $s(x)$ e che $\{\{\emptyset\}\}$ ha un solo elemento, per \hyperref[ax2]{estensionalità} deve essere che che $x = \{x\} = \{\emptyset\}$ (ossia tutti gli elementi di $s(x)$ devono essere 
	uguali all'unico elemento di $\{\{\emptyset\}\}$). Pertanto avremmo che $x = \{\emptyset\}$, ma $s(x) = s(\{\emptyset\}) = \{\emptyset\} \cup \{\{\emptyset\}\} =\footnote{Volendo essere pignoli possiamo usare la definizione dell'unione come il prendere gli elementi degli elementi: $\{\emptyset\} \cup \{\{\emptyset\}\} = \bigcup \{\{\emptyset\},\{\{\emptyset\}\}\}$,
	e l'unione di tale insieme è formata appunto da tutti gli elementi degli elementi (quindi naturalmente il vuoto $\emptyset$ e anche $\{\emptyset\}$).} \{\emptyset,\{\emptyset\}\}$, ma $\{\emptyset\} \ne \emptyset$, perché $\{\emptyset\}$ è non vuoto e $\emptyset$ è proprio il vuoto.\\
	Avendo dimostrato che $\{\{\emptyset\}\}$ non è né un successore né (ovviamente) il vuoto, ci basta mostrare mostrare che non appartiene ad un insieme induttivo $A$ che non ha altri elementi (oltre a $\emptyset$) che non sono successori. Dando per buono che $\omega$ non contenga elementi che non sono successori, si ottiene che
	$\{\{\emptyset\}\} \not \in \omega$.\footnote{Non abbiamo usato l'hint di Mamino e abbiamo usato un fatto non dimostrato.}
\end{soln}

\subsection{Gli assiomi di Peano}
Per convincerci, però, che $\omega$ è, a buon diritto, l'insieme dei numeri naturali, serve qualcosa di più. Classicamente, i numeri naturali si definiscono per mezzo degli
\vocab{assiomi di \href{https://it.wikipedia.org/wiki/Giuseppe_Peano}{\textcolor{purple}{Peano}}}. Questi assiomi, che caratterizzano a meno di isomorfismi l'insieme $\NN$ dotato della funzione di successore, \textbf{per noi diventano dei teoremi} che
dimostreremo a proposito dell'insieme $\omega$\footnote{Cioè gli assiomi di Peano diventano enunciati dimostrabili all'interno della ZFC.}. In questo senso\footnote{Classicamente gli assiomi definivano $\NN$ a meno di isomorfismo, mostrando che $\omega$ li soddisfa siamo sicuri di avere l'oggetto (insieme) $\NN$ definito da tali assiomi nella ZFC, e tale oggetto è appunto $\omega$.}, quindi, $\omega$ codifica legittimamente i numeri naturali.

\begin{definition}
	[Assiomi di Peano al secondo ordine\protect\footnote{qualunque cosa questo significhi...}]
	Dato un insieme $\NN$, un elemento $0 \in \NN$, e una funzione:
	\[ \text{succ} : \NN \longrightarrow \NN
		\]
	diciamo che $(\NN,0,\text{succ})$\footnote{La 3-upla ordinata formata dai tre insiemi $\NN$,$0$,succ: (($\NN$,0),succ)$= \{(\NN,0),\{(\NN,0),\text{succ}\}\} = \{\{\NN,\{\NN,0\}\},\{\{\NN,\{\NN,0\}\},\text{succ}\}\}$.} soddisfa gli assiomi di Peano se:
	\begin{enumerate}[(a)]
		\item \label{a}Il successore è iniettivo:
		\[ \forall n,m \in \NN \; \text{succ}(m) = \text{succ}(n) \rightarrow m = n \, \footnote{L'altra freccia è banale e sarà data sempre per scontata.}
			\]
		\item \label{b}Lo zero non è un successore:
		\[ \not\exists n \in \NN \; \text{succ}(n) = 0
			\]
		\item \vocab{Principio di induzione}: data una qualunque formula insiemistica (proprietà) $\Phi(n)$ vale:
		\[ (\Phi(0) \land \forall n \in \NN \; \Phi(n) \rightarrow \Phi(\text{succ}(n))) \rightarrow \forall n \in \NN \; \Phi(n)
			\]
	\end{enumerate}
\end{definition}


\begin{figure*}[h]
		\centering
		\includegraphics[width=10.5cm]{immagini/peano.png}
		\captionsetup{labelformat=empty}
		\caption{Apparivano così in ``\emph{Arithmetices principia}'', nel 1889, gli assiomi di Peano.}
\end{figure*}


\begin{theorem}[$\omega$ soddisfa gli assiomi di Peano]
	La funzione $\text{succ}: \omega \rightarrow \omega : n \mapsto s(n)$, è ben definita e $(\omega,\emptyset,\text{succ})$
	soddisfa gli assiomi di Peano.
\end{theorem}

\begin{proof}
	Per controllare che $\text{succ}$ sia ben definita, occorre assicurarsi che se $n \in \omega$, allora $\text{succ}(n) = s(n) \in \omega$. Fissiamo $n \in \omega$
	e consideriamo un qualunque insieme induttivo $A$. Siccome $A$ è induttivo $\omega \subseteq A$, quindi $n \in A$, e, di conseguenza $s(n) \in A$. Per 
	l'arbitrarietà di $A$, allora, $s(n)$ appartiene a ogni insieme induttivo (quindi all'intersezione, ovvero $\omega$).\\
	Dimostriamo ora che $\omega$ rispetta gli assiomi di Peano. Iniziamo con dimostrare (b) e (c), poi passeremo ad (a):
	\begin{enumerate}
		\item[(b)] Supponiamo, per assurdo, $s(n) = \emptyset$. Abbiamo allora che:
		\[ n \in s(n) =  n \cup \{n\} = s(n) = \emptyset
			\]
		contro la definizione di $\emptyset$.
		\item[(c)] Dimostriamo che l'insieme $A = \{n \in \omega | \Phi(n)\} \subseteq \omega$ è induttivo, da cui $\omega = A$\footnote{Stiamo costruendo $A$ come sottoinsieme di $\omega$ (che
		a sua volta sarà contenuto in $A$, non appena avremo dimostrato che quest'ultimo è induttivo, per definizione).}, quindi varrà che $\forall n \in \omega \; \Phi(n)$.
		\begin{enumerate}[(1)]
			\item Per ipotesi abbiamo che $\Phi(\emptyset)$, quindi $\emptyset \in A$.
			\item $n \in A \overset{\text{def. $A$}}{\implies} \Phi(n) \overset{\text{ipotesi}}{\implies} \Phi(\text{succ}(n)) = \Phi(s(n)) \overset{\text{$n \in \omega \rightarrow s(n) \in \omega$}}{\implies} s(n) \in A$
		\end{enumerate}
		\item[(a)] La dimostrazione passa attraverso due lemmi.
					\begin{lemma}
					[Lemma 1]
					L'unione di un elemento di $\omega$ è contenuta nell'elemento: $\forall n \in \omega \; \bigcup n \subseteq n$.
					\end{lemma}
					\begin{proof}
						Avendo dimostrato in (c) che in $\omega$ vale l'induzione possiamo usarla con $\Phi(n) \Mydef \bigcup n \subseteq n$. 
						\[
							\begin{split}
							\boxed{\Phi(\emptyset)} & \qquad \bigcup \emptyset = \emptyset \subseteq \emptyset \\
							\boxed{\Phi(n) \rightarrow \Phi(s(n))} & \qquad \bigcup s(n) = \bigcup(n \cup \{n\}) \overset{\star}{=} \underbrace{\left(\bigcup n\right)}_{\subseteq n} \cup n \overset{\text{Hp. indutt.}}{\subseteq} n \cup n = n \subseteq s(n)
							\end{split}
						\]
						(si noti che il passo base è coerente con le definizioni delle abbreviazioni date), e $\star$ vale in quanto:
						\[ \begin{split}
							x \in \bigcup(n \cup \{n\}) \overset{\text{def.}}{\iff} &\quad \exists y ( x \in y) \land (y \in (n \cup\{n\})) \\
														\overset{\text{caratt. $\cup$}}{\iff} &\quad \exists y( x \in y) \land (y \in n \lor y = n)\\
														\overset{\text{distrib. $\land$}}{\iff} &\quad \exists y \; ( x \in y \land y \in n) \lor (x \in y \land y = n)\\
														\iff &\quad \exists y ( x \in y \land y \in n) \lor \exists y(x \in y \land y = n)\\
														\overset{\text{def.}}{\iff} &\quad x \in \bigcup n \lor x \in n\\
														\overset{\text{caratt. $\cup$}}{\iff} &\quad x \in \left(\bigcup n\right) \cup n
						\end{split}
							\]
						(dove alla secondo membro della seconda equivalenza abbiamo che $y \in \{n\}$ e per \hyperref[ax2]{estensionalità} equivale a $y = n$.).
					\end{proof}
					\begin{lemma}
						[Lemma 2]
						L'unione dei successori di un elemento in $\omega$ è proprio l'elemento: $\forall n \in \omega \; \bigcup s(n) = n$.
					\end{lemma}
					\begin{proof}
						Ricopiando quanto fatto nel passo induttivo della dimostrazione precedente abbiamo:
						\[ \bigcup s(n) = \bigcup (n \cup \{n\}) = \left(\bigcup n\right) \cup n \overset{\star}{\subseteq} n
							\]
						dove in $\star$ abbiamo usato che $\bigcup n \subseteq n$, non per ipotesi induttiva (visto che non stiamo facendo alcuna induzione), ma stiamo usando direttamente il risultato del Lemma 1.
						Naturalmente vale anche che $n \subseteq \bigcup s(n)$ (ogni elemento di $n$ è elemento dell'elemento $n$ in $s(n)$), dunque vale la tesi.
					\end{proof}
					Finalmente abbiamo che, per il Lemma 2:
					\[ s(m) = s(n) \implies \bigcup s(m) = \bigcup s(n) \overset{\text{Lemma 2}}{\iff} m = n
						\]
					dove la prima freccia è data dal fatto che stiamo considerando l'unione di insiemi uguali, dunque succ$: \omega \rightarrow \omega$ è iniettiva.
	\end{enumerate}
\end{proof}

\subsection{L'ordine di omega}
Conviene, adesso, sviluppare un po' di tecnologia per manipolare i numeri interi. Dopo, dimostreremo altresì che gli assiomi di Peano hanno un unico modello $(\NN, 0, \text{succ})$
a meno di isomorfismi.

\begin{notation}[Relazione di ordine su $\omega$]
	Dati $m,n \in \omega$, scriviamo:
	\[ m < n \Mydef m \in n \footnote{Per essere precisi non stiamo usando $\in$ come una relazione (visto che abbiamo assunto all'inizio che fosse un simbolo del linguaggio della teoria degli insiemi),
	ma stiamo definendo $< \Mydef \{(m,n) \in \omega \times \omega | m \in n\}$. Inoltre se aggiungiamo la diagonale $\Delta_\omega$ a $<$, otteniamo $\leq$ (cioè $m \leq n \Mydef (m \in n) \lor (m = n)$), che, come visto, è legata alla corrispondente relazione d'ordine 
	stretto, e godrà di tutte le stesse proprietà (come vedremo man mano).}
		\]
\end{notation}

\begin{proposition}[Ordinamento totale di $\omega$]
	La relazione $<$ è un ordine totale su $\omega$. 
\end{proposition}

Per dimostrare questa proposizione, sono comodi alcuni lemmi.

\pagebreak

\begin{remark}[Successore del secondo termine in un'appartenenza]
	\label{succ2}
	Si osserva che valgono le seguenti cose:
	\begin{enumerate}[(1)]
		\item $m \in n \rightarrow m \in s(n)$, infatti $n \subseteq n \cup \{n\} = s(n)$ (banalmente se $m$ è contenuto in $n$ allora è contenuto anche nel suo successore).
		\item $m \in s(n) \rightarrow (m \in n \lor m = n)$, ciò se $m$ è nel successore di $n$, allora è $n$ stesso o un suo elemento, infatti:
		 \[ \begin{split}
			m \in s(n) = n \cup \{n\} & \iff m \in (n \cup \{n\}) \\
									& \iff (m \in n) \lor (m \in \{n\}) \\
									&\iff (m \in n) \lor (m = n)
		 \end{split}
			\]
		(nella seconda equivalenza si è usata la caratterizzazione data dell'appartenenza ad un unione di insiemi, e nella terza il fatto che se $m$ appartiene ad un singoletto, allora per \hyperref[ax2]{estensionalità} è proprio l'unico elemento del singoletto).
	\end{enumerate}
\end{remark}

\begin{lemma}[Successore del primo termine in un'appartenenza]
	$\forall a,b \in \omega \; a \in b \rightarrow (s(a) \in b \lor s(a) = b)$.\footnote{Moralmente: se un numero è strettamente più piccolo di un altro, o il suo successore è a sua volta più piccolo del secondo numero, o coincide con quest'ultimo.}
\end{lemma}

\begin{proof}
	Procediamo per induzione su $b$.
	\begin{itemize}
		\item[$\boxed{\text{caso $b = 0$}}$] $a \in \emptyset \rightarrow \ldots$ vera a vuoto, perché $a \in \emptyset$ è falsa (dunque l'implicazione è sempre vera, indipendentemente dal valore di verità dell'antecedente).
		\item[$\boxed{\text{caso $b = s(n)$}}$] L'ipotesi induttiva è $a \in n \rightarrow (s(a) \in n \lor s(a) = n)$. Dobbiamo dimostrare:
		\[ a \in s(n) \rightarrow (s(a) \in s(n) \lor s(a) = s(n))
			\]
		abbiamo che $a \in s(n) \iff a \in n \cup \{n\} \iff a \in n \lor a = n$. Quindi abbiamo due casi:
		\[ \begin{split}
			& a \in n \overset{\text{Hp. indutt.}}{\implies} (s(a) \in n) \lor (s(a) = n) \overset{\text{def. $s(n)$}}{\iff} s(a) \in s(n) \\
			& a = n \iff s(a) = s(n)
		\end{split}
			\]
		(la seconda equivalenza è giustificata dal fatto che abbiamo dimostrato che la funzione successore in $\omega$ è iniettiva).
	\end{itemize}
\end{proof}

Possiamo ora dimostrare la proposizione iniziale.

\begin{proof}
	Per verificare che $<$ è una relazione di ordine stretto totale, dobbiamo verificare che è irriflessiva, transitiva e totale (cioè presi qualsiasi due elementi di $\omega$ la loro coppia ordinata appartiene a $<$).
	\begin{itemize}
		\item[$\boxed{\text{transitività}}$] Vogliamo verificare che $(a \in b \land b \in c) \rightarrow a \in c$. Procediamo per induzione su $c$:
		\begin{itemize}
			\item[$\boxed{\text{caso $c = 0$}}$] la premessa $b \in c$ è falsa, quindi l'implicazione è vera a vuoto (l'antecedente è sempre falso, quindi l'implicazione sempre vera).
			\item[$\boxed{\text{caso $c = s(n)$}}$] assumiamo per ipotesi induttiva $(a \in b \land b \in n) \rightarrow a \in n$, e vogliamo dimostrare:
				\[ (a \in b \land b \in s(n)) \rightarrow a \in s(n)
					\]
				Osserviamo che $a \in b \implies a \in s(b)$, e che $b \in s(n) \overset{\text{Lemma}}{\implies} s(b) \in s(n) \lor s(b) = s(n)$, abbiamo quindi due casi in base a $s(b)$:
				\[ \begin{split}
					& s(b) = s(n) \implies a \in s(b) = s(n) \implies a \in s(n)\\
					& s(b) \in s(n) \implies a \in s(b) \in s(n) \implies a \in s(n)
				\end{split}
					\]
				questo usando il lemma precedente, potevamo anche scegliere di usare l'osservazione per dire che $b \in s(n) \implies b = n \lor b \in n$ e ottenere ancora i casi:
				\[  \begin{split}
					& b = n \implies a \in b = n \overset{\text{Oss.}}{\implies} a \in s(n) \\
					& b \in n \implies a \in b \in n \implies a \in n \overset{\text{Oss.}}{\implies} a \in s(n)
					\end{split}
					\]
		\end{itemize}
		\item[$\boxed{\text{irriflessività}}$] Vogliamo verificare $\neg \,a \in a$, e lo facciamo per induzione su $a$:
		\begin{itemize}
			\item[$\boxed{\text{caso $a = 0$}}$] $\neg \, \emptyset \in \emptyset$, vero per definizione di $\emptyset$.
			\item[$\boxed{\text{caso $a = s(n)$}}$] L'ipotesi induttiva è $\neg \, n \in n$, e vogliamo verificare che $\neg s(n) \in s(n)$. Procediamo per assurdo, supponiamo che $s(n) \in s(n)$, e per l'osservazione abbiamo due casi:
			\[ \begin{split}
				& s(n) = n \implies n \in n \; \lightning\\
				& s(n) \in n \implies n \in s(n) \in n \implies n \in n \; \lightning
			\end{split}
				\]
			($n \in n$ è falso perché per ipotesi induttiva $\neg(n \in n)$ è vero).
		\end{itemize}
		\item[$\boxed{\text{totalità}}$] Vogliamo dimostrare che $\forall a,b \in \omega (a \in b) \lor (a = b) \lor (b \in a)$. Iniziamo per induzione su $a$:
		\begin{itemize}
			\item[$\boxed{\text{caso $a = 0$}}$] La tesi diventa $\forall b \in \omega (\emptyset \in b) \lor (\emptyset = b) \lor (b \in \emptyset) \footnote{Ovviamente quest'ultimo caso è sempre faslo e quindi può essere escluso.}$. Procediamo quindi per induzione su $b$:
			\begin{itemize}
				\item \textbf{\underline{caso $b = 0$}} La tesi diventa $(\emptyset \in \emptyset) \lor (\emptyset = \emptyset)$, dove naturalmente la prima affermazione è sempre falsa, mentre la seconda è sempre vera, dunque la tesi è vera.
				\item \textbf{\underline{caso $b = s(m)$}} La tesi è $(\emptyset \in s(m)) \lor (\emptyset = s(m))$, con ipotesi induttiva $(\emptyset \in m) \lor (\emptyset = m)$. Abbiamo quindi due casi in base all'ipotesi induttiva:
				\[ \begin{split}
					& \emptyset \in m \implies \emptyset \in s(m) \\
					& \emptyset = m \implies \emptyset \in \{\emptyset\} = s(m)
				\end{split}
					\]
				in entrambi casi è vera la tesi perché è sempre vero il primo termine.
			\end{itemize}
			\item[$\boxed{\text{caso $a = s(n)$}}$] La tesi è $\forall b \in \omega (s(n) \in b) \lor (s(n) = b) \lor (b \in s(n))$, mentre l'ipotesi induttiva è $(n \in b) \lor (n = b) \lor (b \in n)$. Dall'ipotesi induttiva abbiamo quindi tre casi:
				\[ \begin{split}
					& n \in b \overset{\text{Lemma}}{\implies} s(n) \in b \lor s(n) = b \\
					& n = b \overset{\text{Iniett. del succ.}}{\implies} s(n) = s(b) \implies b \in s(b) = s(n) \implies b \in s(n)\\
					& b \in n \overset{\text{Oss.}}{\implies} b \in s(n) \implies b \in a 
				\end{split}
					\]
				in tutti e tre i casi almeno una delle tre proposizioni della tesi è vera, dunque la tesi è sempre vera.
		\end{itemize}
	\end{itemize}
\end{proof}

\begin{remark}[$\leq$ ordina totalmente $\omega$]
	Avendo dimostrato che $<$ è un ordine totale su $\omega$, abbiamo dimostrato in automatico che anche $\leq = < \cup \Delta$ lo è, infatti, per la corrispondenza tra i due (come si è visto precedentemente in una proposizione), anche le definizioni di ordine totale sono corrispondenti (in particolare per 
	$\leq$ ci basta che valga una tra $\leq$ e $\geq$, se valgono entrambe c'è l'=, mentre per $<$ chiedevamo nella definizione che valesse $<$, $>$ o $=$, quindi se nella dimostrazione precedente avessimo usato $\leq$ al posto di $<$ avremmo ottenuto lo stesso risultato perché le richieste nella definizione di ordine totale sono le stesse).
\end{remark}

\begin{corollary}[Rappresentazione dei numeri naturali]
	Un numero naturale è l'insieme dei numeri naturali minori di lui.
	\[ \forall m \in \omega \; m = \{n \in \omega | n < m\}
		\]
\end{corollary}

\begin{proof}
	Vogliamo dire che $m = \{ n \in \omega | n \in m\}$, ossia per definizione di sottoinsieme che $m \subseteq \omega$. Per induzione: $\emptyset \subseteq \omega$ è vera (perché $\omega$ è induttivo).
	Assumiamo che $m \subseteq \omega$, allora $s(m) = \underbrace{m}_{\subseteq \omega} \cup \{m\}$ e $\{m\} \subseteq \omega$ perché $m \in \omega$ per ipotesi iniziale, quindi si conclude che $s(m) \subseteq \omega$.
\end{proof}

\begin{corollary}[Più piccolo = contenuto]
	$\forall m,n \in \omega (m \leq n \leftrightarrow m \subseteq n$).\footnote{Naturalmente il lemma vale anche con $<$ e $\subsetneq$.}
\end{corollary}

\begin{proof}
	Siccome $\omega$ è totalmente ordinato, si danno due casi (nel primo dimostro $\rightarrow$, nel secondo dimostro che la negazione della premessa implica la negazione della conseguenza, che è equivalente [via contronominale] a $\leftarrow$):
	\begin{align*}
		& m \leq n \implies \forall x \in \omega (x < m \rightarrow x < n) \overset{\text{def. $<$}}{\implies} \forall x \in \omega (x \in m \rightarrow x \in n) \overset{\text{def. $\subseteq$}}{\implies} m \subseteq n \\
	    & n < m \implies n \in m \; \text{tuttavia} \; n \not\in n \; \text{quindi non può essere che $m \subseteq n$ ovvero} \; m \not\subseteq n
	\end{align*}
	($n \not \in n$ perché abbiamo dimostrato che $<$ è di ordine stretto su $\omega$, quindi irriflessiva, inoltre, nella dimostrazione del primo caso, si osserva che nel secondo passaggio è
	indifferente usare $<$ o $\leq$ nell'enunciato e dimostrazione del corollario\footnote{Mamino li mischia, ma valgono entrambi gli enunciati e le dimostrazioni.}.
\end{proof}
\pagebreak
\subsection{Induzione forte e principio del minimo}
\begin{theorem}
	[Principio di induzione - forma forte]
	Data una formula insiemistica $\Phi(x)$, vale:
	\[ (\forall n \in \omega (\forall x < n \; \Phi (x)) \rightarrow \Phi(n)) \rightarrow \forall n \in \omega \; \Phi(n)
		\]
	Ovvero, se assumendo $\Phi(x)$ per tutti gli $x < n$, abbiamo $\Phi(n)$, allora $\Phi(n)$ è vera per tutti i numeri $n$.
\end{theorem}

\begin{remark}
	Chiaramente questa forma è ``forte'' perché permette di assumere un'ipotesi induttiva più forte dell'induzione di Peano. In quella, infatti, si deve dedurre $\Phi(n)$ a 
	partire da $\Phi$ del numero precedente. Qui, invece, possiamo far conto di sapere $\Phi$, non solo per il precedente, ma per tutti i numeri minori di $n$.
\end{remark}

\begin{proof}
	Assumiamo vero l'antecedente per ipotesi ovvero assumiamo vera l'implicazione:
	\[ \forall n \in \omega \, (\forall x < n \; \Phi(x)) \rightarrow \Phi(n)
		\]
	Dalle tavole di verità quest'espressione può essere vera sia se antecedente e conseguente sono veri sia se l'antecedente è falso. Mostriamo di essere nel primo caso,
	ovvero dimostriamo per induzione (debole) che [la premessa è vera], ovvero $\forall m \in \omega \; \psi(m)$ dove:
	\[ \psi(m) \Mydef \forall x < m \; \Phi(x)
		\]
	\begin{itemize}
		\item[$\boxed{\text{caso $m = 0$}}$] $\forall x < \emptyset \; \Phi(x)$ è vera a vuoto.
		\item[$\boxed{\text{caso $m = s(n)$}}$] Per ipotesi induttiva abbiamo $\forall x < n \; \Phi(x).$ Vogliamo che $x < s(n) \;\Phi(x)$, dall'osservazione sappiamo che ciò equivale a $x < n \lor x = n$.
		Si danno quindi due casi:
		\begin{itemize}
			\item Nel caso $x < n$ abbiamo $\Phi(x)$ per ipotesi induttiva.
			\item Nel caso $x = n$, l'ipotesi induttiva, combinata con l'antecedente ci dà $\Phi(n)$, ossia $\Phi(x)$ (perché abbiamo che $\forall x < n \;\Phi(x) \rightarrow \Phi(n)$, ma tutta l'espressione è vera per ipotesi e per ipotesi induttiva l'antecedente è vero, quindi anche $\Phi(n)$ lo è).
			(Per l'arbitrarietà di $x<m$ abbiamo dimostrato $\forall x < m \; \Phi(x)$)\footnote{Stiamo solo giustificando formalmente il per ogni.}.
		\end{itemize}
	\end{itemize}
	Ora abbiamo dimostrato che $\forall m \in \omega \; \forall x < m \; \Phi(x)$, quindi siamo nel secondo caso, e otteniamo che nella premessa $\Phi(n)$ è vera. Ora dato un $n \in \omega$ qualunque, ci basta prendere nell'antecedente $m = n + 1$ e $x = n$ e otteniamo in automatico $\Phi(n)$ (e siamo sicuri sia vera 
	visto che abbiamo per ipotesi un'implicazione con antecedente vero).
\end{proof}

\begin{theorem}
	[Principio del minimo]
	Sia $A \subseteq \omega$. Se $A \ne \emptyset$ allora esiste $n \in A$ tale che $\forall x \in A \; n \leq x$. Ovvero, ogni sottoinsieme non vuoto di $\omega$ ha un minimo elemento.
\end{theorem}

\begin{remark}
	[Idea $\lbrack$e parte$\rbrack$ della dimostrazione] Si dimostra per induzione forte che, se $n \in A$, allora $A$ ha un minimo. Poi, siccome $A$ non è vuoto, deve esserci qualche $n \in A$, quindi $A$ ha minimo. L'induzione 
	funziona così. Se $n \in A$, si danno due casi. O esiste $x < n$ con $x \in A$, e allora $A$ ha minimo per ipotesi induttiva (che è quello che stiamo per dimostrare), oppure $\forall x < n \; x \not\in A$, ma allora $n$ è il minimo di $A$ (e abbiamo concluso).
\end{remark}

\begin{proof}
	Dimostriamo la contronominale della tesi (nel caso in cui $x \in A$), ovvero dobbiamo dimostrare che se $A$ non ha un minimo elemento, allora $A$ è vuoto. \\
	Assumiamo quindi per ipotesi induttiva che esista un elemento strettamente più piccolo di tutti gli altri, ovvero $\forall n \in A \; \exists x \in A \; x < n$ (stiamo usando il $<$ perché il caso in cui $x = n$ è già contemplato nell'osservazione dicendo che che $x \not \in A$).
	Osserviamo che la contronominale della nostra tesi\footnote{Cioè di questo caso della dimostrazione come visto nell'osservazione.} è:
	\[ (\neg \exists x < n \; x \in A) \rightarrow n \not \in A
		\]
	ed equivale a:
	\[	\begin{split}
		& (\neg \exists x (x < n) \land (x \in A)) \rightarrow n \not \in A \\
		\overset{\text{$\land$ commut.}}{\iff} & (\neg \exists \; x \in A \; x < n) \rightarrow n \not \in A \\
		\overset{\text{contronom.}}{\iff} & n \in A \rightarrow (\exists x \in A \; x < n)
		\end{split}
		\]
	ma la cosa appena scritta è equivalente all'ipotesi induttiva, pertanto la contronominale della tesi è vera, e quindi anche la tesi. Abbiamo quindi dimostrato anche il secondo caso dell'induzione forte e ciò conclude la dimostrazione del principio del minimo (perché stiamo supponendo ci sia sempre un elemento, come visto nell'osservazione iniziale).
\end{proof}


\begin{remark}
	Per completare l'equivalenza tra induzione, induzione forte e principio del minimo, andrebbe dimostrato anche che principio del minimo$\implies$induzione.
\end{remark}

\begin{definition}[Insieme ben ordinato]
	Un insieme totalmente ordinato $(S, <)$ si dice \vocab{bene ordinato} se ogni sottoinsieme non vuoto ha un minimo.\footnote{Cioè se vale il principio del minimo c(come vale in $\omega$).}
	\[ \forall A \subseteq S \; A \ne \emptyset \rightarrow \exists m \in A \; \forall x \in A \; m \leq x
		\]
\end{definition}

La nozione di buon ordine è stata introdotta da Cantor agli albori della teoria degli insiemi, e giocherà un ruolo centrale in questo corso.

\begin{example}
	$(\omega, <)$ è un insieme bene ordinato\footnote{Si usa la notazione di coppia ordinata per indicare sia l'insieme sia la relazione che c'è sopra.} per quanto visto nel teorema precedente.
\end{example}

\begin{exercise}
	Dimostra che $X = s(s(s(\omega)))$ è bene ordinato dalla relazione $a < b \Mydef a \in b$.
\end{exercise}

\begin{soln}
	Dato $(\omega,<)$, basta considerare la seguente relazione:
	\[ \prec := < \cup (\omega \times \{\omega\}) \cup (s(\omega) \times \{s(\omega)\}) \cup (s(s(\omega)) \times \{s(s(\omega))\})\,\footnote{Che formalmente è un sottoinsieme di $s(s(s(\omega))) \times s(s(s(\omega)))$.}
		\]
	dove $(x,y) \in \prec \leftrightarrow x \in y$. Si vede quindi che $(s(s(s(\omega))),\prec)$ è un ordine totale (fondamentalmente perché $<$ lo è, 
	e le coppie che abbiamo aggiunto sono costruite apposta per rispettare la definizione di ordine [stretto] totale). Abbiamo costruito $\prec$ in modo che 
	$\forall n \in \omega \; n \prec \omega$, inoltre vale anche [per costruzione] che $\omega \prec s(\omega) \prec s(s(\omega))$, dunque, dato $S \subseteq s(s(s(\omega)))$,
	se $S \cap \omega \ne \emptyset$, allora il minimo esiste ed è dato da $\min_<(S \cap \omega)$. Se $S \cap \omega = \emptyset$ (ovvero se $S$ è un sottoinsieme di $\{\omega,s(\omega),s(s(\omega))\}$),
	allora per definizione di $\prec$ (come scritto sopra), per tutti i sottoinsiemi possibili abbiamo sempre un minimo [per la totalità di $\prec$]. Pertanto $\forall S \subseteq s(s(s(\omega)))$ c'è un minimo
	e quindi in $s(s(s(\omega)))$ vale il principio del minimo, cioè è ben ordinato.
\end{soln}

\subsection{Ricorsione numerabile}
La ricorsione è il procedimento per cui si costruisce una funzione $f : \omega \rightarrow \text{qualcosa}$, definendo $f(s(n))$ a partire da $f(n)$, o,
più in generale da $f(\emptyset),\ldots,f(n)$. Questo è un procedimento fondamentale: potremmo dire che è IL modo di pensare gli infidi puntini ($\ldots$). Vediamo qualche esempio.

\begin{example}
	[Operazioni aritmetiche]
	Possiamo definire somma e prodotto come:
	\[ \begin{cases}
		a + \textcolor{red}{0} = a \\
		a + \textcolor{red}{s(b)} = s(a + b)
	\end{cases}
	\qquad
	\begin{cases}
		a \cdot \textcolor{red}{0} = 0\\
		a \cdot \textcolor{red}{s(b)} = a \cdot b + a
	\end{cases}
		\]
	anziché $a + b = \underbrace{s(s(\ldots a \ldots))}_{\text{$b$ successori}}$ (abbiamo il caso base con 0, e poi si procede ricorsivamente dal caso base fino a $b$) e $a \cdot b = \underbrace{a + a + \ldots + a}_{\text{$b$ volte}}$ (ricorsivamente ad un certo 
	punto si partirà da $a$ e si inizierà a sommare).
\end{example}

\begin{example}
	[Potenza e fattoriale]
	Possiamo definire ricorsivamente potenze e fattoriali come segue:
	\[ \begin{cases}
		a^{\textcolor{red}{0}} = 1\\
		a^{\textcolor{red}{s(b)}} = a^b \cdot a
	\end{cases}
	\qquad
	\begin{cases}
		\textcolor{red}{0}! = 1\\
		\textcolor{red}{s(a)}! = a! \cdot s(a)
	\end{cases}
		\]
	anziché $a^b = \underbrace{a \cdot a \cdot \ldots \cdot a}_{\text{$b$ volte}}$ e $a! = 1 \cdot 2 \cdot \ldots \cdot (a - 1) \cdot a$.
\end{example}

\begin{example}
	[Sommatoria]
	Possiamo definire la sommatoria come:
	\[\begin{cases}
		\displaystyle
		\sum_{i = 0}^{\textcolor{red}{0}} f(i) = 0\\
		\displaystyle
		\sum_{i = 0}^{\textcolor{red}{s(a)}} f(i) = \left(\sum_{i=0}^{a} f(i) \right) + f(s(a))
	\end{cases}
		\]
	anziché $\displaystyle\sum_{i = 0}^a f(i) = f(0) + f(1) + \ldots + f(a)$ (cioè con la sommatoria definita ricorsivamente stiamo eliminando il fastidioso discorso (non formale) dei puntini \dots).
\end{example}

Altre \vocab{successioni} - \textbf{ossia funzioni con dominio $\omega$} - sono definite nella maniera più naturale proprio per ricorsione.

\begin{example}[Esempio di applicazione della ricorsione]
	In quanti modi posso coprire una sequenza di $n$ caselle $\underbrace{\square\square\square\ldots\square\square}_{n}$ con tessere di una o due caselle,
	$\square$ e $\square\square$, che non si sovrappongano e non lascino caselle scoperte?
\end{example}

\begin{soln}
	Detto $F_n$ il numero di ricoprimenti di una sequenza lunga $n$, vediamo che la tessera più a sinistra può essere $\square$ o $\square \square$. Nel primo caso, ci sono 
	$F_{n-1}$ modi di completare il ricoprimento, nel secondo caso $F_{n-2}$. Abbiamo quindi trovato una relazione ricorsiva del numero di ricoprimenti in funzione di $n$:
	\[ F_n = F_{n-1} + F_{n-2}\footnote{Cioè il numero totale di modi di ricoprire la sequenza di $n$ caselle deriva dalla somma dei due casi, che rappresentano i modi di ricoprire le altre caselle fissata quella/e iniziale/i, ciò fissati i casi base ci definisce bene (via ricorsione numerabile) una successione che conta il numero 
	di ricoprimenti in funzione di $n$.}
		\]
	La sequenza risulta completamente determinata, per ricorsione, osservando che $F_0 = F_1 = 1$: sono i \vocab{numeri di Fibonacci}.
\end{soln}

\textbf{In un certo senso, induzione e ricorsione sono due facce della stessa medaglia}: dove l'induzione dimostra $\Phi(s(n))$ assumendo di sapere 
$\Phi(n)$, la ricorsione calcola $f(s(n))$ assumendo di sapere $f(n)$. Lo stesso parallelismo, vedremo, si presenterà per l'induzione e la ricorsione transfinita.
Tornando al numerabile: come abbiamo enunciato due forme dell'induzione, enunceremo due forme della ricorsione.\\
La semplice osservazione che segue dice che due funzioni sono uguali precisamente quando assumono gli stessi valori.

\begin{remark}
	[Estensionalità per funzioni]
	Date $f,g : A \rightarrow B$, allora:
	\[ f = g \leftrightarrow \forall x \in A \; f(x) = g(x)
		\]
	(dove l'uguaglianza di funzioni non è altro che uguaglianza di sottoinsiemi in $A \times B$).
\end{remark}

\begin{proof}
	Si osserva che:
	\[ (x,y) \in f \overset{\text{def. $f$}}{\iff} y = f(x) \overset{\text{Hp.}}{\iff} y = g(x) \overset{\text{def. $g$}}{\iff} (x,y) \in g
		\]
	e si conclude per \hyperref[ax2]{estensionalità} che quanto scritto sopra equivale a dire che gli insiemi $f$ e $g$ sono uguali.
\end{proof}

\begin{notation}[Insieme delle funzioni da $A$ a $B$]
	Indichiamo con ${}^{A}B$ l'insieme delle funzioni da $A$ a $B$, che esiste per \hyperref[ax3]{separazione} in $\ps(A \times B)$.
\end{notation}

\begin{theorem}
	[Ricorsione numerabile - prima forma]
	\label{ric1}
	Dato un insieme $A$, un elemento $k \in A$\footnote{$k$ sarà il caso base della ricorsione.} e una funzione:
	\[ h : \omega \times A \longrightarrow A
		\]
	esiste un'unica funzione $f : \omega \rightarrow A$ tale che:
	\[ \forall n \in \omega \quad f(s(n)) = h(n,f(n))
		\]
\end{theorem}

\begin{example}[Potenza e fattoriale con la ricorsione numerabile]
	Per definire $a^b$ considero $k = 1$, $h(n,x) = a \cdot x$, e $h(0,x) = k = 1$. Per definire il fattoriale $k = 1$, $h(n,x) = s(n) \cdot x$ e $h(0,x) = k = 1$.
\end{example}

\begin{exercise}
	Come potrei costruire $F_n$ usando questo teorema?
\end{exercise}

\begin{proof}
	Il piano consiste nel trovare una formula $\Phi(x,y)$ che dice ``$y = f(x)$'' - questa è la vera difficoltà 
	della dimostrazione - poi semplicemente otteniamo $f$ per separazione nell'insieme $\omega \times A$ ($f$ è una funzione da $\omega$ ad $A$) usando la formula $\Phi$.\\
	Per dire ``$y = f(x)$'' diremo equivalentemente ``i primi $x$ passaggi della ricorsione, partendo da $k$, conducono a $y$''.
	Dato $x \in \omega$ diciamo che $g$ è una \vocab{$x$-approssimazione} se la vale la formula seguente:
	\[ (g \in {}^{s(x)}A ) \land (g(\emptyset) = k) \land \forall n \in x (g(s(n)) = h(n,g(n)))
		\]
	ovvero la funzione $g : \{0,\ldots,x\} \rightarrow A$ soddisfa la definizione ricorsiva di $f$, ristretta, naturalmente, al dominio $\{0,\ldots,x\}$ (cioè $s(x)$).
	Il vantaggio di tagliuzzare $f$ in $x$-approssimazioni è che così otteniamo un parametro, $x$, su cui impostare un'induzione.
	
	\begin{lemma}[Esistenza e unicità delle $x$-approssimazioni in $\omega$]
		$\forall x \in \omega \; \exists ! \, g$ ``$g$ è una $x$-approssimazione''.
	\end{lemma}

	\begin{proof}
		Induzione su $x$.
		\begin{itemize}
			\item[$\boxed{\text{caso $x = \emptyset$}}$] Basta osservare che l'unica $\emptyset$-approssimazione è la funzione $\{(\emptyset,k)\}$. Infatti il dominio
			è $\{\emptyset\}$ per definizione, e per soddisfare la definizione deve valere necessariamente $g(\emptyset) = k$, quindi l'unica $\emptyset$-approssimazione possibile è la funzione $g = \{(\emptyset,k)\}$.\\
			\item[$\boxed{\text{caso $x = s(a)$}}$] Per ipotesi induttiva abbiamo che esiste un'unica \textbf{$a$-approssimazione $g$}. Poniamo:
			\[ g' = g \cup \{(s(a), h(a,g(a)))\}
				\]
			ossia $g'(t) = g(t)$ per $t \leq a$, e $g'(s(a)) = h(a,g(a))$. È immediato verificare che $g'$ è una $s(a)$-approssimazione (l'abbiamo costruita apposta per verificare la definizione).\\
			Per verificare l'unicità, osserviamo che, date le $s(a)$-approssimazione $g'$ e $g''$, la loro restrizione a $s(a)$ è una $a$-approssimazione (per definizione), quindi, per ipotesi induttiva $g'_{|s(a)} = g = g''_{|s(a)}$.
			D'altro canto il dominio di una $s(a)$-approssimazione è $s(s(a)) = s(a) \cup \{s(a)\}$, e abbiamo detto che $g'$ e $g''$ coincidono su $s(a)$, quindi coincidono:
			\[ g'(s(a)) = h(a,g'(a)) = h(a,g''(a)) = g''(s(a))
				\]
		\end{itemize}	
	\end{proof}
	Stabilito il lemma, introdurremo la formula $\Phi$:
	\[ \Phi(x,y) \Mydef \exists g \in {}^{s(x)}A \quad \text{``$g$ è una $x$-approssimazione''} \land g(x) = y
		\]
	Per l'unicità della $x$-approssimazione $\forall x \in \omega \; \exists ! \, y \; \Phi(x,y)$, possiamo quindi definire, per ogni $x \in \omega$ e $y \in A$ la funzione via \hyperref[ax3]{separazione}:
	\[ f(x) = y \Mydef \Phi(x,y)\footnote{Formalmente $f = \{(x,y) \in \omega \times A | \Phi(x,y)\} = \{(x,y) \in \omega \times A | \exists! g \in {}^{s(x)}A \; \text{``$g$ è una $x$-approssimazione''} \land g(x) = y\}$, in altre parole,
	dato $x \in \omega$ affido alla sua (unica) $x$-approssimazione il compito di trovare un'immagine, e quindi definisco $f$ attraverso $g$ (che dipende dalla $x$ in input).}
		\]
	Occorre verificare che $f$ soddisfa le condizioni della ricorsione.
	\begin{itemize}
		\item[$\boxed{f(\emptyset) = k}$] Immediata, infatti $f(\emptyset) = g(\emptyset)$, ma abbiamo visto nel lemma che l'unica $\emptyset$-approssimazione possibile in $\omega$ è $\{(0,k)\}$ (cioè soddisfa semplicemente il caso base), quindi $f(\emptyset) = g(\emptyset) = k$.
		\item[$\boxed{f(s(n)) = h(n,f(n))}$] Per costruzione $f(s(n)) = g(s(n))$ per una (l'unica) $s(n)$-approssimazione $g$. D'altro canto $g(s(n)) = h(n,g(n))$ (per definizione di $s(n)$-approssimazione).
		Ora $g_{|s(n)}$ è una $n$-approssimazione, quindi $g_{|s(n)} (n) = g(n) \overset{\text{def.}}{=} f(n)$. Mettendo tutto insieme:
		\[ f(s(n)) \overset{\text{def.}}{=} g(s(n)) \overset{\text{def. $g$}}{=} h(n,g(n)) \overset{\text{def. + oss.}}{=} h(n, f(n))
			\]
	\end{itemize}
	Ciò dimostra che una $f$ ottenuta per separazione come abbiamo visto esiste e soddisfa la tesi del teorema di ricorsione numerabile.
	L'unicità di $f$ segue facilmente per induzione, Date $f'$ e $f''$ che soddisfano la ricorsione abbiamo:
	\[ f'(\emptyset) = k = f''(\emptyset) \qquad f'(s(n)) = h(n,f'(n)) \overset{\text{Hp. indutt.}}{=}\footnote{E usando l'estensionalità per funzioni su $h$.} h(n,f''(n)) = f''(s(n))
		\]
	e per estensionalità di funzioni si conclude che $f' = f''$.
\end{proof}

Procedendo come negli esempi all'inizio di questa sezione, il \hyperref[ric1]{teorema di ricorsione numerabile} ci consente di costruire le operazioni aritmetiche, le potenze, etc.
A titolo di esempio, vediamo nel dettaglio, il caso della somma.

\begin{example}
	[Costruzione di $+ : \omega \times \omega \rightarrow \omega$]
	Vogliamo formalizzare la definizione:
	\[ \begin{cases}
		a + \textcolor{red}{0} = 0\\
		a + \textcolor{red}{s(b)} = s(a+b)
	\end{cases}
		\]
	Per il \hyperref[ric1]{teorema di ricorsione numerabile} sappiamo che, per ogni $a \in \omega$ fissato, esiste un'unica $f : \omega \rightarrow \omega$ tale che:
	\[ f(\textcolor{red}{0}) = a \land \forall b \in \omega \; f(\textcolor{red}{s(b)}) = s(f(b))
		\]
	Scriviamo quindi:
	\[ a + x = y \Mydef \exists f \in {}^{\omega}\omega \;\; f(0) = a \; \land  \; f(x) = y \; \land \; \forall b \in \omega \; f(s(b)) = s(f(b))
		\]
\end{example}

L'applicazione che segue chiude il conto che abbiamo lasciato aperto con gli assiomi di Peano. Dimostriamo che essi identificano un'unica struttura a meno di isomorfismi, quindi $\omega$ è 
a buon diritto, l'insieme dei numeri naturali.

\begin{theorem}[Unicità dei numeri naturali]
	Supponiamo che $(\NN, 0, \text{succ})$ soddisfi gli assiomi di Peano, allora $(\NN, 0, \text{succ})$ \textbf{e} $(\omega,\emptyset,s)$ sono strutture isomorfe - \textbf{ossia, formalmente, esiste}:
	$ f : \omega \rightarrow \NN$ \textbf{bigettiva} tale che:
	\begin{enumerate}[(i)]
		\item $f(\emptyset) = 0$.
		\item $\forall n \in \omega \; f(s(n)) = \text{succ}(f(n))$.\footnote{Cioè è una bigezione tra insiemi, che rispetta lo 0 e la funzione successore che abbiamo definito.}
	\end{enumerate}
\end{theorem}

Fa comodo isolare la seguente osservazione.

\begin{remark}[Ogni numero in $\omega\setminus\emptyset$ è successore]
	$\forall x \in \omega \; x \ne 0 \rightarrow \exists y \in \omega \; x = s(y)$, ovvero ogni numero diverso da 0 è il successore di qualcos'altro.
\end{remark}

\begin{proof}
	Induzione su $x$. Il caso $x = 0$ è vero a vuoto (essendo la premessa sempre automaticamente falsa). Nel caso $x = s(m)$ basta prendere $y = m$ e si ha $x = s(y)$.
\end{proof}

Dimostriamo ora il teorema.

\begin{proof}
	Per il \hyperref[ric1]{teorema di ricorsione} (stiamo prendendo $A = \NN$, e $k = 0$ e $h = \text{succ}$) c'è un'unica $f$ che soddisfa le condizioni $f(\emptyset) = 0$ e $\forall n \in \omega \; f(s(n)) = \text{succ}(f(n))$.
	Resta da constatare che $f$ è bigettiva.
	\begin{itemize}
		\item[\boxed{\text{Surgettività}}] Per ipotesi $(\NN, 0, \text{succ})$ soddisfa il principio di induzione (poiché soddisfa gli assiomi di Peano). Dimostriamo quindi per induzione in $(\NN, 0, \text{succ})$ che $\forall y \in \NN \; \exists x \in \omega \; f(x) = y$.
		\begin{itemize}
			\item[$\boxed{\text{caso $y = 0$}}$] Basta osservare che $f(\emptyset) = 0$ per costruzione.
			\item[$\boxed{\text{caso $y = \text{succ}(n)$}}$] Per ipotesi induttiva esiste $x \in \omega$ tale che $f(x) = n$, da cui si ottiene, per definizione di $f$ che $f(s(x)) = \text{succ}(n)$.
		\end{itemize}
		\item[\boxed{\text{Iniettività}}] Consideriamo, per assurdo, il minimo $x \in \omega$ tale che, per qualche $y \in \omega$ con $y \ne x$, $f(x) = f(y)$.
		Osserviamo che, per la minimalità di $x$, $x<y$, quindi, in particolare $y \ne \emptyset$, e per l'osservazione possiamo scrivere $y = s(y')$. Procediamo quindi per induzione su $x$
		nel trovare un assurdo per ogni $x \in \omega$.
		\begin{itemize}
			\item[$\boxed{\text{caso $x = \emptyset$}}$] In questo caso si deve avere che:
			\[
				\text{succ}(f(y')) \overset{\text{def. $f$}}{=} f(s(y'))
								   \overset{y = s(y')}{=} f(y)
								   \overset{\text{Hp.}}{=} f(x) = 0
				\]
			che equivale a dire che 0 è successore di qualche numero contraddicendo l'osservazione (che vale anche per $(\NN, 0, \text{succ})$, in quanto soddisfa gli assiomi di Peano per ipotesi).
			\item[$\boxed{\text{caso $x \ne \emptyset$}}$] Per l'osservazione possiamo scrivere $x = s(x')$, da cui:
			\[ \text{succ}(f(x')) = f(s(x')) = f(x) = f(y) = f(s(y')) = \text{succ}(f(y'))
				\]
			e, per l'assioma \hyperref[a]{(a)} (iniettività del successore) in $(\NN, 0, \text{succ})$, segue che $f(x') = f(y')$. Allora, per la minimalità di $x$, siccome $x' < x$, dobbiamo avere $x' = y'$ (avevamo posto per ipotesi $x$ come minimo per cui c'è un elemento distinto $y$ che
			ha la stessa immagine, quindi qualsiasi cosa abbia la stessa immagine e sia più piccola di $x$ deve essere unica).
			Ma da questo seguirebbe $x = s(x') = s(y') = y$, contro l'ipotesi $\lightning$
		\end{itemize}
	\end{itemize}
\end{proof}

Se, infine, volgiamo la nostra attenzione all'esempio dei numeri di Fibonacci, vediamo che non è possibile definire questa sequenza applicando il \hyperref[ric1]{teorema di ricorsione}
in maniera diretta, perché $F_n$ non dispense solo dal termine precedente della sequenza, $F_{n-1}$, ma anche da $F_{n-2}$. Ce la si potrebbe cavare con un trucco, per esempio definendo la funzione
$n \mapsto (F_n,F_{n+1})$ da $\omega$ a $\omega \times \omega$. È comodo, però, disporre di una versione più versatile del teorema di ricorsione numerabile.

\begin{theorem}
	[Ricorsione numerabile - seconda forma]
	Dato un insieme $A$, denotiamo con $A^*$ l'insieme delle funzioni $g \subseteq \omega \times A$ con $\Dom(g) \in \omega$\footnote{Cioè è un numero di $\omega$.}. Sia $h : A^* \rightarrow A$, allora esiste un'unica funzione
	$f: \omega \rightarrow A$ tale che:
	\[ \forall n \in \omega \; f(n) = h(f_{|n}) \footnote{In altre parole, $f(n)$, può dipendere in maniera arbitraria dai valori assunti da $f$ sui numeri minori di $n$. Cioè $h$ è una funzione che manda funzioni che hanno come 
	dominio un $n \in \omega$ in $A$, in particolare $h(f_{|n})$ è una funzione di funzioni con dominio in $\omega$.}
		\]
\end{theorem}

\begin{example}
	[Esempio di applicazione]
	Per costruire la successione di Fibonacci, definiamo $h(g)$ in questo modo. Sia $n = \Dom(g)$. Se $n = \emptyset$ o $n = 1$, allora $h(g) = 1$.
	Altrimenti esistono $n-1, n-2 \in \omega$ tali che $s(n-1) = s(s(n-2)) = n$. Definiamo quindi $h(g) = g(n-1) + g(n-2)$\footnote{Abbiamo quindi ottenuto $h$ come funzione di funzioni con dominio in $\omega$ e in particolare più piccolo di $n$, dunque per il teorema tale $h$ definisce univocamente $f(n)$, a partire da $f_{|n} \in A^*$.}.
\end{example}

\begin{proof}
	L'idea è di definire, mediante la prima forma del \hyperref[ric1]{teorema di ricorsione}, la successione della troncata di $f$. Ossia la funzione $f' : n \mapsto f_{|n}$ (che manda $f$ nella sua restrizione al dominio $\{0,\ldots,n-1\}$) - un modo alternativo, sarebbe 
	ripetere la dimostrazione della prima forma -. Procediamo nel primo modo e costruiamo per ricorsione - prima forma - la funzione $f' : \omega \rightarrow A^*$ tale che:
	\[ f'(\emptyset) = \emptyset \qquad f'(s(n)) = f'(n) \cup \{(n,h(f'(n)))\}\,\footnote{Esiste ed è unica per il primo teorema di ricorsione}
		\]
	Ora poniamo $f(n) := f'(s(n))(n)$ ($f' \in A^*$, quindi è una funzione com dominio in $\omega$, quindi $f : \omega \rightarrow A$ è ben definita) e verifichiamo per induzione che effettivamente $f'$ sia la successione della troncata di $f$, cioè $\forall n \in \omega \; f_{|n} = f'(n)$.
	\begin{itemize}
		\item[$\boxed{\text{caso $n = 0$}}$] Si vede subito che $f_{|0} = f'(\emptyset)(n) = \emptyset$ (per come l'abbiamo costruita).
		\item[$\boxed{\text{caso $n = s(m)$}}$] In questo caso abbiamo:
		\[ \begin{split}
			f_{|s(m)} & = f_{|m} \cup \{(m,f(m))\}\\
					& = f'(m) \cup \{(m,f'(s(m))(m))\}\\
					& = f'(m) \cup \{(m,h(f'(m)))\} = f'(s(m))
		\end{split}
			\]
	\end{itemize}
	dove la prima uguaglianza segue per definizione di funzione (successione in questo caso specifico), la seconda per com'è definita $f$ in funzione di $f'$ e l'ultima per la definizione ricorsiva di $f'$. Infine, quindi, $f(n) \overset{\text{def.}}{=} f'(s(n))(n) = h(f'(n)) = h(f_{|n})$ (dove l'ultima uguaglianza segue per quanto abbiamo dimostrato).
\end{proof}

Abbiamo ora terminato di dimostrare le proprietà di base dei numeri naturali. Da qui, prende le mosse il corso di aritmetica. Nella prossima sezione, inizieremo
lo studio di un concetto squisitamente insiemistico: la cardinalità.

\begin{exercise}
	Dimostra commutatività, associatività, etc. di $+$ e $\cdot$.
\end{exercise}

\newpage
\section{Cardinalità}
Il concetto di cardinalità è, forse, il modo più semplice di contare gli elementi di un insieme: diciamo che due insiemi hanno un ugual numero di elementi se esiste
una corrispondenza biunivoca fra di essi.

\begin{definition}[Equipotenza/Cardinalità]
	Dati due insiemi $A$ e $B$:
	\[ |A| = |B| \Mydef \exists f \in {}^A B \; \text{``$f$ è bigettiva $A \rightarrow B$''}
		\]
	diciamo anche che ``$A$ ha la stessa \vocab{cardinalità} di $B$'' o che ``$A$ e $B$ sono \vocab{equipotenti}''. Poniamo inoltre:
	\[ |A| \leq |B| \Mydef \exists B' \subseteq B \; |A| = |B'| 
		\]
	ossia $\exists f \in {}^A B$ ``$f$ è iniettiva'' (la definizione ci dice proprio che esiste un sottoinsieme di $B$ che è in bigezione con $A$, e per definizione di iniettività, si ha proprio che $A \hookrightarrow B$)\footnote{Tale relazione sarà anche una relazione di ordine tra cardinalità quando queste ultime saranno singoli oggetti della teoria.}.
\end{definition}

\begin{note}[Sulla notazione per le cardinalità]
	Osserviamo che:
	\begin{itemize}
		\item La scrittura $|A| = |B|$ suggerisce che esistono insiemi - o oggetti di qualche genere - denotati $|A|$ e $|B|$ di cui si predica l'uguaglianza.
		Effettivamente costruiamo questi oggetti, ma, per ora, la scrittura $|A| = |B|$ è inscindibile, come \ding{168}$[A,B)$ (nel senso che per ora è solo un'abbreviazione per dire bigezione, pertanto non possiamo separare quei simboli o farci qualcosa).
		\item Potrebbe sorgere il sospetto che se $|A|\textcolor{red}{<}|B|$ quando $A \subsetneq B$, ma non è così, come mostra l'esempio di $A = \{x \in \omega | x > 0\}$ e $B = \omega$, infatti $A \subsetneq B$, ma $|A| = |B|$.
	\end{itemize}
\end{note}

\begin{remark}[Proprietà formali di una relazione di equivalenza]
	La relazione $|\cdot| = |\cdot|$ soddisfa le proprietà formali di una relazione di equivalenza (ma per ora NON lo è\footnote{Potrebbe tuttavia essere pensata come una relazione di equivalenza su $V$ (la classe di tutti gli insiemi).}):
	\begin{itemize}
		\item \textbf{riflessività}: $|A| = |A|$.
		\item \textbf{simmetria}: $|A| = |B| \rightarrow |B| = |A|$.
		\item \textbf{transitività}: $|A| = |B| \land |B| = |C| \rightarrow |A| = |C|$.
	\end{itemize}
\end{remark}

\begin{exercise}
	Dimostrare l'osservazione.
\end{exercise}

\begin{soln}
Per la riflessività basta osservare che $\id_A$ è una bigezione da $A$ in $A$. Per la simmetria, abbiamo visto che se $f : A \rightarrow B$ è iniettiva, allora ammette
inversa $g : \Imm(f) \rightarrow A$ a sua volta iniettiva (e surgettiva poiché ha necessariamente come immagine tutto $A$), inoltre, essendo $f$ bigettiva si ha che $\Imm(f) = B$, quindi $g : B \rightarrow A$, e 
per quanto detto è bigettiva, dunque nel linguaggio della cardinalità $|B| = |A|$.\\
Infine, $|A| = |B| \iff \exists f : A \rightarrow B$ bigettiva, $|B| = |C| \iff \exists g : B \rightarrow C$ bigettiva, ora è sufficiente osservare che $g \circ f : A \rightarrow C$ è bigettiva in quanto composizione di funzioni bigettive\footnote{È una semplice verifica.}, per avere $|A| = |C|$.
\end{soln}

\begin{remark}[Proprietà formali $\lbrack$parziali$\rbrack$ di una relazione di ordine $\lbrack$largo$\rbrack$]
	La relazione $|\cdot| \leq |\cdot|$ soddisfa\footnote{Tali proprietà, unite al teorema di Cantor-Bernstein, che stiamo per vedere, ci danno una relazione di ordine totale su $V$.}:
	\begin{itemize}
		\item \textbf{riflessività}: $|A| \leq |A|$.
		\item \textbf{transitività}: $|A| \leq |B| \land |B| \leq |C| \rightarrow |A| \leq |C|$.
	\end{itemize}
\end{remark}

\begin{exercise}
	Dimostrare l'osservazione.
\end{exercise}

\begin{soln}
Per la riflessività basta osservare che $\id_A$ è in particolare una mappa iniettiva (oppure che $A$ è un sottoinsieme [improprio] di se stesso e quindi l'identità è la bigezione richiesta dalla definizione).
Per la transitività $|A| \leq |B| \iff \exists A \hookrightarrow B$, $|B| \leq |C| \iff \exists g : B \hookrightarrow C$, e osservando che la composizione di funzioni iniettive è iniettiva, si ha che $g \circ f : A \rightarrow C$ è iniettiva $\iff |A| \leq |C|$.
\end{soln}

Per stabilire che le cardinalità sono, formalmente, ordinate dalla relazione $|\cdot| \leq |\cdot|$, ci manca l'antisimmetria, che è appunto enunciata dal teorema seguente.

\subsection{Teorema di Cantor-Bernstein}

\begin{theorem}
	[Cantor-Bernstein]
	\label{CB}
	Se c'è una funzione iniettiva $A \rightarrow B$ e una funzione iniettiva $B \rightarrow A$, allora esiste una bigezione fra $A$ e $B$.
	\[ \forall A,B (|A| \leq |B| \land |B| \leq |A|) \rightarrow |A| = |B|
		\]
\end{theorem}

\begin{proof}
	Per ipotesi abbiamo quindi $f: A \rightarrow B$ e $g: B \rightarrow A$ iniettive. Il nostro obiettivo è costruire una nuova funzione $h : A \rightarrow B$ bigettiva.\\
	L'idea è che ogni elemento, poniamo, di $A$, è una tappa di un percorso:
	\[ a \overset{f}{\longrightarrow} f(a) \overset{g}{\longrightarrow} g(f(a)) \overset{f}{\longrightarrow} f(g(f(a))) \overset{g}{\longrightarrow} \dots
		\]
	Siccome $f$ e $g$ sono iniettive, questo percorso ha altresì un'unica estensione all'indietro (abbiamo visto che se le funzioni sono iniettive, allora ammettono un'inversa iniettiva dalle rispettive immagini (che è anche surgettiva), dunque possiamo sempre tornare indietro in modo unico, estendendo quindi 
	il nostro percorso anche nell'altra direzione):
	\[ \textcolor{red}{f^{-1}(g^{-1}(a))} \overset{f}{\longrightarrow} \textcolor{red}{g^{-1}(a)} \overset{g}{\longrightarrow} a \overset{f}{\longrightarrow} f(a) \overset{g}{\longrightarrow} g(f(a)) \overset{f}{\longrightarrow} f(g(f(a))) \overset{g}{\longrightarrow} \ldots
		\]
	a patto che $a \in \Imm(g)$ (perché l'inversa $g^{-1}$ va da $\Imm(g)$ a $B$), $g^{-1}(a) \in \Imm(f)$, etc. Quando, e se, non possiamo più applicare la funzione inversa, il percorso (all'indietro) si interrompe. Con questa catena di composizioni ci sono quindi tre tipi di percorsi possibili:
 	\begin{center}
		\begin{figure}[H]
			\centering
			\includegraphics[width=12.5cm]{immagini/cantor-ber1a.png}
		\end{figure}
	\end{center}
	\begin{center}
		\begin{figure}[H]
			\centering
			\includegraphics[width=12.5cm]{immagini/cantor-ber1b.png}
		\end{figure}
	\end{center}
	
	Per gli elementi che si trovano su un percorso circolare, o su un percorso illimitato avanti e indietro, $f$ fornisce una bigezione, come la fornirebbe anche $g^{-1}$ - la scelta è arbitraria a patto di 
	usare la medesima funzione per l'intero percorso - nel modo seguente\footnote{Informalmente, se siamo in uno dei due casi, allora $f$ è per forza una mappa bigettiva, perché è iniettiva e ``prende'' tutti gli elementi in arrivo, idem $g^{-1}$.}:
	\begin{center}
		\begin{figure}[H]
			\centering
			\includegraphics[width=12.5cm]{immagini/cantor-ber2a.png}
		\end{figure}
	\end{center}
	\begin{center}
		\begin{figure}[H]
			\centering
			\includegraphics[width=11.5cm]{immagini/cantor-ber2b.png}
		\end{figure}
	\end{center}
	Per i percorsi, invece, illimitati solo a destra, occorre vedere in quale insieme sta l'elemento iniziale del percorso: se questo è in $A$, la bigezione è data da $f$,
	altrimenti se sta in $B$ la bigezione è data da $g^{-1}$.
	\begin{center}
		\begin{figure}[h!]
			\centering
			\includegraphics[width=12.5cm]{immagini/cantor-ber3.png}
		\end{figure}
	\end{center}
	Per comodità poniamo quindi [la bigezione $h$], $h(x) = f(x)$ in ogni caso, eccetto quando $x$ è lungo un percorso che parte da $B$, nel cui caso poniamo $h(x) = g^{-1}(x)$.\\
	Formalmente, definiamo per \hyperref[ric1]{ricorsione} (prima forma) le seguenti successioni di sottoinsiemi di $B$ e $A$ rispettivamente - ossia, tecnicamente, la funzione $\omega \rightarrow \ps(B) \times \ps(A) : i \mapsto (B_i,A_i)$, con:
	\[ B_0 = B \setminus\Imm(f) \qquad A_i = g[B_i] \qquad B_{s(i)} = f[A_i]
		\]
	(ovvero la successione dei $B_i$ è definita con la prima forma della ricorsione, mentre quella degli $A_i$ dipende semplicemente da quest'ultima, ma non direttamente per ricorsione). Definiamo quindi:
	\[ B_* = \bigcup_{i \in \omega}B_i \Mydef \bigcup\{B_i | i \in \omega\} \qquad A_* = \bigcup_{i \in \omega} A_i
		\]
	Questi sono i punti che appartengono a cammini che partono da $B$, definiamo quindi $h : A \rightarrow B$ e $k : B \rightarrow A$ come segue:
	\[ h(x) = \begin{cases}
		g^{-1}(x) &\text{se $x \in A_*$}\\
		f(x) &\text{altrimenti}
	\end{cases}
	\qquad
	k(y) = \begin{cases}
		g(y) &\text{se $y \in B_*$}\\
		f^{-1}(y) &\text{altrimenti}
	\end{cases}
		\]
	queste mappe coprono tutti i casi  possibili, infatti, i percorsi ciclici e illimitati da entrambe le parti sono coperti da $k = f^{-1}$ ed $h = g^{-1}$, mentre nel caso di percorsi che partono da $B$ e sono limitati a sinistra, ovvero con primo elemento in $B^*$ abbiamo che $k(y) = g(y)$, invece nel caso simmetrico, 
	in cui si parte da $A$ con percorso limitato a sinistra si ha $h(x) = f(x)$, in tal modo prendiamo tutti gli elementi di tutti i cicli possibili che si formano nei due insiemi usando i percorsi descritti sopra.\\
	Ci basta quindi dimostrare che $h$ e $k$ sono ben definite, $k \circ h = \id_A$ e $h \circ k = \id_B$, in tal modo avremo la nostra bigezione (e la sua inversa).
	\begin{itemize}
		\item[$\boxed{\text{$h$ e $k$ ben definite}}$] Occorre verificare che stiamo applicando $g^{-1}$ e $f^{-1}$ a elementi della immagine di $g$ e $f$ rispettivamente.
		Nella definizione di $h$, se $x \in A_*$, allora $x \in A_i$, per qualche $i \in \omega$, quindi $x \in g[B_i] \subseteq \Imm(g)$. Nella definizione di $k$, se $y \not \in B_*$, in particolare,
		$y \not \in B_0$, per cui $y \in \Imm(f)$.
		\item[$\boxed{k \circ h = \id_A}$] Se $x \in A_*$, allora $x \in A_i$, per qualche $i \in \omega$, quindi $x = g(y)$, con $y \in B_i$, per cui $k(h(y)) = k(g^{-1}(x)) = k(y) = g(y) = x$ (abbiamo usato che $y = g^{-1}(x) \in B_*$ per quanto supposto sopra).\\
		Per il caso $x \not \in A_*$, osserviamo, intanto, che $x \not \in A_* \implies f(x) \not \in B_*$. Infatti, se $f(x) \in B_i$, con $i \in \omega$, allora $i \ne 0$, perché $B_0 = B \setminus \Imm(f)$, quindi possiamo scrivere $i = s(j)$, 
		e $f(x) \in B_{s(j)} = f[A_j]$. Per l'iniettività di $f$, abbiamo allora $x \in A_j \;\lightning$\\
		Di conseguenza, se $x \not \in A_*$, $k(h(x)) = k(f(x)) \overset{f(x) \not \in B_*}{=} f^{-1}(f(x)) = x$.
		\item[$\boxed{h \circ k = \id_B}$] Se $y \in B_*$, allora $y \in B_i$, per qualche $i \in \omega$, quindi $g(y) \in A_i$. Di conseguenza $h(k(y)) = h(g(y)) = g^{-1}(g(y)) = y$. Altrimenti $y \not \in B_*$ e, se $f^{-1}(y)\in A_*$, avremmo una contraddizione,
		perché $f^{-1}(y) \in A_i \rightarrow y = f(f^{-1}(y)) \in A_{s(i)}$. Quindi $h(k(y)) = h(f^{-1}(y)) = f(f^{-1}(y)) = y$.
	\end{itemize}
\end{proof}

Visto che $|\cdot|\leq|\cdot|$ ha le proprietà formali di una relazione d'ordine fra le classi di equivalenza della relazione $|\cdot| = |\cdot|$, possiamo definire il corrispondente ordine stretto.

\begin{definition}[Ordinamento stretto fra cardinalità]
	Dati due insiemi $A$ e $B$ definiamo:
	\[ |A| < |B| \Mydef |A| \leq |B| \land |A| \ne |B| \, \footnote{Dove ricordiamo che $|A| \ne |B| \Mydef \neg(|A| = |B|)$.}
		\]
\end{definition}

\subsection{Teorema di Cantor}

\begin{theorem}[Cantor]
	\label{cantor}
	Dato un qualunque insieme $A$ vale:
	\[ |A| < |\ps(A)|
		\]
\end{theorem}

La dimostrazione di questo enunciato è, ancora una volta, il medesimo argomento del paradosso di Russell.

\begin{proof}
	La disuguaglianza $|A| \leq |\ps(A)|$ è facile: basta considerare la funzione iniettiva:
	\[ A \longrightarrow \ps(A) : x \longmapsto \{x\}
		\]
	(che è iniettiva per \hyperref[ax2]{estensionalità}). Consideriamo, ora, una qualunque funzione $f : A \rightarrow \ps(A)$ iniettiva.
	Dobbiamo dimostrare che $\Imm(f) \subsetneq \ps(A)$ (cioè che non è surgettiva). Consideriamo:
	\[ B = \{x \in A | x \not \in f(x)\} \, \footnote{$f(x) \in \ps(A)$, ovvero è un sottoinsieme di $A$, quindi stiamo considerando il sottoinsieme degli elementi di $A$ che non stanno nelle loro immagini (dei sottoinsiemi di $A$).}
		\]
	Ora $B \subseteq A$, supponendo per assurdo che $f$ sia bigettiva, ovvero che $B = f(a)$ per qualche $a \in A$, avremmo:
	\[ a \in f(a) \subseteq A \iff a \in B \iff a \not \in f(a) \; \lightning
		\]
\end{proof}

\subsection{Operazioni fra cardinalità}

\begin{definition}[Somma, prodotto e potenze di cardinalità]
	Dati $A$ e $B$ possiamo definire somma, prodotto e potenze di cardinalità come segue:
	\[ \begin{split}
		|A| + |B| &\Mydef |A \sqcup B| \Mydef |(A \times \{0\}) \cup (B \times \{1\})| \\
		|A|\cdot |B| &\Mydef |A \times B| \\
		|A|^{|B|} &\Mydef |{}^{B}A|
	\end{split}
		\]
	(nella definizione di unione disgiunta abbiamo fatto il prodotto per cose diverse, in modo che gli elementi comuni ai due insiemi sono comunque diversi per la seconda componente, e quindi siano contati due volte.)
\end{definition}

Osserviamo che le operazioni fra cardinalità così date sono ben definite.

\begin{proposition}[Buona definizione delle operazioni]
	Le operazioni di somma, prodotto e potenza fra cardinalità sono ben definite, ossia dati $A,B,A',B'$, con $|A| = |A'|$ e $|B| = |B'|$, vale:
	\[ |A| + |B| = |A'| + |B'| \qquad |A|\cdot|B| = |A'|\cdot|B'| \qquad |A|^{|B|} = |A'|^{|B'|}
		\]
\end{proposition}

\begin{proof}
	Date $f : A \rightarrow A'$ e $g : B \rightarrow B'$ bigettive, è immediato verificare che le seguenti sono bigezioni:
	\[  \begin{split}
		A \sqcup B \longrightarrow A' \sqcup B' :\,& (a,0) \longmapsto (f(a),0) \\
											 \,& (b,1) \longmapsto (g(b),1) \\
	 	A \times B \longrightarrow A' \times B' :\,& (a,b) \longmapsto (f(a),g(b)) \\
		{}^{B}A \longrightarrow {}^{B'}A' :\,& h \longmapsto f \circ h \circ g^{-1} \, \footnote{Deriva dal diagramma commutativo che si può disegnare.}
		\end{split}
		\]
	ed equivalgono alle uguaglianze di cardinalità nella tesi.
\end{proof}

\begin{notation}[Cardinalità finite]
Riferendoci alle cardinalità finite $|\emptyset|,|1|,|2|,\ldots$ se non c'è rischio di confusione, scriveremo semplicemente $0,1,2,\ldots$
\end{notation}

\begin{remark}[Teorema di Cantor rivisitato]
	$|\ps(A)| = 2^{|A|}$, per cui il \hyperref[cantor]{teorema di Cantor}, può essere enunciato dicendo che, dato un qualunque $A$, vale $|A| < 2^{|A|}$.
\end{remark}

Verifichiamo che effettivamente ci sia una bigezione tra l'insieme delle parti di $A$ e quello delle funzioni da $A$ in 2.

\begin{proof}
	La funzione che ad ogni $B \in \ps(A)$ associa la sua \vocab{funzione indicatrice} $\chi_B : A \rightarrow 2$ è definita da:
	\[ \chi_B(x) = \begin{cases}
		1 &\text{se $x \in B$}\\
		0 &\text{altrimenti}
	\end{cases}
		\]
	ed è una bigezione $\ps(A) \rightarrow {}^{A}2$ (ovvero $|\ps(A)| = |{}^{A}\{0,1\}| = |{}^{A}2|$ per la nostra codifica dei naturali, e per la definizione data prima la seconda cardinalità corrisponde proprio all'operazione $|2|^{|A|}$).
\end{proof}

\begin{proposition}[Proprietà delle operazioni fra cardinalità]
	Le operazioni fra cardinalità godono delle proprietà seguenti: denotando, per brevità, con $\alpha,\beta,\gamma$ i simboli: $|A|,|B|,|C|$:
	\begin{align*}
		&\alpha + 0 = \alpha 	 & \quad & \alpha + \beta = \beta + \alpha 		 	& \quad & \alpha + (\beta + \gamma) = (\alpha + \beta) + \gamma \\
		&\alpha \cdot 0 = 0 	 & \quad & \alpha \cdot \beta = \beta \cdot \alpha 	& \quad & \alpha \cdot (\beta \cdot \gamma) = (\alpha \cdot \beta) \cdot \gamma \\
		&\alpha \cdot 1 = \alpha & \quad & \alpha \cdot (\beta + \gamma) = \alpha \cdot \beta + \alpha \cdot \gamma \\
		&\alpha^0 = 1 			 & \quad & (\alpha^\beta)^\gamma = \alpha^{\gamma \cdot \beta} & \quad & (\alpha \cdot \beta)^\gamma = \alpha^\gamma \cdot \beta^\gamma \\
		&1^\alpha = 1			 & \quad & \alpha^{\beta + \gamma} = \alpha^\beta \cdot \alpha^\gamma
	\end{align*}
\end{proposition}

\begin{proof}
	In ciascun caso, si tratta semplicemente di esibire una bigezione esplicita fra il membro di sinistra e il membro di destra. Come esempio, vediamo uno dei casi più complicati, il resto 
	è lasciato come \textcolor{red}{esercizio}. \\
	Dimostriamo che $(|A|^{|D|})^{|C|} = |A|^{|C|\cdot|B|}$. Dobbiamo esibire una bigezione fra l'insieme ${}^C({}^BA)$ delle funzioni che ad ogni elemento di $C$ associano una funzione $B \rightarrow A$, e l'insieme ${}^{C \times B}A$, delle funzioni 
	che ad ogni coppia di elementi in $C \times B$ associano un elemento di $A$. Associamo a $f \in {}^C({}^BA)$ la funzione $\widetilde{f} \in {}^{C \times B}A$ definita da:
	\[ \widetilde{f}(c,b) = (\underbrace{f(c)}_{\in {}^{B}A})(\underbrace{b}_{\in B}) \, \footnote{Cioè la mappa $\sim$ prende una funzione da $C$ a ${}^{B}A$ e la manda in un'altra che prende coppie di elementi in $C \times B$, e valuta il primo elemento in $f$ 
	per ottenere una mappa da $B$ a $A$, che poi valuta in $b \in B$.}
		\]
	Dimostriamo che l'inversa di questa applicazione associa a $g \in {}^{C \times B}A$ la funzione $\overline{g} \in {}^{C}({}^B A)$ definita da:
	\[	\overline{g}(c) : B \longrightarrow A : b \longmapsto g(c,b) \, \footnote{Ovvero la mappa $-$ associa una mappa di ${}^{C\times B}A$ con la mappa $\overline{g} \in {}^{C}({}^B A)$, che valutata in $c \in C$, dà una funzione da $B$ in $A$, che ad ogni $b \in B$ associa $g(c,b)$.}
		\]
	La verifica è facilissima, presa $g \in {}^{C \times B}A$ si ha:
	\[ \forall (c,b) \in C \times B \quad \widetilde{\overline{g}}(c,d) = (\overline{g}(c))(b) = g(c,b) \implies \widetilde{\overline{g}} = g
		\]
	(quindi $\sim \circ -$ è l'identità). Presa $f \in {}^{C}({}^B A)$, e fissato un qualunque $c \in C$, si ha:
	\[ \forall b \in B \,(\overline{\widetilde{f}}(c)(b)) = \widetilde{f}(c,b) = (f(c))(b) \implies \overline{\widetilde{f}}(c) = f(c)
		\]
	da cui, per l'arbitrarietà di $c$, $\overline{\widetilde{f}} = f$ (e quindi $- \circ \sim$ è l'identità).
\end{proof}


\newpage
\section{Cardinalità finite}
Ora inizia una breve carrellata fra le cardinalità più facile da definire. Parliamo qui di cardinalità finite, poi introdurremo la cardinalità numerabile e la cardinalità del continuo.

\begin{definition}[Insieme finito/infinito]
	Diciamo che $A$ è \vocab{finito} se $\exists n \in \omega \, |A| = |n|$. Se $A$ non è finito, diciamo che $A$ è \vocab{infinito}.
\end{definition}

Storicamente, è riflessiva una definizione alternativa di finitezza, data originariamente da Dedekind.

\begin{definition}[Dedekind-finitezza]
	Diciamo che $A$ è \vocab{Dedekind-finito} se non può essere messo in corrispondenza biunivoca con un suo sottoinsieme proprio. Ossia $A$ è Dedekind-finito se:
	\[ \forall B \subsetneq A \, |B| < |A|
		\]
\end{definition}

\subsection{Principio dei cassetti}
Con gli assiomi introdotti fino ad ora, possiamo solo dimostrare che finito $\rightarrow$ Dedekind-finito, mentre l'implicazione inversa è conseguenza dell'assioma della scelta.

\begin{proposition}[Principio dei cassetti - ossia - finito $\rightarrow$ Dedekind-finito]
	\label{cassetti}
	Dato $A$ finito e $B$ un sottoinsieme proprio di $A$, $B \subsetneq A$, vale $|B| < |A|$. 
\end{proposition}

\begin{proof}
	Naturalmente $|B| \leq |A|$ vale perché l'identità $\id_B$ è una funzione iniettiva $B \rightarrow A$. Occorre quindi dimostrare che $|B| \ne |A|$.\\
	Supponiamo per assurdo che $|B| = |A|$. Osserviamo che, senza perdita di generalità, possiamo assume $A = n \in \omega$\footnote{Quello che faremo è proprio portare $B \subsetneq A$ in $f[B] \subsetneq f[A] = n$ e qui trovare l'assurdo, che è come assumere sempre che 
	$A = n$, perché possiamo sempre spostare il problema in $\omega$ con una bigezione.}. Per ipotesi, infatti esiste $f : A \rightarrow n$ bigettiva, per un opportuno $n \in \omega$.
	Quindi $f[B] \subsetneq n$ (volendo perché la restrizione di $f$ a $B$ è ancora iniettiva ma non surgettiva\footnote{È conseguenza del fatto che $B$ sia un sottoinsieme proprio e che $f$ è bigettiva.}, quindi non può
	avere in arrivo tutto $n$). D'altro canto, per l'iniettività di $f$, $|f[B]| = |B| \overset{\text{Hp. assurda}}{=} |A| = n$.
	Ci basta quindi dimostrare per induzione su $n$, che:
	\[ \forall n \in \omega \; \forall B \subseteq n (|B|=|n|\rightarrow B = n)
		\]
	(cioè che ogni sottoinsieme di un numero naturale con la stessa cardinalità è il numero stesso) in questo modo avremmo $f[B] = f[A] = n$ (prima avevamo un sottoinsieme di $A$ non di $n$), che è assurdo in quanto abbiamo detto che $f[B] \subsetneq n$.
	\begin{itemize}
		\item[$\boxed{\text{caso $n = \emptyset$}}$] Necessariamente $B = \emptyset$, quindi $B = n$ come richiesto dalla tesi.
		\item[$\boxed{\text{caso $n = s(m)$}}$] L'ipotesi induttiva è $\forall C \subseteq m \; |C| = |m| \rightarrow C = m$, vogliamo dimostrare che $\forall B \subseteq s(m) \; |B| = |s(m)| \rightarrow B = s(m)$.\\
		Sia $f : s(m) \rightarrow B$ bigettiva (come ipotesi antecedente). Si danno due casi. Se $f(m) = m$ (ricordiamo che l'insieme d'arrivo è un sottoinsieme di $s(m)$), allora sia $C := \Imm(f_{|m})$, e, per l'iniettività di $f$, si ha $|C| = |m|$, quindi, per l'ipotesi induttiva (essendo $C \subseteq m$), vale $C = m$. Ma in questo modo $B = \Imm(f) = C \cup \{\underbrace{f(m)}_{= m}\} = m \cup \{m\} = s(m) = n$.
		\begin{figure}[H]
			\centering
			\includegraphics[width = 1.95cm]{immagini/cassetti1.png}
		\end{figure}
		Se $f(m) \ne m$, allora vediamo che esiste $a < m$ tale che $f(a) = m$. Se così non fosse, infatti, $f_{|m}$ sarebbe una bigettiva fra $m$ e $m \setminus \{f(m)\}$, contro l'ipotesi induttiva. Ora, però, possiamo costruire una nuova bigezione $f' : s(m) \rightarrow B$ che ricade nel caso precedente:
		\[ f'(x) = \begin{cases}
			m &\text{se $x = m$} \\
			f(m) &\text{se $x = a$} \\
			f(x) &\text{altrimenti} \\
		\end{cases}
			\]
			\begin{figure}[H]
				\centering
				\includegraphics[width = 1.95cm]{immagini/cassetti2.png}
			\end{figure}
		in altre parole stiamo ``aggiustando'' la bigezione $f$ in modo che venga di nuovo una bigezione $f'$, tale che $f'(m) = m$ e si ricade nel caso precedente (e lo possiamo sempre fare, come osservato).
	\end{itemize}
\end{proof}

\begin{corollary}[$A$ finito $\implies$ ha un'unica cardinalità]
	Se $A$ è un insieme finito, allora esiste ed è unico un elemento di $\omega$ con cui è in bigezione:
	\[ \exists \textcolor{red}{!} n \in \omega \; |A| = |n|
		\]
\end{corollary}

\begin{proof}
	Se $|m| = |A| = |n|$, possiamo assumere, senza perdita di generalità $m \leq n$, ossia $m \subseteq n$, quindi, usando il \hyperref[cassetti]{principio dei cassetti} $m = n$, abbiamo quindi l'unicità.
\end{proof}

Se adesso volessimo dimostrare il viceversa: [che formulato in versione contronominale è] che un insieme infinito non è Dedekind-finito, quale sarebbe l'ostacolo? Abbiamo già osservato che $\omega$ non è finito, perché la funziona successore 
stabilisce una corrispondenza biunivoca fra $\omega$ e $\omega \setminus\{0\} \subsetneq \omega$ (quindi non è Dedekind-finito, e per la contronominale del \hyperref[cassetti]{principio dei cassetti} non è finito). Ne segue la seguente osservazione.

\begin{remark}
	Se esiste $f : \omega \rightarrow A$ iniettiva, allora $A$ non è Dedekind-finito.
\end{remark}

\begin{proof}
	Basta considerare la funzione iniettiva:
	\[ g : A \longrightarrow A : a \longmapsto \begin{cases}
		f \circ s \circ f^{-1}(a) &\text{se $a \in f[\omega]$} \\
		\id_A(a) &\text{altrimenti}
	\end{cases}
		\]
	È immediato vedere che $\Imm(g) = A \setminus \{f(0)\} \subsetneq A$ (l'unico escluso è lo 0, perché non può esserci un elemento che ha come controimmagine un elemento di $\omega$ il cui
	successore sia 0, perché per quanto visto non esiste), dunque $A \hookrightarrow \Imm(g) \subsetneq A$, pertanto è in bigezione con un suo sottoinsieme proprio, per cui non può essere Dedekind-finito.
	\begin{figure}[H]
		\centering
		\includegraphics[width = 4.5cm]{immagini/cassetti3.png}
	\end{figure}
\end{proof}

Quindi ci basterebbe dimostrare che $\omega$ si immerge in ogni insieme infinito (e dal lemma appena visto avremmo che l'insieme non è Dedekind-finito, completando l'altra freccia del principio dei cassetti).
Un tentativo di dimostrazione potrebbe andare come segue.

\begin{proof}
	Sia $A$ infinito, costruiamo per ricorsione, seconda forma, una $f : \omega \rightarrow A$ iniettiva. Supponiamo di conoscere $f_{|n}$, il nostro scopo è definire il prossimo valore: $f(n)$.
	Siccome $A$ è infinito, $f_{|n}$, che è iniettiva per costruzione, non può essere surgettiva, quindi esiste $a \in A$ con $a \not \in \Imm(f_{|n})$. Pongo $f(n) = a$.
\end{proof}

Dov'è l'errore? Nell'ultima riga! Noi sappiamo che, data $f_{|n}$, esistono degli $a \in A$ con $a \not\in\Imm(f_{|n})$, questo è corretto. È anche corretto che ci basterebbe porre $f(n) =$``uno qualunque di questi $a$''.
Il guaio è che, \underline{per applicare il teorema di ricorsione}, ci serve una funzione che fissa (nel senso che la $h$ del teorema di ricorsione essendo un insieme deve avere tutti gli elementi già fissati, cosa che non può avvenire in questo caso) uno degli $a$. A patto di averne una, ne andrebbe bene una qualunque.\\
Purtroppo però, a partire dalla mera ipotesi che $A$ è infinito, non abbiamo modo di procurarci nessuna funzione del genere. Potremmo cavarcela se avessimo qualche struttura su $A$, sulla quale far leva - per esempio per dire ``prendo il minimo 
fra gli $a \not\in \Imm(f_{|n})$'', o ``prendo il più giallo'' - ma di $A$ non sappiamo nulla, e non abbiamo modo di indurre una struttura di questo genere.\\
Accettato che non possiamo dimostrare che $\omega$ si immerge in qualsiasi insieme infinito, possiamo però lambire questa soglia: dimostriamo che, in un insieme infinito, si immergono tutti i numeri naturali.

\begin{proposition}[Tutti i naturali si immergono in un insieme infinito]
	Sia $A$ infinito, allora $\forall n\in \omega \; |n| < |A|$.
\end{proposition}

\begin{proof}
	Basta dimostrare il $\leq$, infatti $|n| < |n+1| \leq |A|$. Dimostriamo per induzione su $n$ che c'è una funzione iniettiva da $n$ ad $A$.
	\begin{itemize}
		\item[$\boxed{\text{caso $n = 0$}}$] La funzione vuota, $f = \emptyset$.
		\item[$\boxed{\text{caso $n = m+1$}}$] Per ipotesi induttiva esiste $f : m \rightarrow A$ iniettiva. Siccome $A$ è infinito (e $m$ è finito), esiste $a \in A \setminus \Imm(f)$ (e non ci serve fissarlo poiché non stiamo usando il teorema di ricorsione).
		La funzione $f' = f \cup \{(n,a)\}$, che si ottiene estendendo $f$ col 
		mandare $n$ in $a$, è iniettiva $n \hookrightarrow A$.
	\end{itemize}
\end{proof}

\begin{corollary}[Ovvietà]
	Un sottoinsieme di un insieme finito è finito.
\end{corollary}

\begin{proof}
	Sia $A$ finito e $B \subseteq A$. Se, per assurdo $B$ fosse infinito, avremmo $|A| < |B| \leq |A| \, \lightning$ (poiché $|A| = |n|$ per definizione di finito e per la proposizione precedente tutti gli $n$ si immergono in un insieme infinito si ha $A \rightarrow n \hookrightarrow B$, dove la prima funzione è bigettiva e la seconda iniettiva, e per le proprietà di composizione delle 
	funzioni iniettive, la composizione di queste ultime due ci dà $A \hookrightarrow B$, da cui $A \hookrightarrow A$, ma essendo finito è anche Dedekind-finito, quindi questo è assurdo).
\end{proof}

\begin{exercise}
	Dimostrare che:
	\begin{itemize}
		\item se $|A| < |n|$ con $n \in \omega$, allora $|A| = |m|$ per qualche $m < n$.\footnote{Fondamentalmente ogni sottoinsieme di $\omega$ (che non è detto sia un elemento di $\omega$) è in bigezione con un elemento di $\omega$.}
		\item se $A$ è finito e $f : A \rightarrow B$, allora $f[A]$ è finito.
	\end{itemize}	
\end{exercise}

\begin{soln}
	Verifichiamo le due cose separatamente:
	\begin{itemize}
		\item Se $|A|<|n|$, allora esiste una funzione $f : A \hookrightarrow n$ iniettiva ma non surgettiva, in particolare si ottiene che $f[A]\subsetneq n$,
		con $|A| = |f[A]|$, osservando che $n$ è finito e usando il corollario sopra, si ottiene che $f[A]$ è finito, in particolare $|A| = |f[A]| = m$, per $m \in \omega$.
		Infine, abbiamo che $|m| = |A| < |n|$, ci resta da osservare che:
		\[ \forall m,n \in \omega \; |m|<|n|\longleftrightarrow m <n
			\]
		La freccia $\leftarrow$ è banale, perché, per un noto corollario $m < n \leftrightarrow m \subsetneq n$, che implica $|m| < |n|$ (basta usare $\id_m$).\footnote{Per concludere si può osservare,
		alternativamente, che $n$ finito implica Dedekind-finito, dunque non può essere in bigezione con un suo sottoinsieme in proprio $m$ (di fatto stiamo nascondendo sotto al tappeto la definizione di $<$ tra cardinalità),
		dunque la disuguaglianza [ottenuta da $m \subseteq n$] deve essere stretta.}\\
		Viceversa, posto $|m|<|n|$, se per assurdo fosse $m \geq n$, allora per definizione di $\leq$ tra cardinalità $|n| \leq |m|$, da cui $|m|<|n|\leq |m| \implies |m| < |m|$, che è assurdo perché viola il principio dei cassetti.
		\item Diamo per buono che in generale $|f[A]| \leq |A|$. Ora $|A|$ è finito, dunque, per il corollario sopra, il suo sottoinsieme $g[f[A]]$ è finito (con $g$ la mappa iniettiva da $f[A]$ ad $A$), pertanto [poiché $g$ iniettiva]
		$|f[A]| = |g[f[A]]| = m$, per $m \in \omega$, e quindi abbiamo che $f[A]$ è finito.\\
		Ci resta da verificare l'assunzione iniziale, possiamo farlo con la seguente funzione:
		\[ g : f[A] \longrightarrow A : x \longmapsto \min_{<_{|n}}\{h[f^{-1}(x)]\}
			\]
		dove $<_{|n}$ è l'ordine usuale di $\omega$ ristretto ad $n$ e $h$ è la bigezione che esiste per ipotesi da $A$ ad $n$.
		Vediamo che $g$ è iniettiva:
		\[	\begin{split}
			g(x) = g(y) &\iff \min_{<_{|n}}\{h[f^{-1}(x)]\} = \min_{<_{|n}}\{h[f^{-1}(y)]\} \\
						&\iff h(a) = h(b)
		\end{split}
			\]
		con $a \in f^{-1}(x)$ e $b \in f^{-1}(y)$ (in altre parole abbiamo dato un nome ai minimi). Ora, essendo $h$ bigettive quanto scritto equivale ad $a  = b$, che, applicando $f$,
		equivale a:
		\[ x = f(a) \overset{a = b}{=} f(b) = y
			\]
		per cui $g$ è iniettiva e vale la disuguaglianza iniziale.
	\end{itemize}
\end{soln}

\subsection{Operazioni fra le cardinalità finite}

\begin{proposition}[Le operazioni tra cardinalità finite possono essere definite in funzione delle operazioni su $\omega$]
	\label{op_card_fin}
	Dati $m,n \in \omega$ vale che:
	\[ |m| + |n| = |m+n| \qquad |m|\cdot|n| = |m \cdot n| \qquad |m|^{|n|} = |m^n|
		\]
	ovvero, per gli elementi di $\omega$ le operazioni tra cardinalità corrispondo alla cardinalità delle operazioni tra gli elementi, già definite per ricorsione su $\omega$.
\end{proposition}

\begin{proof}
	Dimostriamo, intanto che $|m| + |1| = |s(m)|$. A sinistra abbiamo, infatti la cardinalità di $(m \times \{0\}) \cup \{(0,1)\}$\footnote{Typo del prof. Mamino sui suoi appunti in quanto $1 = \{0\}$.} e a destra abbiamo la cardinalità di $m \cup \{m\}$.
	Quest'ultimo insieme si mappa bigettivamente nel primo, mandando $x \in m$ in $(x,0)$ e $m$ in $(0,1)$. 
	Ora, le uguaglianze asserite seguono, per induzione su $n$, dalle proprietà delle operazioni sulle cardinalità e dalla definizione ricorsiva delle operazioni su $\omega$.\\
	\textcolor{purple}{$|m|+|n| = |m+n|$}
	\begin{itemize}
		\item[$\boxed{\text{caso $n = 0$}}$] $|m| + |0| =  |(m \times \{0\}) \cup \emptyset| = |m| = |m + 0|$.
		\item[$\boxed{\text{caso $n = s(a)$}}$] Per ipotesi induttiva abbiamo $|m| + |a| = |m + a|$, da cui possiamo verificare la tesi come segue:
		\[ \begin{split}
			|m| + |s(a)| \overset{\text{oss. iniziale}}{=}\quad& |m| + (|a| + |1|) \\
						 \overset{\text{propr. operaz. card.}}{=}& (|m| + |a|) + |1| \\
						 \overset{\text{Hp. indutt}}{=}\quad& |m+a| + |1| \\
						 \overset{\text{oss. iniziale}}{=}\quad& |s(m+a)| \\
						 \overset{\text{def. di $+$}}{=}\quad\;& |m + s(a)|
 		\end{split} 
			\]
	\end{itemize}
	\textcolor{purple}{$|m|\cdot|n| = |m \cdot n|$}
	\begin{itemize}
		\item[$\boxed{\text{caso $n = 0$}}$] $|m| \cdot |0| =  |m \times \emptyset| = |0| \overset{\text{def. di $\cdot$}}{=} |m \cdot 0|$.
		\item[$\boxed{\text{caso $n = s(a)$}}$] Per ipotesi induttiva abbiamo $|m|\cdot|a| = |m \cdot a|$, da cui possiamo verificare la tesi come segue:
		\[ \begin{split}
			|m| \cdot |s(a)| \overset{\text{oss. iniziale}}{=}\quad& |m| \cdot (|a| + |1|) \\
						 \overset{\text{propr. operaz. card.}}{=}& |m| \cdot |a| + \underbrace{|m| \cdot |1|}_{|m \times \{0\}| = |m|} \\
						 \overset{\text{Hp. indutt}}{=}\quad& |m \cdot a| + |m| \\
						 \overset{\text{propr. $+$ card. fin.}}{=}\quad& |m \cdot a + m| \\
						 \overset{\text{def. di $\cdot$}}{=}\quad\;& |m \cdot s(a)|
 		\end{split} 
			\]
	\end{itemize}
	\textcolor{purple}{$|m|^{|n|} = |m^n|$}
	\begin{itemize}
		\item[$\boxed{\text{caso $n = 0$}}$] $|m|^{|0|} =  |{}^{0}m| = |\{f : 0 \rightarrow m\}| = |\{\emptyset\}| =  |1| = |m^0|$ (l'unica funzione possibile dal vuoto a $m$ è $f = \emptyset$\footnote{E quindi ${}^0m = \{f : \emptyset \rightarrow m\} = \{\emptyset\} = 1$, o in alternativa si può pensare che
		$f \subseteq \emptyset \times m = \emptyset \implies f \in \ps(\emptyset) = \{\emptyset\}$ e quindi $f = \emptyset \implies {}^0m = \{f\} = \{\emptyset\} = 1$.}).
		\item[$\boxed{\text{caso $n = s(a)$}}$] Per ipotesi induttiva abbiamo $|m|^{|a|} = |m^a|$, da cui possiamo verificare la tesi come segue:
		\[ \begin{split}
			|m|^{|s(a)|} \overset{\text{oss. iniziale}}{=}\quad& |m|^{|a| + |1|} \\
						 \overset{\text{propr. operaz. card.}}{=}& |m|^{|a|} \cdot \underbrace{|m|^{|1|}}_{=|m|} \\
						 \overset{\text{Hp. indutt}}{=}\quad& |m^a| \cdot |m| \\
						 \overset{\text{propr. del $\cdot$ card.}}{=}\quad& |m^a \cdot m| \\
						 \overset{\text{def. potenza}}{=}\quad\;& |m^{s(a)}|
 		\end{split} 
			\]
		(dove $|m|^{|1|} = |m|$ perché $|{}^{1}m|  = |\{f : 1 \rightarrow m\}| = |\{\{(0,0)\},\{(0,1)\},\{(0,2)\},\ldots,\linebreak(0,m-1)\}|$, e quest'ultimo insieme è banalmente in bigezione con $m$).
	\end{itemize}
\end{proof}

\begin{note}
	Questa proposizione ci fornisce una dimostrazione delle proprietà aritmetiche elementari delle operazioni su $\omega$ [sfruttando le proprietà delle operazione fra cardinalità], alternativa a quella per induzione (che è stata lasciata per esercizio).
	Basta, infatti, applicare le corrispondenti proprietà delle operazioni sulle cardinalità\footnote{E ciò non comporta problemi di circolarità poiché nella dimostrazione della proposizione precedente abbiamo usato \textbf{solo} la definizione delle tre operazione e nessuna delle  loro proprietà.}.
\end{note}

\begin{exercise}
	Dimostra che se $m,n \in \omega$ e $m \leq n$, esista un unico $n-m \in \omega$ tale che $m + (n-m) = n$. In due modi diversi.
\end{exercise}

\begin{soln}
	
\end{soln}


\newpage
\section{La cardinalità del numerabile}

\begin{definition}[Numerabilità]
	Diciamo che $A$ è \vocab{al più numerabile} se $|A| \leq |\omega|$ ed è \vocab{numerabile} se $|A| = |\omega|$.
	Il simbolo $\aleph_0$ - aleph con zero - è semplicemente un'abbreviazione per $|\omega|$ (per cui $|A| \leq \aleph_0$ si può leggere ``$A$ è al più numerabile'' e $|A| = \aleph_0$ si 
	può leggere ``$A$ è numerabile'').
\end{definition}

\begin{remark}
	In altri termini, dire che $A$ è al più numerabile significa dire che c'è una funzione iniettiva $A \hookrightarrow \omega$. Dire che è numerabile significa dire che c'è una bigezione con $\omega$.
\end{remark}

\begin{proposition}
	Se $A$ è al più numerabile, allora o $A$ è finito o $A$ è numerabile.
\end{proposition}

\underline{Ossia}: $|A| < \aleph_0$ se e solo se $A$ è finito [non è altro che una formulazione equivalente della proposizione sopra].

Potremmo dimostrare la proposizione direttamente, ma ci conviene, invece, passare attraverso alcune considerazioni che saranno utili in seguito.\\
In generale, per costruire una bigezione fra due insiemi $A$ e $B$ - ossia per dimostrare $|A| = |B|$ - occorre appoggiarsi a qualche struttura definita sugli insiemi $A$
e $B$. Per esempio, una funzione successore. In questo corso, giocheranno un ruolo importante, in questa direzione, le relazioni d'ordine, e, in particolare - l'idea è di Cantor - i 
\vocab{buoni ordini}. Ricordiamo la definizione.

\begin{definition}[Buon ordinamento]
	Un insieme totalmente ordinato $(S,<)$ si dice \vocab{bene ordinato} se ogni suo sottoinsieme non vuoto ha un minimo.
	\[ \forall A \subseteq S \; A \ne \emptyset \rightarrow \exists m \in A \; \forall a \in A \; m \leq x
		\]
\end{definition}

Il trucco è che un isomorfismo di ordini è, in particolare, una bigezione, e spesso, per costruire bigezioni, costruiamo isomorfismi di ordini.

\begin{definition}[Isomorfismo]
	Due insiemi (parzialmente\footnote{Dove parziale indica l'assenza della proprietà di totalità nella definizione di relazione d'ordine.}) ordinati
	$(A,<_A)$ e $(B,<_B)$ sono \vocab{isomorfi}, in simboli $(A,<_A) \sim (B,<_B)$ se esiste una bigezione $f : A \rightarrow B$ tale che:
	\[ \forall x,y \in A \; x <_A y \longleftrightarrow f(x) <_B f(y)
		\]
	(cioè se esiste una bigezione che rispetta le relazioni d'ordine).
\end{definition}

\begin{remark}[Funzioni strettamente crescenti]
	Due insiemi TOTALMENTE ordinati $(A,<_A)$ e $(B,<_B)$ sono isomorfismi se e solo se esiste una funzione $f : A \rightarrow B$ \underline{surgettiva} e \vocab{strettamente crescente} - cioè tale che:
	\[ \forall x,y \in A \; x <_A y \longleftrightarrow f(x) <_B f(y)
		\]
	(non è altro che la definizione in cui supponiamo gli insiemi totalmente ordinati e diamo un nome alla funzione che realizza l'isomorfismo in questo caso).
\end{remark}

\begin{exercise}
	Dimostrare la proposizione enunciata sopra.
\end{exercise}

\begin{remark}[Ogni insieme finito è isomorfo alla sua cardinalità]
	Sia $(A,<_A)$ totalmente ordinato con $|A| = n \in \omega$. Allora $(A,<_A) \sim (n,<)$, dove $<$ denota l'ordinamento [buono\footnote{Per restrizione.}] indotto da $\omega$ (cioè l'ordine che abbiamo definito su $\omega$ ristretto a $n$).
\end{remark}

\begin{proof}
	Procediamo per induzione su $n$.
	\begin{itemize}
		\item[$\boxed{\text{caso $n = 0$}}$] $A = \emptyset$, quindi $(A,<_A) \sim (\emptyset,\emptyset)$.
		\item[$\boxed{\text{caso $n = s(m)$}}$] Se $m = 0$, allora $A = \{a\}$ e $(A,<_A) \sim (1,<)$, cioè la tesi è banalmente vera. Assumiamo quindi $m>0$. Dimostriamo intanto che $(A,<_A)$ ha un massimo elemento.
		Fissiamo una bigezione $f : s(m) \rightarrow A$ (esiste per ipotesi). Allora $|f[m]| = m$, quindi $f[m]$ con l'ordinamento indotto da $<_A$ è isomorfo a $(m,<)$ per ipotesi induttiva e, in particolare, ha massimo $M$. Ora per la totalità di $<_A$,
		o $M < f(m)$ oppure $f(m)<M$. Si verifica immediata che, nel primo caso, $f(m)$ è il massimo di $A$, e nel secondo $M$ è il massimo di $A$.\\
		Stabilito che $A$ ha un massimo $N$, osserviamo che, detto $A' := A \setminus\{N\}$, siccome $|A'| = m$, usando nuovamente l'ipotesi induttiva abbiamo un isomorfismo $f : A' \rightarrow m$ fra $A'$, con l'ordinamento indotto da $<_A$ e $(m,<)$.
		Si verifica facilmente che l'isomorfismo cercato è:
		\[ f' : A \longrightarrow s(m) : x \longmapsto \begin{cases}
			f(x) &\text{se $x \in A'$} \\
			m &\text{se $x = N$}
		\end{cases}
			\]
	\end{itemize}
\end{proof}

\begin{remark}[Ogni ordine finito totale ha massimo e minimo]
	Questa proposizione ci dice che ogni ordine totale finito è isomorfo ad un buon ordine ($n \in \omega$), dunque ogni ordine totale finito ammette sia minimo [perché $\omega$ è ben ordinato], sia massimo [come vedremo a breve nella caratterizzazione di $\omega$].
\end{remark}

Possiamo caratterizzare $\omega$ in termini delle proprietà del suo ordinamento naturale. Quelle che servono sono le seguenti.

\begin{proposition}[Proprietà di $(\omega,<)$]
	Dato $(\omega,<)$ ordine totale allora valgono le seguenti:
	\begin{enumerate}[(1)]
		\item $(\omega,<)$ è un buon ordine.
		\item $(\omega,<)$ è \vocab{illimitato} - ossia $\forall x \in \omega \; \exists y \in \omega \; x < y$.
		\item Ogni $A \subseteq \omega$ superiormente limitato e non vuoto ha un massimo, ossia:
		\[ \forall A \subseteq \omega (A \ne \emptyset \land (\exists L \in \omega \; \forall x \in A \; x \leq L)) \rightarrow (\exists M \in A \; \forall x \in A \; x \leq M)
			\]
	\end{enumerate}
\end{proposition}

\begin{proof}
	Abbiamo che (1) è il principio del minimo che abbiamo già dimostrato su $\omega$, per (2) basta prendere $y = s(x)$ (e $x \in s(x) \implies x < y$). Per (3) se $A$ è superiormente limitato da $L \in \omega$, allora $A \subseteq s(L)$, quindi $A$ è finito (perché sottoinsieme di un 
	insieme finito). Siccome $A$ è finito, l'ordinamento totale su $A$ (eredita la totalità da quello di $\omega$) definito da:
	\[ x \prec y \Mydef y < x
		\]
	è buono [perché ogni insieme finito è isomorfo al buon ordine della sua cardinalità], quindi, in particolare, c'è il minimo di $A$ (sottoinsieme improprio di se stesso) 
	secondo l'ordinamento $\prec$. Questo è il massimo di $A$ (secondo l'ordinamento $<$).
\end{proof}

\begin{proposition}[Caratterizzazione di $\omega$ come ordine]
	Sia $(A,\prec)$, con $A \ne \emptyset$, un ordinamento:
	\begin{enumerate}
		\item buono
		\item illimitato
		\item tale che ogni sottoinsieme superiormente limitato e non vuoto di $A$ ha un massimo secondo $\prec$
	\end{enumerate}
	allora $(A,\prec) \sim (\omega,<)$.\footnote{Questa proposizione completa la caratterizzazione di $(\omega,<)$ come ordine totale.}
\end{proposition}

Dimostriamo prima un facile lemma.

\begin{lemma}[Stretta crescenza col successore $\implies$ stretta crescenza]
	Sia $(A,\prec)$ un ordine, e sia $f : \omega \rightarrow A$ tale che:
	\[ \forall n \in \omega \; f(n) \prec f(s(n)) \, \footnote{Typo di Mamino nelle dispense.}
		\]
	allora $f$ è strettamente crescente, cioè $\forall m,n \in \omega \; m < n \rightarrow f(m) \prec f(n)$, e in particolare è iniettiva.
\end{lemma}

\begin{proof}
	Considero, per assurdo, $m < n$ tali che $f(m) \not \prec f(n)$, con $n$ minimo [tale per cui accade ciò]. Siccome $0 \leq m < n$, esiste $n'$ tale che $n = s(n')$.
	Ora, da un'\hyperref[succ2]{osservazione precedente}, essendo $m < s(n')$, si ha $m = n' \lor m < n'$. Nel primo caso, dall'ipotesi segue:
	\[ f(m) \prec f(s(m)) = f(s(n')) = f(n)
		\]
	contraddicendo $f(m) \not\prec f(n)$. Nel secondo caso, per la minimalità di $n$ (quindi ciò che è più piccolo di $n$ ha immagine sopra $m$), deve accadere per forza $f(m) \prec f(n')$, ma $f(n') \prec f(s(n')) = f(n)$ per ipotesi, quindi abbiamo di nuovo una contraddizione,
	pertanto deve essere necessariamente $f(m) \prec f(n)$.
\end{proof}

Possiamo ora dimostrare la proposizione.

\begin{proof}
	Costruiamo per ricorsione un isomorfismo $f$ da $(\omega, <)$ a $(A,\prec)$:
	\[ f(0) = \min_\prec A \qquad f(s(n)) = \min_\prec\{a \in A | f(n) \prec a\} \, \footnote{Cioè la funzione manda il successore nel più piccolo termine in $(A,\prec)$ che sta ``sopra'' a $f(n)$ (in pratica la stiamo costruendo apposta affinché sia strettamente crescente).}
		\]
	dove $\min_\prec$ denota il minimo secondo la relazione d'ordine (buona) $\prec$ di $A$. Occorre dimostrare intanto che $f$ è ben definita. $f(0)$ è ben definita, perché $A \ne \emptyset$, e quindi vale il principio del minimo (che abbiamo per ipotesi).
	Per dire che $f(s(n))$ è ben definita, occorre dire che la funzione $h : A \rightarrow A$, $h(x) = \min_\prec\{a \in A|x \prec a\}$ è ben definita (sarebbe la funzione che definisce la ricorsione - prima forma -), 
	ossia che $\{a \in A | x \prec a\}$ è non vuoto, e quindi di nuovo esiste il minimo usando che per ipotesi $A$ è ben ordinato. Ma questo [cioè il fatto che quell'insieme sia non vuoto] avviene, qualsiasi sia $x\in A$, perché altrimenti $A$ sarebbe limitato [superiormente] da $x$ (e non illimitato superiormente come abbiamo supposto nelle ipotesi).\\
	Per come è costruita, e per il lemma, $f$ è [strettamente] crescente (cioè l'abbiamo costruita in modo che sia una funzione da $\omega$ in $A$ crescente rispetto al successore, per cui vale il lemma sopra, dunque è sempre crescente), quindi iniettiva.
	Di conseguenza, ci basta dimostrare la surgettività.\\
	Prendiamo $y \in A$ e cerchiamo $x \in \omega$ tale che $y = f(x)$. Se, per ogni $x \in \omega$, avessi $f(x) \prec y$, allora $f[\omega]$ sarebbe [non vuoto e] superiormente limitato da $y$, tuttavia non avrebbe massimo perché ogni $f(x)$ è $\prec$ di $f(s(x))$, il che è assurdo [\underline{qui} stiamo usando che violerebbe 3.].
	Quindi c'è \textbf{il minimo $x \textcolor{red}{\in \omega}$ tale che $y \preceq f(x)$}. Dimostriamo che, per tale $x$, $f(x) \preceq y$, da cui l'uguaglianza (e quindi la surgettività).
	\begin{itemize}
		\item[$\boxed{\text{$x = 0$}}$] in tal caso $f(x)$ è il minimo di $A$, quindi $f(x) \preceq y \in A$.
		\item[$\boxed{x = s(x')}$] in questo caso $f(x') \prec y$ per la minimalità di $x$ (avendo preso $x$ come il minimo in $\omega$ tale che $f(x) \succeq y$, tutto ciò che sta sotto non può rispettare l'ultima condizione), ma allora, $y \in \{a \in A | f(x') \prec a\}$, quindi $f(x) = f(s(x')) = \min_\prec\{a \in A | f(x') \prec a\} \preceq y$ (dove l'ultima disuguaglianza deriva dal fatto che $y$
		appartiene all'insieme di cui stiamo facendo il minimo, mentre la seconda uguaglianza è la definizione di $f$).
	\end{itemize}
\end{proof}

Tornando alla proposizione iniziale.

\begin{proposition}[Caratterizzazione insiemi al più numerabili]
	Se $A$ è al più numerabile, allora o $A$ è finito o $A$ è numerabile.
\end{proposition}

\begin{proof}
	Per ipotesi esiste $f : A \rightarrow \omega$ iniettiva, per cui abbiamo $|A| = |f[A]|$, e siccome $f[A] \subseteq \omega$,
	ci basta dimostrare che dato $B \subseteq \omega$, o $B$ è finito o è numerabile.\\
	Sia $B \subseteq \omega$ infinito, dimostriamo che $B$, con l'ordinamento indotto dall'ordine naturale di $\omega$ soddisfa le ipotesi della proposizione precedente. 1 e 3 \footnote{Typo di Mamino.} valgono
	in quanto ogni sottoinsieme di $B$ è in particolare, sottoinsieme di $\omega$ (dunque abbiamo buon ordinamento ed esistenza del massimo). Per ottenere 2 dobbiamo dire che $B$
	non ha un massimo elemento (cioè è illimitato). Se, infatti, ci fosse un $M \in B$ tale che $\forall b \in B \; b \leq M$, allora avremmo che $B \subseteq s(M)$, $B$ sarebbe dunque finito [perché sottoinsieme di un insieme finito], contro l'ipotesi.
	Pertanto $(B,<_{|B}) \sim (\omega,<) \implies |B| = \aleph_0$, dunque se un sottoinsieme di $\omega$ è infinito, allora è necessariamente numerabile.\\
	Il caso di un sottoinsieme non infinito coincide col caso di un elemento di $\omega$ (che sappiamo essere un sottoinsieme per le proprietà di $\omega$), che è dunque banalmente in bigezione con se stesso (via identità) e quindi finito per definizione.
\end{proof}
\pagebreak
\begin{exercise}
	\label{ex7.13}
	Dimostra che se $|A| \leq \aleph_0$ e $f : A \twoheadrightarrow B$ è surgettiva, allora $|B| \leq \aleph_0$.
\end{exercise}

\begin{soln}
	Mostriamo che sotto queste ipotesi esiste $h : B \hookrightarrow \omega$ (iniettiva), sia $g : A \hookrightarrow \omega$ e poniamo:
	\[ h(b) = \min_<(g[\underbrace{\{a \in A | f(a) = b\}}_{= ``f^{-1}(b)''}]) \, \footnote{Si noti che, essendo $f$ non necessariamente iniettiva, $f^{-1}$ denota la controimmagine, non la funzione inversa, da cui la scelta delle parentesi quadre quando si applica $g$, per evidenziare che stiamo facendo l'immagine di un'insieme.}
		\]
	l'insieme tra graffe è non vuoto per surgettività di $f$, dunque il minimo è ben definito.
	Inoltre, se $h(b) = h(b')$, allora i minimi [che chiamiamo] $g(a)$ e $g(a')$ sono uguali, ma $a$ e $a'$ sono elementi nelle controimmagini rispettivamente di $b$ e $b'$, cioè tali che $f(a) = b$ e $f(a') = b'$.
	Sappiamo quindi per ipotesi che $g(a) = g(a')$ e per l'iniettività di $g$ segue $a = a'$, da cui $f(a) = f(a')$ (ovviamente sono lo stesso elemento), da cui $b = f(a) = f(a') = b'$.
\end{soln}

\subsection{Insiemi numerabili in pratica}
Sapere che, se $|A| \leq \aleph_0$, allora o $A$ è finito o è numerabile, ci fornisce lo strumento essendo per dimostrare la numerabilità di molti insiemi concreti. Spesso, infatti,
è facile dimostrare che un insieme infinito è tale. Rimane poi da gestire un discorso di disuguaglianze per dire che esso è al più numerabile.\\
Cominciamo quindi con qualche considerazione generale a proposito delle disuguaglianze fra cardinalità.

\begin{remark}[Compatibilità tra operazioni e ``ordinamentro'' fra cardinalità]
	\label{compatibilità_operazioni_cardinalità}
	Dati gli insiemi $A,B,C$ con $|B| \leq |C|$ allora vale:
	\begin{align*}
		|A| + |B| \leq |A| + |C| \qquad & |A|^{|B|} \leq |A|^{|C|} \\
		|A| \cdot |B| \leq |A| \cdot |C| \qquad & |B|^{|A|} \leq |C|^{|A|}
	\end{align*}
\end{remark}

Vale a dire che le operazioni sulle cardinalità sono monotone, nel senso delle disuguaglianze larghe. \underline{Attenzione però} che, in generale, NON sono strettamente monotone!

\begin{proof}
	Detta $f : B \rightarrow C$ la funzione iniettiva che testimonia che $|B| \leq |C|$ e detto $B' = f[B]$ abbiamo che $|B| = |B'|$ (come al solito per definizione di disuguaglianza
	tra cardinalità), quindi basta dimostrare le disuguaglianze asserite con $B'$ al posto di $B$\footnote{Oppure potevamo assumere WLOG che $B$ fosse proprio contenuto in $C$ e che la mappa
	fosse proprio $\id_B$, in ogni caso è solo una questione di nomi.}. Ora, giocando sul fatto che $B' \subseteq C$ (abbiamo fatto apposta lo scambio tra $B$ e $B'$ per poter usare i contenimenti),
	si vede che queste disuguaglianze rappresentano, in realtà, relazioni di contenimento fra RHS e LHS. Per esempio:
	\[ \begin{split}
		B' \subseteq C &\overset{\text{ovvio}}{\implies} (A \times \{0\}) \cup (B' \times \{1\}) \subseteq (A \times \{0\}) \cup (C \times \{1\}) \; \textcolor{red}{= A \sqcup B' \subseteq A \sqcup C}\\
					   &\overset{\id_A \times \id_{B'}}{\implies} |(A \times \{0\}) \cup (B' \times \{1\})| \leq |(A \times \{0\}) \cup (C \times \{1\})| \\
					   &\overset{\text{def.}}{\iff}|A| + |B'| \leq |A| + |C|
	\end{split}
		\]
	Le altre si ottengono allo stesso modo.
\end{proof}

\begin{remark}[Disuguaglianza di inclusione-esclusione]
	$|A \cup B| \leq |A| + |B|$.
\end{remark}

\begin{proof}
	Basta osservare che la seguente funzione è iniettiva:
	\[ f : A \cup B \longrightarrow (A \times \{0\}) \cup (B \times \{1\}) : x \longmapsto \begin{cases}
		(x,0) &\text{se $x \in A$} \\
		(x,1) &\text{altrimenti}\,\footnote{Cioè se $x \in B\setminus(A \cap B)$.}
	\end{cases}
		\]
\end{proof}

Veniamo roa a calcolare le operazioni aritmetiche. Già sappiamo, per il \hyperref[cantor]{teorema di cantor}, 
che $2^{\aleph_0} > \aleph_0$, per cui mettere un $\aleph_0$ a esponente di qualunque cosa non sia uno 0 o un 1 conduce fuori dal numerabile.
Tutto il resto invece no.

\begin{proposition}[Operazioni aritmetiche con $\aleph_0$]
	$\aleph_0 + \aleph_0 = \aleph_0 \cdot \aleph_0 = \aleph_0^{n} = \aleph_0$, con $n \in \omega\setminus\{0\}$.
\end{proposition}

\begin{proof}
	Supponiamo di sapere già che $\aleph_0 \cdot \aleph_0 = \aleph_0$, allora possiamo formare la catena di disuguaglianze:
	\[ \aleph_0 \overset{\text{op. card.}}{=} \aleph_0 + 0 \overset{\text{oss. sopra}}{\leq} \aleph_0 + \aleph_0 \overset{\text{op. card.}}{=} \aleph_0 \cdot 2 \overset{\text{oss. sopra}}{\leq} \aleph_0 \cdot \aleph_0 \overset{\text{ipotesi}}{=} \aleph_0
		\]
	Da cui per il \hyperref[CB]{Cantor-Bernstein}:
	\[ \aleph_0 + \aleph_0 = \aleph_0 \cdot \aleph_0 = \aleph_0
		\]
	Ora è facile vedere per induzione che $n \in \omega \setminus\{0\} \rightarrow \aleph_0^n = \aleph_0 $, infatti $\aleph_0^1 = \aleph_0$ [e $\aleph_0^2 = \aleph_0 \cdot \aleph_0 = \aleph_0$], quindi $\aleph_0^{n+1} = \aleph_0^n \cdot \aleph_0 \overset{\text{Hp. indutt.}}{=} \aleph_0 \cdot \aleph_ 0 = \aleph_0$.
\end{proof}

Per concludere la dimostrazione precedente, resta da dimostrare il lemma seguente.

\subsection{Prodotto di numerabili è numerabile}

\begin{lemma}[$\aleph_0 \cdot \aleph_0 = \aleph_0$]
	$\aleph_0 \cdot \aleph_0 = \aleph_0$, ossia esiste una bigezione fra $\omega \times \omega$ e $\omega$.
\end{lemma}

\begin{wrapfigure}[15]{r}{2.82cm}
	\includegraphics[width=2.82cm]{immagini/aleph2.png}
\end{wrapfigure}
Ci sono diverse vie per illustrare questo risultato. Per esempio, possiamo rappresentare le coppie $(x,y) \in \omega \times \omega$ sotto la specie di una griglia a maglie quadrate.
Poi disegnare un percorso che pare visitare tutte le maglie della griglia, con sufficiente apparenza di regolarità, possibilmente, da convincere il lettore che vi debba essere un metodo.
Infine numeriamo le maglie secondo l'ordine in cui sono visitate dal percorso. Avremo così numerato tutte le coppie di numeri naturali del disegno.\\
Altrimenti, è possiamo esibire delle bigezioni esplicite, per esempio:
\[ f(x,y) = 2^x \cdot (2y + 1) - 1 \qquad g(x,y) = \frac{(x+y)^2 + 3x + y}{2}
	\]
È possibile scrivere i due numeri della coppia in base 10 a cifre alternate, tipo: $(\textcolor{blue}{64},\textcolor{red}{4096}) \mapsto \textcolor{red}{4}\textcolor{cyan}{0}\textcolor{red}{0}\textcolor{red}{9}\textcolor{cyan}{0}\textcolor{blue}{6}\textcolor{red}{6}\textcolor{red}{4}\textcolor{blue}{4}$.\\


\newpage
\pagebreak
\hspace{-0.43cm}\emph{Dimostrazione.}\hspace{-0.70cm} Consideriamo l'ordinamento su $\omega \times \omega$ definito come segue:
	\begin{wrapfigure}[9]{l}{2.5cm}
		\includegraphics[width=2.5cm]{immagini/ordine_omega.png}
	\end{wrapfigure}
	\[ \begin{split}
		(a,b) \prec (a',b') &\Mydef \max(a,b) < \max(a',b') \\
							&\lor (\max(a,b) = \max(a',b') \land a < a') \\
							&\lor (\max(a,b) = \max(a',b') \land a = a' \land b < b')
	\end{split}
		\]
	(dove per max sulla coppia si intende il max tra $a$ e $b$) ossia per confrontare $(a,b)$ con $(a',b')$, si confrontano prima $\max(a,b)$ e $\max(a',b')$; a parità si confrontano $a$ ed $a'$ (cioè se hanno una delle due componenti con lo stesso modulo massimo, si passa a confrontare il valore delle prime componenti);
	se queste coincidono, allora si confrontano $b$ e $b'$.\footnote{Come si vede nella figura a lato, nel primo caso, avendo $b$ modulo massimo, ci sono anche punti più a destra, che in quest'ordinamento sono più piccoli (perché hanno un valore più piccolo come massima componente).}\\
	\textcolor{MidnightBlue}{L'idea è che, in questo modo, le coppie $\prec$ di una certa $(a,b)$ fissata sono tutte contenute nel quadrato $\{0,\ldots,\max(a,b)\} \times \{0,\ldots,\max(a,b)\}$, quindi sono in numero finito, e questo 
	implica che $(\omega \times \omega, \prec)$ è isomorfo $(\omega,<)$.}\\
	Formalmente, iniziamo col verificare che $\prec$ sia effettivamente un ordine stretto e totale. La proprietà irriflessiva è immediata (perché in tutti gli OR nella definizione stiamo usando l'ordinamento stretto di $\omega$, dunque
	$\neg (a,b) \prec (a,b)$). Per verificare la proprietà transitiva, prendiamo $(a,b) \prec (a',b) \prec (a'',b'')$ (vorremo vedere che questo implica $(a,b) \prec (a'',b'')$). Dalle disuguaglianze precedenti segue $\max(a,b) \leq \max(a',b') \leq \max(a'',b'')$. Se una di queste disuguaglianze è stretta allora $(a,b) \prec (a'',b'')$ (e avremmo concluso),
	altrimenti $\max(a,b) = \max(a',b') = \max(a'',b'')$, segue dalla definizione che $a \leq a' \leq a''$.
	Nuovamente, se una disuguaglianza è stretta abbiamo concluso, altrimenti $a = a' = a''$, quindi, affinché la scrittura iniziale sia ancora vera deve essere necessariamente che $b < b' < b''$, da cui $b < b''$, e quindi anche in questo caso vale $(a,b) \prec (a'',b'')$.
	Per dire che l'ordine è totale osserviamo che se $(a,b)$ e $(a',b')$ non sono né $\prec$ ne $\succ$ allora dobbiamo avere $\max(a,b) = \max(a',b')$, $a = a'$, $b = b'$, ovvero $(a,b) = (a',b')$, dunque l'ordine stretto è anche totale.\\
	Ora vogliamo dire che $(\omega \times \omega, \prec) \sim (\omega, <)$ (in questo modo, avendo un'isomorfismo di ordini, avremmo in particolare una bigezione tra $\omega$ e $\omega \times \omega$, dunque il prodotto di cardinalità numerabili è numerabile).
	Partiamo dall'osservazione che se $(a,b) \in \omega \times \omega$ allora possiamo definire:
	\[ (\omega \times \omega)_{(a,b)} \Mydef \{(x,y) \in \omega \times \omega | (x,y) \prec (a,b)\}
		\]
	detto il ``\vocab{segmento iniziale} determinato da $(a,b)$ su $(\omega \times \omega,\prec)$''. Tale segmento iniziale è finito, infatti $(\omega \times \omega)_{(a,b)} \subseteq s(\max(a,b)) \times s(\max(a,b))$ (il RHS è un insieme finito e quindi tutti i suoi sottoinsiemi 
	sono finiti).\\
	Ci serve dire: \textcolor{red}{1.} $(\omega \times \omega, \prec)$ è bene ordinato \textcolor{red}{2.} $(\omega \times \omega, \prec)$ è illimitato \textcolor{red}{3.} ogni sottoinsieme non vuoto e superiormente limitato di $\omega \times \omega$ ha un massimo.
	\begin{enumerate}[1.]
		\item Dato $A \subseteq \omega \times \omega$ con $A \ne \emptyset$, considero $a \in A$. Se $(\omega \times \omega)_a \cap A = \emptyset$ (stiamo considerando il segmento iniziale rispetto a un generico elemento $a\in\omega \times \omega$), allora $a$ è il minimo di $A$ (sta in $a$ e
		non c'è nulla più piccolo nell'insieme perché l'intersezione col segmento iniziale di $a$ (= cose strettamente più piccole in $(\omega \times \omega,\prec)$) è vuota). Altrimenti $A' = (\omega \times \omega)_{(a,a)} \cap A$ è non vuoto e finito [perché sto intersecando con un insieme finito],
		quindi ha minimo $m$ (perché $\prec_{|A}$ è un ordine totale).\\ Questo deve essere anche il minimo di $A$, perché se $x \in A \setminus A'$, con $(A \setminus A') \cap (\omega \times \omega)_a = \emptyset$, allora $m \prec a \preceq x$ (dove la seconda disuguaglianza segue esattamente per il caso dell'intersezione vuota,
		mentre la prima disuguaglianza perché $m, a \in (\omega \times \omega)_a \cap A$, e quindi $m \prec_{|A} a$ per come è definito).
		\item Dato $(a,b) \in \omega \times \omega$, $(a,b) \prec (s(a),s(b))$, dunque $\omega \times \omega$ è illimitato.
		\item Dato $A \subseteq \omega \times \omega$ non vuoto e superiormente limitato da $(a,b) \in \omega \times \omega$, abbiamo che $A \subseteq (\omega \times \omega)_{(a+1,b+1)}$ è finito (per quanto osservato sopra), quindi ammette massimo perché $\prec$ è totale (abbiamo un numero finito di elementi da confrontare).
	\end{enumerate}
$\hfill\square$

\subsection{Numeri interi e razionali}
Usando la proposizione appena dimostrata, potremmo dimostrare, per esempio, che $\ZZ$ e $\QQ$ sono numerabili, se non fosse che non abbiamo ancora definito questi oggetti. Allo scopo, ricordiamo che - \hyperref[3.73]{esercizio 3.73} - una relazione di equivalenza
induce un insieme di classi di equivalenza.

\begin{definition}[$\ZZ$]
	Definiamo $\ZZ$ come l'insieme delle classi di equivalenza su $\omega \times \omega$ indotte dalla relazione:
	\[ (a,b) \sim_\ZZ (a',b') \Mydef a + b' = b + a' \, \footnote{Morale: ``$(a,b) = a - b$''.}
		\]
\end{definition}

\begin{exercise}
	Dimostrare che $\sim_\ZZ$ è una relazione di equivalenza.
\end{exercise}

\begin{example}[Operazioni su $\ZZ$]
	Definiamo $+,-,\cdot$ su $\ZZ$ mediante:
	\begin{align*}
		[(a,b)]_\ZZ + [(a',b')]_\ZZ &\Mydef [(a+a',b+b')]_\ZZ \\
		-[(a,b)]_\ZZ &\Mydef [(b,a)]_\ZZ \\
		[(a,b)]_\ZZ \cdot [(a',b')]_\ZZ &\Mydef [(a\cdot a' + b \cdot b', a \cdot b' + a' \cdot b)]_{\ZZ}
	\end{align*}
	dimostra che $\ZZ$, con queste operazioni, è un anello commutativo con identità: $1 \Mydef [(1,0)]_\ZZ$. 
\end{example}

\begin{definition}[$\QQ$]
	Definiamo $\QQ$ come l'insieme delle classi di equivalenza su $\ZZ \times (\omega\setminus\{0\})$ indotte dalla relazione:
	\[ (n,d) \sim_\QQ (n',d') \Mydef n \cdot d' = n' \cdot d \, \footnote{Morale: ``$(n,d) = \frac nd$''.}
		\]
\end{definition}

\begin{exercise}
	Dimostrare che $\sim_\QQ$ è una relazione di equivalenza.
\end{exercise}

\begin{exercise}[Operazioni su $\QQ$]
	Definisci $+,-,\cdot$ e $\square^{-1}$ su $\QQ$ nella maniera ragionevole e dimostra che $\QQ$ è un campo.
\end{exercise}

\begin{exercise}[Ordinamento su $\QQ$]
	Definisci la relazione $<$ su $\QQ \times \QQ$ dicendo che $q \in \QQ$ è positivo se $q = [(n,d)]_\QQ$, con $n,d \in \omega\setminus\{0\}$, e dicendo che $a < b$ se e solo se $b - a$ è positivo.
	Dimostra che questo è un ordine totale e \vocab{denso}, cioè:
	\[ \forall a,b \in \QQ \; a < b \rightarrow \exists c \in \QQ \; a< c <b \, \footnote{Typo di Mamino.}
		\]
\end{exercise}

\begin{note}
	Gli esercizi precedenti sono tediosi, ma non sono difficili. Nel resto del corso daremo per scontate le proprietà aritmetiche elementari di $\ZZ$ e $\QQ$.
	D'ora innanzi scriveremo:
	\[ a - b \Mydef [(a,b)]_{\ZZ} \qquad \frac{n}{d} \Mydef [(n,d)]_\QQ
		\]
\end{note}

Per dimostrare la numerabilità di $\ZZ$ e $\QQ$, è comodo richiamare ancora un \hyperref[ex7.13]{esercizio}, però, questa volta, lo risolviamo\footnote{La soluzione riportata è quella di Mamino.}.

\begin{corollary}[Definizione di al più numerabile al contrario]
	\label{disugcardnum}
	Un insieme $A \ne \emptyset$ è al più numerabile se e solo se esiste $f : \omega \rightarrow A$ surgettiva.\footnote{Formalmente da questo momento in poi, avere una
	funzione surgettiva da un insieme al più numerabile (e nulla di più per ora) ad un altro, ci permette di dire che la cardinalità del primo è $\geq$ cardinalità del secondo (cosa che fin'ora non potevamo dire).}
\end{corollary}

\begin{proof}
	La freccia $\Longleftarrow$ deriva dall'esercizio citato prima con $A = \omega$ (l'insieme al più numerabile) e $B = A$ (l'insieme  a cui arriva la mappa surgettiva)\footnote{Quelli al LHS sono quelli nell'enunciato dell'esercizio,
	quelli al RHS sono quelli presi dalle ipotesi del corollario.}.\\
	Per l'inverso, supponiamo $A$ al più numerabile e mostriamo che c'è sempre una mappa surgettiva tra $\omega$ ed $A$. Abbiamo dimostrato che se un insieme è al più numerabile, o è finito o è numerabile, se $|A| = \aleph_0$ allora c'è $f$
	bigettiva (e quindi in particolare surgettiva), se $|A| < \aleph_0$ allora c'è [per definizione] $g : n \rightarrow A$ bigettiva per qualche $n \in \omega\setminus\{0\}$, da questa definiamo:
	\[ f(x) = \begin{cases}
		g(x) &\text{se $x < n$} \\
		g(0) &\text{altrimenti}
	\end{cases}
		\]
	come mappa surgettiva da $\omega$ in $A$ (cioè estendiamo la funzione che già c'è con $n$ a tutti i naturali maggiori o uguali ponendola come g(0)).
\end{proof}
\pagebreak
\begin{notation}[Successione]
	Con \vocab{successione} (numerabile) intendiamo semplicemente una funzione con dominio $\omega$, per cui:
	\[ \alpha = \{\alpha_i\}_{i \in \omega} \Mydef \alpha : \omega \longrightarrow \ldots: i \longmapsto \alpha_i \, \footnote{Stiamo abbreviando la successione elencando direttamente i suoi elementi indicizzati.}
		\]
	una \vocab{enumerazione}\footnote{Moralmente: una successione surgettiva.} di $A$ è una successione $\alpha = \{a_i\}_{i \in \omega}$ tale che $A = \Imm(\alpha)$ (come nella notazione sopra $\alpha$ è la successione che associa ai naturali gli elementi dell'insieme,
	ed è \underline{surgettiva}, affinché $\Imm(\alpha) = A$), ossia, informalmente, $A = \{a_i | i \in \omega\}$.
\end{notation}

Il corollario sopra, quindi, non ci dice altro che $A \ne \emptyset$ è al più numerabile se e solo se ha almeno un'enumerazione.

\begin{example}[L'insieme dei numeri interi è numerabile]
	$\ZZ$ è numerabile.
\end{example}

\begin{proof}
	La funzione $\omega \times \omega : (a,b) \mapsto a - b$ è surgettiva per definizione (è la proiezione al quoziente di $\omega \times \omega$ modulo $\sim_\ZZ$, che sappiamo essere sempre surgettiva, in questo caso stiamo indicando le classi $[(a,b)]_\ZZ$ con $a - b$, ma sono sempre classi di equivalenza), e $\omega \times \omega$ è numerabile\footnote{$|\omega \times \omega| = \aleph_0 \cdot \aleph_0 = \aleph_0$.} dunque $|\ZZ|\leq \aleph_0$.\\
	D'altro canto, la funzione $\omega \rightarrow \ZZ : n \mapsto [(n,0)]_\ZZ$ è iniettiva, infatti $[(n,0)]_\ZZ = [(m,0)]_\ZZ \iff (n,0) \sim (m,0) \iff n = m$ (per definizione di $\sim_\ZZ$), dunque $\aleph_0 \leq |\ZZ|$, pertanto [per \hyperref[CB]{Cantor-Bernstein}] $|\ZZ| = \aleph_0$.
\end{proof}

\begin{example}[L'insieme dei numeri razionali è numerabile]
	$\QQ$ è numerabile.
\end{example}

\begin{proof}
	Come nell'esempio precedente, la proiezione al quoziente $\ZZ \times (\omega \setminus\{0\}) \rightarrow \QQ : (n,d) \mapsto \frac{n}{d}$ (dove la frazione è un'abbreviazione per la classe di equivalenza $[(n,d)]_\QQ$), è surgettiva per costruzione, inoltre $|\ZZ \times (\omega \setminus\{0\})| = |\ZZ| \cdot |\omega \setminus\{0\}| = \aleph_0 \cdot \aleph_0 = \aleph_0$, dunque vale il 
	\hyperref[disugcardnum]{corollario} sulla disuguaglianza tra cardinalità, pertanto $\aleph_0 \geq |\QQ|$.\\
	Viceversa, la funzione $\omega \rightarrow \QQ : n \longmapsto \frac{n}{1}$ è iniettiva, infatti $\frac n1 = \frac m1 \iff n \cdot 1 = m \cdot 1 \iff m = n$, dunque per definizione si ha $\aleph_0 \leq |\QQ|$. Da cui per \hyperref[CB]{Cantor-Bernstein} $|\QQ| = \aleph_0$.
\end{proof}

Adesso, ci piacerebbe poter dire che, se abbiamo un insieme $A$ al più numerabile, e tutti i suoi elementi sono, a loro volta, insiemi al più numerabili, allora
$\bigcup A$ è al più numerabile. D'altro canto è ragionevole: se esiste una enumerazione $\{a_i\}_{i \in \omega}$ di $A$ (= $A$ è al più numerabile), e, per ogni $i \in \omega$, esista una enumerazione $\alpha_i = \{a_{i,j}\}_{j \in \omega}$ (= per ogni elemento $a_i \in A$ esiste 
una enumerazione, dunque ogni elemento (= insieme) è a sua volta al più numerabile)
di $a_i$, allora possiamo mandare surgettivamente [cioè enumerare] $\omega \times \omega$ in $\bigcup A$: $(i,j) \mapsto \alpha_{i,j}$ (in questo modo abbiamo un'enumerazione degli elementi degli elementi, e quindi l'unione di $A$ è al più numerabile), e, siccome $\omega \times \omega$ è al più numerabile, lo è anche $A$ (per il solito \hyperref[disugcardnum]{corollario}).\\
L'\textcolor{red}{errore} è credere di poter fissare una $\alpha_i$ per ogni $i \in \omega$. Usando l'assioma della scelta potremo farlo, ma, per ora, non abbiamo modo, in generale, di procurarci la funzione $i \mapsto \alpha_i$ (cioè la funzione che sceglie in quale enumerazione mandare 
ogni $i \in \omega$). Possiamo però 
assumere di averla, così si corregge il ragionamento impreciso di prima.

\begin{proposition}[$|A| \leq \aleph_0 \implies |\bigcup A| \leq \aleph_0$]
	Sia $A = \{a_i \in A| i \in \omega\}$ e \textcolor{red}{sia $\{\alpha_i\}_{i \in \omega}$ una successione di funzioni}\footnote{Come prima stiamo supponendo di averle già, altrimenti ci vuole scelta per procurarci la famiglia numerabile di enumerazioni, con tale assioma la parte in rosso di questo enunciato può essere rimossa.} tali che,
	per ogni $i \in \omega$, $\alpha_i : \omega \rightarrow a_i$ è una enumerazione di $a_i$\footnote{Cioè è una famiglia di enumerazioni degli elementi dell'$i$-esimo elemento (ciò ci dice anche che gli elementi di $A$ sono a loro volta AL PIÙ numerabili).}. Allora $|\bigcup A| \leq \aleph_0$.
\end{proposition}

\begin{proof}
	Basta osservare che la funzione:
	\[ f : \omega \times \omega \longrightarrow \bigcup A : (i,j) \longmapsto \alpha_i(j)
		\]
	è surgettiva e vale quindi il solito \hyperref[disugcardnum]{corollario}.
\end{proof}

\begin{notation}
	Data una funzione $f : I \rightarrow S$ definiamo:
	\[ \bigcup_{i \in I} f(i) \Mydef \bigcup f[I]
		\]
	Così, per esempio, se $A = \{a_i | i \in \omega\}$ (cioè sto enumerando gli elementi di $A$):
	\[ \bigcup_{i \in \omega} a_i = \bigcup A = \{x | \exists i \in \omega \; x \in a_i\}
		\]
	(cioè gli elementi degli elementi di tutti gli elementi $a_i$ sono la stessa cosa che prendere gli elementi degli dell'unione di $A$, cioè l'immagine dell'enumerazione data per come è definito).
\end{notation}

\begin{definition}
	[Parti finite]
	Definiamo le \vocab{parti finite} di un insieme $A$ come:
	\[ \psf(A) \Mydef \{X \in \ps(A) | |X| < \aleph_0\}
		\]
\end{definition}

\begin{proposition}[Insieme al più numerabile $\implies$ parti finite al più numerabile]
	$|A| \leq \aleph_0 \rightarrow |\psf(A)| \leq \aleph_0$.
\end{proposition}

\hspace{-0.5cm}\emph{Dimostrazione}. Per induzione, il caso $A = \emptyset$ è immediato. Assumiamo $A \ne \emptyset$, sia:
	\[ \ps^{\leq n} = \{X \in \ps(A) | |X| \leq n\}
		\]
	siccome $\psf(A) = \bigcup_{n \in \omega} \ps^{\leq n}(A)$\footnote{Ricordiamo che $\bigcup_{n \in \omega} \ps^{\leq n}(A) = \bigcup\{\ps^{\leq n}(A) | n \in \omega\}$, dunque stiamo facendo l'unione di un insieme numerabile.}, basta esibire una successione di enumerazione $\alpha_n$ di $\ps^{\leq n}(A)$
	(cioè una mappa surgettiva da $\omega$ a $\ps^{\leq n}(A)$, in modo da poter usare il \hyperref[disugcardnum]{corollario} ed ottenere che $\ps^{\leq n}(A)$ è al più numerabile,
	da cui, per la proposizione precedente l'unione è al più numerabile).
	Fissiamo $f : \omega \rightarrow \omega \times A : x \mapsto (f_1(x),f_2(x))$ surgettiva, che esiste perché $A$ è al più numerabile [quindi anche $\omega \times A$ lo è] (per il \hyperref[disugcardnum]{corollario} l'avere una funzione surgettiva da $\omega$
	ad un altro insieme è un fatto equivalente al fatto che il secondo insieme sia al più numerabile).\\
	Costruiamo una enumerazione\footnote{In questo caso è una successione di enumerazioni, cioè una successione di funzioni surgettive.} $\{\alpha_n\}_{n \in \omega}$ di $\ps^{\leq n}(A)$, cioè, data la famiglia numerabile $\{\ps^{\leq n}(A)\}_{n \in \omega}$,
	costruiamo una successione $\{\alpha_n\}_{n \in \omega}$ di successioni surgettive della prima famiglia (in modo da enumerare tutti gli elementi degli elementi e poter dire che la famiglia numerabile all'inizio è fatta da elementi al più numerabili, in questo modo siamo nelle ipotesi del lemma
	dell'unione visto prima).\\
	Costruire una famiglia numerabile di enumerazioni è equivalente al costruire una successione di successioni surgettive, e, come ogni successione, la si può costruire per ricorsione numerabile - prima forma -:
	\begin{itemize}
		\item[$\boxed{\text{Per $n = 0$}}$] poniamo $\ps^{=0}(A) = \{\emptyset\}$, dunque $\alpha_0$ è la costante [funzione vuota] $\emptyset$ (cioè la successione $\alpha_0$ che enumera $\ps^{=0}(A) = \{\emptyset\}$, è la funzione vuota).
		\item[$\boxed{\text{Per $n = s(m)$}}$] in questo caso dobbiamo definire un'enumerazione per $\ps^{\leq s(m)}$, dando per nota un'enumerazione $\alpha_m$ per $\ps^{\leq m}(A)$, e ciò lo possiamo fare definendo $\alpha_{s(m)}$ ricorsivamente come segue:
		\[ 	\alpha_{s(m)} : \omega \rightarrow \ps^{\leq s(m)} : x \mapsto
			\alpha_{s(m)}(x) = \begin{cases}
			\emptyset &\text{se $x = 0$} \\
			\alpha_m(f_1(x-1)) \cup \{f_2(x - 1)\} &\text{se $x > 0$}
		\end{cases}
			\]
		Stiamo di fatto partendo dal vuoto e aggiungendo in ogni passaggio un elemento di $A$ dato da $f_2$ (viceversa stiamo ``tornando indietro ricorsivamente'' tramite $f_1$, che rimanda indietro $x-1 \in \omega$).\\
		Vogliamo ora dimostrare per induzione che, per ogni $n \in \omega$, $\alpha_n : \omega \rightarrow \ps^{\leq n} (A)$ è surgettiva\footnote{In questo modo abbiamo enumerato gli elementi degli elementi, e in realtà abbiamo anche
		già enumerati gli elementi $\ps^{\leq i}(A)$, perché lo abbiamo detto all'inizio (formalmente è proprio per costruzione delle parti finite che i $\ps^{\leq i}(A)$ sono numerabili).}, cioè che la nostra successione di successioni, è in particolare una successione di enumerazioni:
		\begin{itemize}
			\item[$\boxed{\text{caso $n = 0$}}$] la successione vuota $\alpha_0$ è banalmente surgettiva.
			\item[$\boxed{\text{caso $n = s(m)$}}$] per ipotesi induttiva la successione $\alpha_m : \omega \rightarrow \ps^{\leq m}(A)$ è surgettiva. Dato $Y \in \ps^{\leq s(m)}(A)$ si danno due casi.
			Se $Y = \emptyset$, allora $Y = \alpha_{s(m)}(0) = \emptyset$. Oppure esiste almeno un elemento $y \in Y$.\\
			In questo caso $|Y\setminus\{y\}| \leq m$, quindi vale l'ipotesi induttiva e $Y \setminus\{y\} = \alpha_m(t)$ per qualche $t \in \omega$ (cioè $\alpha_m$ è surgettiva, quindi $Y$ è immagine di qualche $t \in \omega$).
			Per la surgettività di $f$, la funzione surgettiva da $\omega$ a $\omega \times A$, la coppia $(t,y)$ è uguale a $f(x)$ per qualche $x \in \omega$, cioè $f(x) = (f_1(x),f_2(x)) = (t,y)$.
			Quindi si ha proprio che $x+1$ dà $Y$:
			\[ \begin{split}
				\alpha_{s(m)}(x+1) \overset{\text{def.}}{=} &\alpha_m(f_1(x)) \cup \{f_2(x)\} \\
								   \overset{\text{$f(x) = (t,y)$}}{=} &\alpha_m(t) \cup \{y\} \\
								   \overset{\text{Hp. indutt.}}{=} &(Y \setminus\{y\}) \cup \{y\} = Y
			\end{split}
				\]
			(di fatto, fatto il caso $Y = \emptyset$, facciamo in modo di poter sempre tornare indietro a $\alpha_0$, da $\alpha_{s(m)}$ e aggiungere ricorsivamente tutti gli elementi a $Y$ a partire dal vuoto).\hfill$\square$
		\end{itemize}
	\end{itemize}

\underline{\textbf{Applicazione}} Dimostriamo che l'insieme dei numeri reali algebrici $\A_R$\footnote{Sarebbe $\overline{\QQ} \cap \RR$.} è numerabile. Per questa applicazione, assumiamo le proprietà elementari di $\RR$.
L'insieme $\A_R$ è definito come l'insieme degli $x \in \RR$ che sono zeri di qualche polinomio a coefficienti razionali:
\[ \A_R \Mydef \{x \in \RR | \exists p(x) \in \QQ[x]\setminus\{0\} \; p(x) = 0\}
	\]
I numeri reali che non sono algebrici si dicono \vocab{trascendenti} (= $\RR\setminus(\overline{\QQ} \cap \RR)$), siccome - formalmente, vedremo questo risultato in seguito - $\RR$ non è numerabile, deduciamo dalla numerabilità di
$\A_R$ che ci sono numeri reali trascendenti.\\
Dimostriamo, intanto, che l'insieme $\QQ[x]$, dei polinomi a coefficienti razionali nella indeterminata $x$, è numerabile.\\
Possiamo identificare un polinomio:
\[ p(x) = a_0 + a_1x + a_2x^2 + \ldots + a_d x^d
	\]
con l'insieme dei suoi monomi:
\[ p(x) = \{a_0,a_1x,a_2x^2,\ldots,a_dx^d\}
	\]
e ciascun monomio con la coppia (grado, coefficiente):
\[ p(x) = \{(0,a_0),(1,a_1),\ldots,(d,a_d)\}\,\footnote{Può essere pensata come funzione da $d$ in $\QQ$.}
	\]
Formalmente, come accade per i numeri, le coppie ordinate, le funzioni, etc., anche i polinomi non sono oggetti atomici della teoria degli insiemi: occorre, in qualche modo, fissare una codifica.
Quella appena descritta è una codifica ragionevole. Rappresentando i polinomi in questo modo:
\[ \QQ[x] \subseteq \psf(\omega \times \QQ) \, \footnote{Cioè, abbiamo visto che un polinomio può essere pensato come una funzione da un qualche elemento di $\omega$ (= anche sottoinsieme) a $\QQ$, in particolare ogni elemento di $\omega$ né è un sottoinsieme finito,
quindi tutti i polinomi a coefficienti in $\QQ$ saranno funzioni da un sottoinsieme (in particolare funzioni) finito di $\omega$ a $\QQ$, e ricordando, come visto in un esercizio che l'immagine di un insieme finito è finita, abbiamo che i polinomi, viste come funzioni di questo tipo, sono sottoinsiemi finiti di $\omega \times \QQ$,
pertanto l'insieme dei polinomi $\QQ[x]$ è contenuto nelle parti finite di $\omega \times \QQ$.}
	\]
per cui, essendo che $|\omega \times \QQ| = \aleph_0 \implies |\psf(\omega \times \QQ)| = \aleph_0$, e che $\QQ[x]$ si immerge in quest'ultimo insieme (ad esempio con $\id_{\QQ[x]}$), si ha $|\QQ[x]| \leq \aleph_0$.
Inoltre è elementare che $\QQ \hookrightarrow \QQ[x]$ (ad esempio $q \mapsto \{(0,q)\}$ è una mappa iniettiva che dà tutti i polinomi di grado 0), in tal modo si ha anche l'altra disuguaglianza di 
cardinalità e quindi [come al solito per \hyperref[CB]{Cantor-Bernstein}] $|\QQ[x]| = \aleph_0$. Venendo ad $\A_R$ abbiamo una facile surgezione:
\begin{multline*}
	f : (\QQ[x]\setminus\{0\}) \times \omega \longrightarrow \A_R :\\
	 (p,i) \longmapsto \text{``la $i$-esima radice di $p$ se questa esiste, altrimenti 0''}
\end{multline*}
Vediamo, però, in maggior dettaglio come si può rappresentare $f$ mediante una formula insiemistica.
\[
	\text{``$\alpha$ è la $i$-esima radice di $p$''} \equiv
	p(\alpha) = 0 \land |\{x \in \RR | x \leq\footnote{Nell'ordine di $\RR$ che prima non avevamo.} \alpha \land p(x) = 0\}| = |i|
\]
\begin{multline*}
	y = f(p,i) \equiv \text{``$y$ è la $i$-esima radice di $p$''} \\
	\land(y = 0 \land \neg \exists \alpha \in \RR \; \text{``$\alpha$ è la $i$-esima radice di $p$''})
\end{multline*}
Per separazione esiste, quindi, $f$, e, di conseguenza $|\A_R| \leq \aleph_0$. La disuguaglianza opposta è immediata perché $\QQ \subseteq \A_R$ (è facile scrivere un polinomio in $\QQ[x]$ che abbia come radice un qualsiasi $q \in \QQ$ fissato).

\begin{exercise}
	Dato un insieme $X$, una funzione $f : X^2 \rightarrow X$, e un sottoinsieme $A \subseteq X$ al più numerabile, dimostra che esiste un $\ol A \subseteq X$ al più numerabile tale che 
	$f[\ol A \times \ol A] \subseteq \ol A$. Concludi che un gruppo finitamente generato è al più numerabile.
\end{exercise}

\begin{soln}
	
\end{soln}

\subsection{Ordini densi numerabili}
Il prossimo risultato che vedremo è, come al solito, dovuto a Cantor, e caratterizza l'ordine di $\QQ$ a meno di isomorfismi.

\begin{definition}[Densità]
	Sia $(A,<)$ totalmente ordinato, e $B \subseteq A$. $B$ è \vocab{denso in} $(A,<)$ se:
	\[ \forall x,y \in A \; x < y \rightarrow \exists z \in B \; x < z < y
		\]
	(cioè tra due elementi di $A$ c'è sempre un elemento di $B$). $(A, <)$ è \vocab{denso}, cioè è denso in se stesso, se:
	\[ \forall x,y \in A \; x < y \rightarrow \exists z \in A \; x < z < y
		\]
		(cioè tra due elementi di $A$ c'è sempre qualche elemento di $A$).
\end{definition}

\begin{example}[$(\QQ,<)$ è denso in se stesso]
	Abbiamo già osservato, in un esercizio, che $\QQ$ è denso, infatti:
	\[ x < y \rightarrow x < \frac{x+y}{2} < y
		\]
	cioè presi due qualsiasi elementi di $\QQ$, la loro media aritmetica è sempre in mezzo e sta in $\QQ$ (formalmente le due disuguaglianze si giustificano con le operazioni di $\QQ$ + l'ordinamento totale + le proprietà 
	di compatibilità tra operazioni e ordinamento).
\end{example}

\begin{notexample}[$(\omega,<)$ non è denso in se stesso]
	L'insieme $\omega$ con il suo ordinamento naturale non è denso, perché $\not\exists z \in \omega \; 0 < z < 1$.
\end{notexample}

\begin{theorem}[Teorema di isomorfismo di Cantor]
	\label{isoCantor}
	Sia $(A,<)$ un insieme totalmente ordinato tale che:
	\begin{enumerate}[1.]
		\item $|A| = \aleph_0$
		\item $(A,<)$ è denso
		\item $(A,<)$ non ha \vocab{estremi}, ossia non ha né massimo né minimo elemento
	\end{enumerate}
	allora $(A,<)\sim(\QQ,<)$.\footnote{Questa è una condizione sufficiente, quella necessaria consisterebbe nel verificare che $(\QQ,<)$ soddisfa le tre proprietà, ma sono ovvie per osservazioni precedenti.}
\end{theorem}

L'idea è di costruire l'isomorfismo per ricorsione. Ad ogni passo della ricorsione avremo $f_i : A_i \rightarrow Q_i$ isomorfismo con $A_i \subseteq A$ finito 
e $Q_i \subseteq \QQ$. Dovremo quindi estendere $f_i$ ingrandendo il suo dominio.
Supponiamo, per esempio, di voler definire $f_{i+1}(x)$ con $x \not \in A_i$. Allora, siccome $A_i$ è finito, per sapere la posizione di $x$ a ciascuno degli elemento di $A_i$, ci basta sapere quale sia l'ultimo elemento prima di $x$,
e quale sia il primo dopo $x$ - diciamo che, per esempio, sono $a_2$  e $a_3$ rispettivamente. Dovremo allora mandare $x$ in un $f_{i+1}(x)$ con $f_i(a_2) < f_{i+1}(x) < f_i(a_3)$, e questo esiste per la densità di $\QQ$.

\begin{figure}[h]
	\centering
	\includegraphics[width = 12cm]{immagini/IsoCantor.png}
\end{figure}

Ragionando simmetricamente, possiamo anche estendere $f_i$, dato un $y \in \QQ$ con $y \not\in Q_i$, in modo tale che $y \in \Imm(f_{i+1})$.

\begin{center}
	\begin{figure}[h!]
		\centering
		\includegraphics[width = 12cm]{immagini/IsoCantor2.png}
	\end{figure}
\end{center}

In definitiva, ci basta quindi fissare un'enumerazione di $A$ e una di $\QQ$, e fare questi passi di estensione in maniera alternata, assicurandoci così di aggiungere al dominio della $f$, uno per uno,
tutti gli elementi di $A$, e di aggiungere all'immagine, uno per uno, tutti gli elementi di $\QQ$. Ci farà comodo la segue osservazione.

\begin{remark}[L'unione di un insieme di funzioni è una funzione]
	Sia $F \subseteq \ps(A \times B)$ un insieme di funzioni. Se vale che:	
	\[ \forall f_1,f_2 \in F \; f_{1|\Dom(f_1) \cap \Dom(f_2)} = f_{2|\Dom(f_1) \cap \Dom(f_2)}
			\]
	cioè se le funzioni da $A$ a $B$ coincidono sull'intersezione dei domini [a due a due], $\forall x \in \Dom(f_1) \cap Dom(f_2) \; f_1(x) = f_2(x)$, allora $\bigcup F$ è ancora una funzione dall'unione dei domini a $B$:\footnote{Moralmente se prendiamo l'unione delle funzioni 
	sull'unione dei domini, se tutti i pezzi che ``incolliamo'' coincidono sugli intervalli dove sono definiti comunemente, allora non c'è alcun problema di buona definizione di una funzione.}
	\[ \bigcup F : \bigcup\{\Dom(f) | f\in F\} \rightarrow B
		\]
\end{remark}

\begin{proof}
	Bisogna verificare che vale la proprietà fondamentale delle funzioni, ovvero che, se $(x,y_1) \in \bigcup F$ e $(x,y_2) \in \bigcup F$, allora $y_1 = y_2$.\\
	Dalla prima cosa abbiamo che esiste $f_1 \in \bigcup F$ tale che $f_1(x) = y_1$ e, dalla seconda, sappiamo che esiste $f_2 \in \bigcup F$ tale che $f_2(x) = y_2$, ma questo significa [per definizione di dominio]
	che $x \in \Dom(f_1) \cap \Dom(f_2)$, dunque dall'ipotesi si ha che:
	\[ y_1 = f_1(x) = f_2(x) = y_2
		\]
\end{proof}

Siamo ora pronti per dimostrare formalmente il teorema.

\begin{proof}
	Per l'ipotesi 1. possiamo fissare un'enumerazione di $A$ e $\QQ$ rispettivamente:
	\[ A = \{a_i | i \in \omega\} \qquad \QQ = \{q_i | i \in \omega\}
		\]
	Intendiamo costruire una successione di funzioni $\{f_i\}_{i \in \omega}$ tali che, per ogni $i \in \omega$:
	\begin{enumerate}[1.]
		\item $f_i : A_i \rightarrow Q_i$ con $|A_i| = |Q_i| < \aleph_0$\footnote{Ricordiamo che $A_i = \{a_0,\ldots,a_{i-1}\} \subseteq A$ e $Q_i = \{q_0,\ldots,q_{i-1}\} \subseteq \QQ$.}
		\item $f_i$ è un isomorfismo di ordini fra $A_i$ e $Q_i$
		\item $f_{i} \subseteq f_{s(i)}$, ossia $f_{s(i)}$ estende $f_i$
		\item $\forall j < i \; a_j \in A_i \land q_j \in Q_i$, ossia $A_i = \{a_0,\ldots,a_{i-1}\} \subseteq \Dom(f_i)$ e $Q_i = \{q_0,\ldots,q_{i-1}\} \subseteq \Imm(f_i)$, $\forall i \in \omega$.
	\end{enumerate}
	Verifichiamo, per cominciare, che dalle proprietà appena elencate segue che $f\Mydef \bigcup_{i \in \omega} f_i$ è un isomorfismo di ordini fra $A$ e $\QQ$.\\
	Da 3. segue, con una facile induzione, che $\forall i,j \in \omega \; i \leq j \rightarrow f_i \subseteq f_j$ (stiamo semplicemente estendendo il fatto che la successiva estenda la precedente a 
	due arbitrarie nell'ordine giusto). Quindi [visto che sono tutte estensioni] siamo nelle ipotesi dell'osservazione precedente e $f$ è una funzione.\\
	4. invece implica che [l'unione numerabile dei domini dà proprio] $\Dom(f) = A$ e $\Imm(f)  = \QQ$ (avendo usato $a_i$ e $q_i$ per enumerare $A$ e $\QQ$, questa cosa implica in automatico $f$ surgettiva). Ci resta quindi da verificare che, dati $x,y \in A$ la mappa è crescente [e quindi in automatico anche iniettiva]:
	\[ x < y \leftrightarrow f(x) < f(y)
		\]
	Fissati $x,y \in A$, siccome $\{a_i\}_{i \in \omega}$ enumera [aka è surgettiva] $A$, esistono $m,n \in \omega$ tali che $x = a_m$ e $y = a_n$. Preso $t \in \omega$, con $m,n < t$ [possiamo perché $\omega$ è illimitato], per la 4., $a_m,a_n \in \Dom(f_t)$ e, siccome $f_t$ è 
	un isomorfismo di ordini per la 2. [ora possiamo usarla perché ha nel suo dominio sia $a_m$ che $a_n$ e quindi ha senso usare la proprietà di isomorfismo], abbiamo:
	\[ x \overset{\text{enum.}}{=} a_m < a_n \overset{\text{enum.}}{=} y \leftrightarrow f(x) \overset{\text{def.}}{=} f_t(a_m) < f_t(a_n) \overset{\text{def.}}{=} f(y)
		\]
	abbiamo quindi che $f$ è ben definita [è una bigezione] ed è l'isomorfismo di ordini tra $(\QQ,<)$ e $(A,<)$ cercato. Non ci resta altro da fare che definire per ricorsione numerabile la successione di funzioni $\{f_i\}_{i \in \omega}$ (in particolare 
	stiamo definendo via ricorsione numerabile una mappa $\omega \rightarrow {}^{\omega}A$). Intanto poniamo $f_0 = \emptyset$.\\
	Per costruire $f_{s(i)}$ definiamo prima un passo intermedio $f_{i+0.5}$ (notazione puramente indicativa). Se $a_i \in \Dom(f_i)$ [$a_i$ è preso in $A_{i+1}$ perché stiamo estendendo, dunque dobbiamo aggiungere il nuovo elemento nell'insieme di partenza], allora $f_{i+0.5} = f_i$ [cioè è già definita ed è $f_i$]. Altrimenti [ovvero se $a_i \not \in \Dom(f_i)$] sia:
	\[ \overline{j} := \min\{j \in \omega | f_i \cup \{(a_i,q_j)\} \,\text{è un isomorfismo}\}
		\]
	poniamo $f_{i+0.5} = f_i \cup \{(a_i,q_{\ol j})\}$. Ora possiamo definire $f_{s(i)}$.\\
	Se $q_i \in \Imm(f_{i+0.5})$\footnote{Stiamo estendendo $f_i$ in due passi in modo da poterla estendere prima in avanti [aggiungendo $a_i$ al dominio] e poi all'indietro [aggiungendo $q_i$ all'insieme d'arrivo], che è proprio la tecnica del \vocab{back-and-forth}.} [$q_i$ preso in $Q_{i+1}$, stiamo estendendo l'insieme d'arrivo (e estendendo a sua volta $f_i$ in modo che rimanga un isomorfismo)] allora $f_{s(i)} = f_{i+0.5}$ [in analogia con prima, l'isomorfismo estende il precedente se l'elemento cade dentro $\Imm(f_i)$]. Altrimenti, sia:
	\[ \ol \iota := \min\{\iota \in \omega | f_{i+0.5} \cup \{(a_\iota,q_i)\}\,\text{è un isomorfismo}\}
		\]
	poniamo $f_{s(i)} = f_{i+0.5} \cup \{(a_{\ol \iota},q_i)\}$. Le proprietà 1.,\ldots,4. seguono in maniera immediata per induzione, a patto che la costruzione sia ben posta, ossia i minimi esistano.\\
	Ad essere precisi, occorre quindi dimostrare, per induzione su $i$, la proposizione:
	\[ \forall i \in \omega \; \text{``la costruzione di $f_i$ è ben posta e valgono 1.,\ldots,4.''}
		\]
	Per verificare che la costruzione della successione delle $f_i$ sia ben posta, vediamo che esiste il minimo nel primo passaggio:
	\[ \ol j = \min\{j \in \omega | f_i \cup \{(a_i,q_j)\}\,\text{è un isomorfismo}\}
		\]
	ossia che l'insieme di cui si prende il minimo non è vuoto, il secondo caso [per vedere che il minimo esiste] sarà analogo.\\
	Per ipotesi induttiva $A_i$ è finito. Se $A_i = \emptyset$ non c'è niente da dimostrare, altrimenti, 
	detto $n = |A_i|$, e sfruttando il fatto che un ordine totale finito è isomorfismo ad un numero naturale [lo si può ordinare totalmente]:
	\[ A_i = \{\alpha_0,\ldots,\alpha_{n-1}\} \qquad\text{con $\alpha_0<\ldots<\alpha_{n-1}$}
		\]
	Ora, l'ipotesi [nella costruzione] è che $a_i \not \in A_i$, quindi [sta fuori o in uno dei ``buchi'' nel dominio, ovvero] o $a_i <\alpha_0$, o $\alpha_k < a_i < \alpha_{k+1}$ per qualche $k$, o $a_{n-1}<a_i$. Nel primo e terzo caso, rispettivamente, siccome $\QQ$ non ha estremi, c'è un $q_j < f_i(\alpha_0)$, o $q_j > f_i(\alpha_n)$ rispettivamente
	[dunque possiamo estendere $f_i$ prendendo quest'elemento fuori da associare al nostro $a_i \not \in \Dom(f_i)$ (a sua volta fuori), per preservare l'ordinamento].
	Nel secondo caso, per la densità di $\QQ$, esiste $q_j$ con $f_i(\alpha_k)<q_j<f_i(\alpha_{k+1})$ [e quindi come prima, possiamo estendere la funzione, preservando l'ordinamento con questo elemento].
\end{proof}

\begin{corollary}[Ogni ordine al più numerabile è isomorfo ad un sottoinsieme di $\QQ$]
	Sia $(A,<)$ un ordine totale con $|A| \leq \aleph_0$. Allora esiste $B \subseteq \QQ$ tale che $(A,<) \sim (B,<)$ con l'ordinamento indotto su $B$ da $\QQ$.
\end{corollary}

\begin{note}
	Volendo, si potrebbe dimostrare questo corollario ripetendo, con qualche variazione, la dimostrazione del teorema. Ora daremo, però,
	un argomento che, invece, applica il teorema. È comodo definire, prima, il prodotto di ordini.
\end{note}

\begin{definition}[Prodotto lessicografico di ordini]
	Dati $(A,<_A)$ e $(B,<_B)$ definiamo il \vocab{prodotto lessicografico} di ordini come:
	\[ (A,<_A) \times (B,<_B) \Mydef (A \times B, <_{A \times B})
 		\]
	dove $(a,b) <_{A \times B} (a',b') \Mydef (b <_B b') \lor (b = b' \land a <_A a')$.
\end{definition}

Ossia: $(A,<_A) \times (B,<_B)$ è il prodotto cartesiano $A \times B$ munito dell'ordine che CONFRONTA PRIMA LA SECONDA COMPONENTE.
Visualmente, si può immaginare $(A,<_A) \times (B,<_B)$ come ``$(A,<_A)$ ripetuto $(B,<_B)$ volte''.

\begin{figure}[h]
	\centering
	\includegraphics[width = 12cm]{immagini/ordine_lessicografico.png}
\end{figure}

In altre parole, prendiamo l'insieme $A$, $B$ volte, e disponiamo le sue copie secondo l'ordine degli elementi di $B$ (ogni elemento in una copia di $A$ avrà come prima componente un elemento di $A$, 
e come seconda l'elemento di $B$ che corrisponde a quella copia di A). Questa immagine rispetta perfettamente l'ordine dato dal prodotto lessicografico, infatti, confrontando elementi a caso in $A \times B$ si guarda
prima la seconda componente (che determina l'ordine delle copie di $A$), e a parità di quest'ultima (aka siamo nella stessa copia di $A$) si confronta la prima secondo $<_A$.

\begin{remark}[Ordine totale $\implies$ prodotto ordine totale]
	Il prodotto è un ordine. Inoltre se $(A, <_A)$ e $(B,<_B)$ sono ordini totali, allora anche 
	$(A,<_A) \times (B,<_B)$ lo è.
\end{remark}

\begin{exercise}[Associatività del prodotto lessicografico]
	Dati $(A,<_A),(B,<_B)$ e $(C,<_C)$ dimostra che:
	\[ ((A,<_A) \times (B,<_B)) \times (C,<_C) = (A,<_A) \times ((B,<_B) \times (C,<_C))
		\]
	ossia che il prodotto lessicografico di ordini è associativo a meno di isomorfismi.
\end{exercise}

Veniamo ora alla dimostrazione del corollario

\begin{proof}
	Se $A = \emptyset$ è banale. Supponiamo $A \ne \emptyset$ (il caso di $|A|$ finito è anche banale e si può [volendo] trattare separatamente\footnote{Avremmo $A$
	in bigezione con $n$ che si immerge in $\omega$ che si immerge in $\QQ$, componendo le mappe avremmo che l'immagine con l'ordine indotto da $\QQ$ è proprio
	il sottoinsieme voluto.} tuttavia questa dimostrazione copre comunque il caso di $A$ finito). Consideriamo quindi:
	\[ (S,<) \Mydef (\QQ,<) \times (A,<)
		\]
	\begin{figure}[H]
		\centering
		\includegraphics[width = 11cm]{immagini/coroll_iso_cantor.png}
	\end{figure}
	L'insieme $S = \QQ \times A$ è [in ambo i casi] numerabile. Inoltre, dato $(q,a) \in \QQ \times A$ abbiamo:
	\[ (q-1,a) < (q,a) < (q+1,a) \,\footnote{Ricordiamo che stiamo usando $(A,<)$ come secondo termine del prodotto lessicografico, dunque le copie di $(\QQ,<)$ sono ordinate come gli elementi di $A$.}
		\]
	quindi $\QQ \times A$ non ha estremi [né superiori né inferiori]. Per verificare che è denso, consideriamo $(q_1,a_1) < (q_2, a_2)$.
	\begin{itemize}
		\item Se $a_1 < a_2$, allora $(q_1,a_1) < (q_1+1,a_1) < (q_2,a_2)$ (per la definizione di ordine nel prodotto lessicografico, ci possiamo spostare come ci pare [sulla prima componente] se le due copie di $\QQ$ in cui prendo gli elementi sono distinte, ottenendo elementi nel mezzo).
		\item Se $a_1 = a_2$ (quindi siamo nella stessa copia di $\QQ$), si ha che $(q_1,a_1) < \left(\frac{q_1+q_2}{2},a_1\right) < (q_2,a_2) = (q_2,a_1)$, ottenendo ancora un elemento nel mezzo.\footnote{Morale della favola: se ho un ordine non denso, è sufficiente moltiplicarlo per $\QQ$.}
	\end{itemize}
	quindi $(S,<)$ è denso e per il \hyperref[isoCantor]{teorema di isomorfismo di Cantor} si ha $(S,<) \sim (\QQ,<)$. Ma $A \hookrightarrow \QQ \times A : a \mapsto (0,a)$, quindi, componendo l'immersione con l'isomorfismo trovato, abbiamo trovato che $(A,<)$ è isomorfo ad un sottoinsieme di $(\QQ,<)$. 
\end{proof}

\begin{exercise}[Isomorfismo di Cantor senza l'ipotesi di illimitatezza]
	Dimostra che se $(A,<)$ è denso \textcolor{red}{[ma non necessariamente senza estremi]} e $2 \leq |A| \leq \aleph_0$, allora $(A,<)$ è isomorfismo a uno dei seguenti intervalli di $\QQ$:
	\[ [0,1]_{\QQ} \qquad ]0,1]_{\QQ} \qquad [0,1[_{\QQ} \qquad ]0,1[_{\QQ}
		\]
\end{exercise}

\begin{soln}
	Osserviamo che per l'ipotesi di densità $2 \leq |A| \leq \aleph_0 \implies |A| = \aleph_0$\footnote{Stiamo escludendo l'insieme denso con un solo elemento}. A questo punto, se $A$ è senza estremi si ha:
	\[ (A,<) \sim (\QQ,<) \sim ]0,1[_\QQ
		\]
	($]0,1[_\QQ$ è totalmente ordinato [ereditariamente dall'ordine di $\QQ$], senza estremi, numerabile [sottoinsieme di $\QQ$ + $n \mapsto \frac 1n$] e denso [basta prendere la media di due elementi e osservare che sta sempre in mezzo per le proprietà algebriche di $\QQ$]).\\
	Negli altri casi si osserva che:
	\[ [0,1]_{\QQ} = ]0,1[_{\QQ} \,\cup\, \{0,1\}  \qquad ]0,1]_{\QQ} = ]0,1[_{\QQ} \,\cup\, \{1\} \qquad [0,1[_{\QQ} = ]0,1[_{\QQ} \,\cup\, \{0\}
		\]
	vediamo, ad esempio, nel caso di $A$ con entrambi gli estremi, che, detti questi $a$ e $b$, si ha:
	\[ (A\setminus\{a,b\},<_{|A\setminus\{a,b\}}) \sim (\QQ,<) \sim (]0,1[_{\QQ},<)
		\]
	e analogamente negli altri due casi. Pertanto, nel caso in cui $A$ abbia entrambi gli estremi:
	\[ (A,<) \sim (\QQ\cup\{\alpha,\beta\},<') \sim ([0,1]_\QQ,<) = (]0,1[_{\QQ} \,\cup\, \{0,1\},<)
		\]
	con $f(a) = \alpha$, $f(b) = \beta$ e $<' = < \cup (\{\alpha\} \times \QQ) \cup (\QQ \times \{\beta\})$\footnote{Per essere precisi,
	detto $f$ l'isomorfismo tra $(\QQ,<)$ e $]0,1[_{\QQ}$, per estenderlo ad un isomorfismo tra $(\QQ \cup \{\alpha,\beta\},<')$ e $([0,1]_\QQ,<)$, ci basta porre $f' := f \cup \{(\alpha,0),(\beta,1)\}$,
	in questo modo, componendolo con l'isomorfismo tra $(A,<)$ e $(\QQ\cup\{\alpha,\beta\},<')$, si ottiene l'isomorfismo tra $A$ il chiuso $[0,1]_\QQ$ voluto.}. Gli altri due casi ($A$ chiuso in un estremo solo) sono analoghi e danno l'isomorfismo 
	con $[0,1[_{\QQ}$ e $]0,1[_{\QQ}$.
\end{soln}

\subsection{Il grafo random}

La tecnica di estendere indefinitamente isomorfismi parziali che ci ha permesso di dimostrare il teorema di isomorfismo di Cantor
si chiama \vocab{back-and-forth}, ed è un metodo fondamentale per trovare isomorfismi fra strutture.\\
Cogliamo questa occasione per suggerire un esercizio di applicazione della medesima tecnica che è un po' complicato. Si tratta di definire il \vocab{grafo random}
o \vocab{grafo di Rado}.

\begin{definition}[Grafo]
	Un \vocab{grafo} $(V,e)$ sull'insieme di vertici $V$ è dato da una relazione $e$ simmetrica ($\forall x,y \in V \; (x,y) \in e \leftrightarrow (y,x) \in e$) e 
	irriflessiva ($\forall x \in V \; (x,x) \not\in e$).
\end{definition}

L'idea è che $V$ può essere immaginato come un insieme di punti che possono essere connessi da archi. C'è un arco fra $x$ e $y$ se $(x,y) \in e$.\\
%
Partiamo da un'idea intuitiva -  chi ha già seguito un corso di probabilità saprà formalizzare questa cosa in termini precisi. Data una probabilità $p \in ]0,1[$ costruiamo un grafo
$G_p$ con insieme di vertici $\omega$ come segue. Per ogni coppia $(i,j) \in \omega \times \omega$ con $i < j$ [solo per prendere tutte le coppie una volta e non beccare le stesse andando avanti] lanciamo una moneta \textbf{che fa testa con probabilità $p$} - tutte queste monete indipendentemente - e, se viene testa, mettiamo un arco 
fra $i$ e $j$.\\
Potremmo pensare che i grafi $G_{0.01}$ e $G_{0.99}$ debbano venire molto diversi: uno ha l'1\% degli archi possibili, l'altro ha il 99\%, insomma uno è quasi vuoto, l'altro quasi completo. \textbf{Avviene, tuttavia, che, con probabilità 1, questi grafi sono isomorfismi}\footnote{A meno di rinominare i vertici, che è quello che diremo nella definizione di isomorfismo.}, dove, per essere precisi
[possiamo definire l'isomorfismo tra grafi].

\begin{definition}[Isomorfismo fra grafi]
	I grafi $(V_1,e_1)$ e $(V_2,e_2)$ sono \vocab{isomorfi} se esiste una bigezione $f : V_1 \rightarrow V_2$ tale che:
	\[ \forall v,w \in V_1 \; (v,w) \in e_1 \longleftrightarrow (f(v),f(w)) \in e_2
		\]
\end{definition}

Vediamo perché. Dati due sottoinsiemi finiti $X$ e $Y$ di $\omega$, e dato un vertice $v \not \in X \cup Y$ la probabilità che $x$ sia connesso da un arco a tutti i vertici di $X$ e a nessuno di quelli di $Y$
è $p^{|X|} \cdot (1 - p)^{|Y|}$\footnote{Eventi indipendenti: $v$ è connesso ad un vertice di $X$ con probabilità $p$, ed è connesso a tutti i vertici di $X$ con probabilità $p^{|X|}$, viceversa non è connesso ad alcun vertice di $Y$ con
probabilità $(1-p)^{|Y|}$.} - come che sia, è un certo numero $>0$ - e ci sono infiniti [siamo in $\omega$] $v \not \in X \cup Y$. Si capisce, quindi, che con probabilità 1 - ossia certamente - almeno uno di questi vincerà questa lotteria (ne abbiamo infiniti, quindi quasi certamente ne troviamo uno), ossia sarà connesso 
a tutti gli $X$ e a nessuno degli $Y$. Usiamo l'esistenza di questo $v$ per definire un grafo random.

\begin{definition}[Grafo random]
	Il grafo $(\omega,e)$ è un \vocab{grafo random} se:\footnote{Typo di Mamino.}
	\[ \forall X \subseteq \omega \; \forall \,Y \subseteq \omega\setminus X \; |X|,|Y|<\aleph_0\; \exists v \in \omega \setminus(X \cup Y) \; \underbrace{X \times \{v\} \subseteq e}_{\forall x\in X \; (x,v) \in e} \land \underbrace{(Y \times \{v\}) \cap e = \emptyset}_{\neg\exists y \in Y \; (y,v) \in e}
		\]
	(cioè se per ogni coppia di sottoinsiemi di vertici disgiunti esiste un vertice fuori dall'unione di questi ultimi, connesso a tutti i vertici di uno ed a nessuno dei vertici dell'altro).
\end{definition}

\begin{exercise}[Esistenza e unicità del grafo random]
	Dimostra che esiste un grafo random, ed è unico a meno di isomorfismi.\footnote{\underline{Hint}: Usare il back-and-forth per l'unicità.}
\end{exercise}

\begin{soln}
	
\end{soln}

\newpage
\section{\texorpdfstring{$\mathbb{R}$ e la cardinalità del continuo}{R e la cardinalità del continuo}}

In questa sezione daremo una definizione di $\RR$ come insieme ordinato. Estenderemo, poi, la definizione ad includere le operazioni di campo, ma senza svolgere le verifiche.

\begin{definition}[Maggiorante, insieme superiormente limitato ed estremo superiore]
	Sia $(A,<)$ un ordine totale, allora:
	\begin{itemize}
		\item $m \in A$ è un \vocab{maggiorante} di $B \subseteq A$ se $\forall x \in B \; x \leq m$
		\item $B \subseteq A$ è \vocab{superiormente limitato} se ha un maggiorante
		\item $s\in A$ è \vocab{l'estremo superiore} di $B$ - denotato con $\sup B$ - se $s$ è il minimo dei maggioranti di $B$.
	\end{itemize}
\end{definition}

\begin{note}
	Non sempre gli estremi superiori esistono, e, se $B$ ha un estremo superiore, questo è unico\footnote{È una facile verifica che passa attraverso la definizione di minimo.}.
\end{note}

\begin{definition}[Ordine totale completo]
	Un ordine totale $(A,<)$ è \vocab{completo} se ogni $B \subseteq A$ superiormente limitato ha un estremo superiore $\sup B \in A$.\footnote{Questa definizione [la Dedekind-completezza] è a priori diversa dalla Cauchy-completezza (ovvero che tutte le successioni di Cauchy convergono).}
\end{definition}

\begin{exercise}[$\QQ$ non è completo]
	\label{q_noncompleto}
	Dimostra, usando solo le proprietà di $\QQ$, che l'insieme $\{x \in \QQ | x^2 < 2\}$ non ha estremo superiore in $\QQ$.
\end{exercise}

\begin{soln}
	
\end{soln}

In conseguenza dell'esercizio, possiamo dire che $\QQ$ non è completo. Costruiamo ora un ordine completo $(\RR,<)$ che contiene una copia isomorfa di $\QQ$ come sottoinsieme denso.

\begin{definition}[Segmento iniziale]
	Sia $(A,<)$ un ordine totale. $B \subseteq A$ è un \vocab{segmento iniziale} di $A$ se $\forall x \in B \; \forall y \in A \; y < x \rightarrow y \in B$. [Se contiene un punto, contiene tutti i precedenti, strettamente].
\end{definition}

Ossia $B$ è un segmento iniziale di $A$ se, ogniqualvolta $B$ contiene un elemento, $B$ contiene altresì tutti gli elementi minori di questo.
Un segmento iniziale $B$ di $A$ si dice \vocab{proprio} se $B \ne A$.

\begin{example}[Segmento iniziale principale]
	Dato $(A,<)$ ordine totale, $A$ stesso e $\emptyset$ sono segmenti iniziali di $A$. Dato $x \in A$, l'insieme:
	\[ A_x \Mydef \{y \in A | y < x\}
		\]
	è un segmento iniziale proprio di di $A$ - detto \vocab{segmento inziale principale} determinato da $x$. Ad esempio $\{x \in \QQ | x < 0 \lor x^2 < 2\}$
	è un segmento iniziale [proprio] di $\QQ$ che non è principale.
\end{example}

\begin{note}
	Useremo nuovamente il concetto di segmento iniziale studiando gli ordinali. Il prossimo concetto, quello di sezione di Dedekind, invece, ci serve unicamente per definire $\RR$.
\end{note}

\begin{definition}[Sezioni di Dedekind]
	Una \vocab{sezione} sull'insieme totalmente ordinato $(A,<)$ è un segmento iniziale \textcolor{red}{proprio}
	e \textcolor{red}{non vuoto} di $A$ che \textcolor{red}{non ha un massimo elemento} [per convenzione il punto lo metto nel complementare].
\end{definition}

Ossia $B$ segmento iniziale di $A$ è una sezione se $B \ne A$, $B \ne \emptyset$ e $\forall x \in B \; \exists y \in B \; x < y$.

\begin{definition}[Insieme ordinato dei numeri reali]
	Definiamo l'insieme dei \vocab{numeri reali} come l'insieme delle sezioni di Dedekind di $\QQ$:
	\[ \RR \Mydef \{x \in \ps(\QQ) | \, \text{$x$ è una sezione su $\QQ$}\}\,\footnote{Moralmente: sono tutti i modi di prendere $\QQ$ e tagliarlo in due (indipendentemente da cosa chiamo numero reale, i.d. la cosa a destra o a sinistra,
	cioè la sezione di Dedekind o il suo complementare, basta fissare una codifica).}\,\footnote{Dunque nella nostra codifica un reale non è altro che una semiretta sinistra di $\QQ$.}
		\]
	con l'ordine dato da:
	\[ \forall x,y \in \RR \; x \leq y \Mydef x \subseteq y \, \footnote{Era equivalente definire il $<$ a partire da $\subsetneq$, avremmo ottenuto comunque lo stesso ordine su $\RR$.}
		\]
\end{definition}

\begin{proposition}[$\RR$ è completo]
	$(\RR,<)$ è un ordine totale completo.
\end{proposition}

Prima della dimostrazione, isoliamo un semplice lemma.

\begin{lemma}[L'unione di segmenti iniziali è un segmento iniziale]
	Sia $(A,<)$ un ordine totale e $X$ un insieme di segmenti iniziali di $A$. Allora $\bigcup X$ è un segmento iniziale di $A$.
\end{lemma}

\begin{proof}
	Sia $\alpha \in \bigcup X$ e $\beta \in A$, con $\beta < \alpha$. Dobbiamo dimostrare che $\beta \in \bigcup X$ [cioè che l'unione è ancora un segmento iniziale].
	Siccome $\alpha \in \bigcup X$, esiste $x \in X$, tale che $\alpha \in x$ (cioè $\alpha$ è un elemento di un elemento per definizione di unione). Siccome $x$ è un segmento
	iniziale di $A$, allora $\beta < \alpha \rightarrow \beta \in \underbrace{x}_{\in X} \subseteq \bigcup X$ [cioè è un elemento di un elemento di $X$ (= sottoinsieme dell'unione degli elementi degli elementi), dunque sta nell'unione e quindi questa è un segmento iniziale].
\end{proof}

Ora possiamo dimostrare la proposizione come segue.

\begin{proof}
	Abbiamo un ordine parziale perché il contenimento $\subseteq$, è un ordine parziale su $\ps(\text{quello che sia})$. Supponiamo per assurdo, che non sia totale, allora esistono $x,y \in \RR$ per cui $x \not\subseteq y$ e 
	$y \not\subseteq x$, quindi ci sono $a \in x \setminus y$ e $b \in y \setminus x$ (non essendo contenuti né uguali fare queste sottrazioni di insiemi ci lascia sempre insiemi non vuoti in cui prendere gli elementi).
	Ora si danno due casi: se $a <_{\QQ} b$\footnote{Le sezioni di Dedekind sono sottoinsiemi di $\QQ$, quindi i loro elementi sono ordinati dall'ordine totale in $(\QQ,<)$.} allora [per definizione di segmento iniziale] $b \in y \implies a \in y \,\lightning$, simmetricamente, se $b < a$ allora $a \in x \implies b \in x \, \lightning$. Dunque $\subseteq$ è un ordine totale.
	Resta da dimostrare la completezza.\\ Sia $A \subseteq \RR$ non vuoto e superiormente limitato [= ammette un maggiorante] da $m \in \RR$. Dimostriamo che $\sup A = \bigcup A \in \RR$.\\
	Per il lemma precedente $\bigcup A$ è ancora un segmento iniziale, e siccome $A$ non è vuoto $\bigcup A \ne \emptyset$, inoltre poiché $m$ è un maggiorante di $A$ [quindi per l'ordinamento definito contiene tutti gli elementi e in automatico gli elementi degli elementi], si ha $\bigcup A \subseteq m$, per cui $\bigcup A \ne A$ (ovvero è un segmento iniziale proprio).
	In definitiva $\bigcup A$ è una sezione di Dedekind di $\QQ$, e, di conseguenza un elemento di $\RR$.\\
	Verifichiamo che $\bigcup A$ è un maggiorante di $A$. Se $x \in A$, allora $x \subseteq \bigcup A$ [per definizione], cioè, appunto $x \leq \bigcup A$ per come abbiamo definito l'ordine su $\RR$.\\
	Ora, se $m$ è un altro maggiorante di $A$, allora $\forall x \in A \; x \subseteq m$, ma ciò equivale a $\bigcup A \subseteq m$ (se tutti gli elementi sono contenuti in $m$, allora lo sono in automatico tutti gli elementi degli elementi), quindi $\bigcup A$ è il minimo dei maggioranti di $A$.
\end{proof}

\begin{remark}[$\QQ$ si immerge in maniera ordinata e densa in $\RR$]
	La funzione $\iota : \QQ \hookrightarrow \RR : a \mapsto \QQ_a = \{x \in \QQ | x < a\}$, cioè la funzione che manda ogni razionale nella sua sezione di Dedekind principale\footnote{È in automatico ben definita essendo l'oggetto in arrivo una sezione di Dedekind di $\QQ$.}, immerge $\QQ$ in $\RR$ in maniera strettamente
	crescente e densa (ossia $\iota(\QQ) = \Imm(\iota)$ è densa in $\RR$).
\end{remark}

\begin{proof}
	Dati $a,b \in \QQ$, con $a < b$, abbiamo $\QQ_a \subsetneq \QQ_b$ (perché ad esempio $a \not \in \QQ_a$, ma $a \in \QQ_b$, dunque vale $\QQ_a \subsetneq \QQ_b \equiv \QQ_a < \QQ_b$), quindi $\iota$ è strettamente crescente [dunque anche iniettiva]\footnote{In particolare così abbiamo già che $(\QQ,<)$ è isomorfo a $(\iota[\QQ],<_{|\iota[\QQ]})\subseteq (\RR,<)$.}.
	Dati $x,y \in \RR$, con $x < y$, ciò equivale per definizione di ordine su $\RR$ a $x \subsetneq y$, dunque esiste $a \in y \setminus x$ ($a \in \QQ$ per definizione di sezione).\\
	Siccome $y$ non ha massimo (per definizione di sezione) [e $a \in y$], c'è un $b \in y$ [dunque $b \in \QQ$] con $a < b$. Ora per tale $b$ si ha: $x \subsetneq \QQ_b \subsetneq y$,
	dove il primo contenimento\footnote{Per costruzione $a \in y\setminus x$, e $a \in \QQ_b$, dunque $x < a$ e per la definizione di segmento iniziale tutti gli elementi di $x$ stanno in $\QQ_b$.} è stretto perché $a \not \in x$ [per definizione di $a$] e $a \in \QQ_b$ [perché $a<b$ per come è definito $b$], mentre 
	il secondo è stretto perché $b \not \in \QQ_b$ [per definizione di di segmento iniziale principale] e $b \in y$ [per definizione] (inoltre sono contenimenti di segmenti iniziali, dunque è naturale che tutti gli elementi di quelli più a sinistra siano contenuti da quelli più a destra). Dunque $\Imm(\iota)$ densa in $\RR$.
\end{proof}
\pagebreak
\begin{notation}[Abuso di immersioni]
	Siccome le immersioni:
	\[ \omega \hookrightarrow \ZZ \hookrightarrow \QQ \hookrightarrow \RR
		\]
	sono tutte iniettive e crescenti, quando non c'è pericolo di confusione, possiamo abusare della notazione immaginando che 
	queste siano vere e proprie inclusioni [di insiemi, senza passare per le immagini\footnote{Anche più immagini visto che per
	arrivare in $\RR$ gli insiemi più a sinistra devono passare per una composizione di funzioni.}]:
	\[ \omega \;\textcolor{red}{\subseteq}\; \ZZ \;\textcolor{red}{\subseteq} \;\QQ \;\textcolor{red}{\subseteq}\; \RR
		\]
	In realtà non è vero: per esempio è $\iota[\QQ]$, non $\QQ$, a essere sottoinsieme di $\RR$, ma $\iota[\QQ]$ è in corrispondenza biunivoca, in maniera 
	canonica, tramite appunto $\iota$, con $\QQ$, e questa corrispondenza preserva tutta la struttura rilevante - l'ordine come abbiamo verificato, ma anche le 
	operazioni di campo.
\end{notation}

\begin{corollary}[$\RR$ è più che numerabile]
	$\aleph_0 < |\RR|$.
\end{corollary}

\begin{proof}
	Dall'osservazione sulla notazione di sopra, abbiamo visto che $\QQ \overset{\iota}{\hookrightarrow} \RR$, da cui $\aleph_0 = |\QQ| \leq |\RR|$, inoltre $\QQ$ è denso in $\RR$, pertanto 
	$\RR$ è denso [in se stesso] (per la seconda cosa ci basta che ci sia sempre qualcosa in $\RR$ tra due elementi di $\RR$, se questo qualcosa è un elemento di $\iota[\QQ]$ poco importa,
	la proprietà è verificata lo stesso).\\ Si vede facilmente che $\RR$ non ha massimo né minimo, quindi se $\RR$ fosse numerabile sarebbe isomorfo, per l'\hyperref[isoCantor]{isomorfismo di Cantor},
	a $\QQ$. D'altro canto $\RR$ è completo e $\QQ$ no [per l'\hyperref[q_noncompleto]{esercizio} visto], dunque non possono essere isomorfi, e quindi [non vale l'ipotesi 1. dell'isomorfismo di Cantor che avevamo assunto dunque] non può esserci una bigezione $\implies \aleph_0 < |\RR|$.
\end{proof}

\subsection{Caratterizzazione dei reali come ordine}
Abbiamo stabilito che $(\RR,<)$ è un ordine completo senza estremi con un sottoinsieme, $\QQ$, denso e numerabile.
Queste proprietà, a loro volta, caratterizzano l'insieme ordinato $(\RR,<)$ a meno di isomorfismi.

\begin{proposition}[Caratterizzazione di $(\RR,<)$]
	Sia $(A,<)$ un ordine totale, se:
	\begin{enumerate}[1.]
		\item $(A,<)$ è completo
		\item $(A,<)$ è senza estremi
		\item esiste $B \subseteq A$ numerabile e denso in $A$
	\end{enumerate} 
	allora $(A,<)$ è isomorfo a $(\RR,<)$.\footnote{Come al solito il teorema è una
	condizione sufficiente per essere isomorfo ad $\RR$, e l'altra freccia è già stata verificata man mano che si costruiva $\RR$ in precedenza.}
\end{proposition}

\begin{proof}
	Sia $\widetilde{A}$ l'insieme delle sezioni su \textcolor{red}{$B$}. Osserviamo che $(A,\leq) \sim (\widetilde{A},\subseteq)$. L'isomorfismo è infatti dato da:
	\[ f : A \rightarrow \widetilde{A} : a \mapsto B_a = \{x \in B | x < a\}\subsetneq B
		\]
	la cui inversa è:
	\[ g : \widetilde{A} \rightarrow A : Y \mapsto \sup Y
		\]
	\textcolor{MidnightBlue}{Verifiche: $a$ è un maggiorante di $B_a$ (per definizione), ed è il minimo perché se $x<a$ fosse un maggiorante, per la densità di $B$ in $A$, esiste $y \in B$ con $x<y<a$, quindi $y \in B_a$ per la
	seconda disuguaglianza (cioè per definizione di segmento iniziale), e $x<y$ non può essere ovviamente un maggiorante di $B_a$ [abbiamo appena trovato un elemento in $B_a$ più grande]. Quindi $\sup B_a = a$, ossia $g(f(a)) = a$.\\
	Per ottenere la composizione opposta, $B_{\sup Y} = Y$, dimostriamo che $x \in B_{\sup Y} \leftrightarrow x \in Y$ [che è equivalente per estensionalità].
	\begin{itemize}
		\item[$\boxed{\longleftarrow}$] Per costruzione, vale che $x \in Y \rightarrow x \leq \sup Y$, perché il sup per definizione è un maggiorante di $Y$, inoltre 
		non può essere $x = \sup Y$ perché, per definizione di sezione, $Y$ non ha massimo, quindi $x \in B_{\sup Y}$.
		\item[$\boxed{\longrightarrow}$] Per ottenere la freccia opposta, abbiamo $x \in B_{\sup Y} \iff x < \sup Y$, allora $x$ non può essere un maggiorante di $Y$
		- perché $\sup Y$ è il minimo di questo e $x < \sup Y$ - quindi esiste $y \in Y$\footnote{Sarebbe la caratterizzazione del sup di un insieme.} [se non ci fosse $y$, $x$ sarebbe un maggiorante, ma come detto, ciò è assurdo], con $x < y$, ma $Y$ è un segmento iniziale, quindi per definizione $x \in Y$.
	\end{itemize}}
	Ora per il \hyperref[isoCantor]{teorema di isomorfismo di Cantor} [è numerabile per 3., è denso per lo stesso motivo (se lo è in $A$, lo è
	a maggior ragione in se stesso\footnote{Tutti gli elementi di $B$ sono anche elementi di $A$.}), ed essendo $A$ senza estremi e $B$ denso
	in $A$, anche $B$ è senza estremi\footnote{Se $B$ fosse limitato superiormente o inferiormente, ci sarebbe un elemento di $A$ più grande del
	limite, e per densità uno di $B$ tra il limite e quello più grande.}], $B$ con l'ordine indotto da $(A,<)$ è isomorfo a $(\QQ,<)$, quindi le
	sezioni di Dedekind di $(B,<)$ sono isomorfe alle sezioni di $\QQ$, ossia $(\widetilde{A},\subseteq) \sim (\RR, \leq)$ e quindi $(A,\leq) \sim (\RR,\leq)$.
\end{proof}

\begin{note}
	Come conseguenza della dimostrazione abbiamo ottenuto che le sezioni di Dedekind di $\RR$ con l'ordine indotto sono isomorfe a $(\RR,<)$.
\end{note}

Per completezza, definiamo ora la struttura di campo di $\RR$. Non verificheremo le proprietà, e neanche la correttezza di queste definizioni.

\begin{definition}[Campo ordinato]
	$(F,0,1,+,\cdot,\leq)$ è un \vocab{campo ordinato} se:
	\begin{itemize}
		\item $(F,0,1,+,\cdot)$ è un campo
		\item $(F,<)$ è un'ordine totale \footnote{Come ribadito più volte è indifferente usare $<$ o $\leq$.}
		\item $\forall x,y,z \in F \; x < y \rightarrow x + z < y + z$ \textcolor{orange}{(compatibilità con la somma)}
		\item $\forall x,y \in F (0 < x \land 0 < y) \rightarrow 0 < x \cdot y$ \textcolor{orange}{(compatibilità con il prodotto)}
	\end{itemize}
	(le ultime due richieste sono le proprietà di \textbf{compatibilità} della struttura di campo [= compatibilità delle operazioni] con l'ordinamento $<$ di $F$).
\end{definition}

\begin{definition}[Somma su $\RR$]
	Dati $x,y \in \RR$ definiamo la \vocab{somma di numeri reali}:
	\[ x + y \Mydef \{a + b \in \QQ | a \in x \land b \in y\}
		\]
	cioè la sezione di $\QQ$ che ha come elementi i razionali somme di elementi di $x$ e $y$.
\end{definition}

\begin{definition}[Prodotto su $\RR$]
	Dati $x,y \in \RR$ con $x > 0$ e $y > 0$ definiamo il \vocab{prodotto di numeri reali}:
	\[ x \cdot y \Mydef \{q \in \QQ | q \leq 0\} \cup \{a\cdot b \in \QQ| a \in x  \land b \in y \land a > 0 \land b > 0\}
		\]
	cioè l'unione di $\QQ_0 \cup \{0\}$ con la sezione di $\QQ$ che ha come elementi i razionali prodotti di elementi \textbf{positivi} di $x$ e $y$.
\end{definition}

Definiamo quindi $-x$ tramite l'inverso additivo ed il prodotto nei casi $x < 0$, $y>0$ etc. tramite l'uso della regola dei segni.

\begin{theorem}[Unicità di $(\RR,0,1,+,\cdot,\leq)$]
	$\RR$ dotato delle operazioni definite, è l'unico campo ordinato completo a meno di isomorfismo.
\end{theorem}

La dimostrazione di questo teorema, talvolta, si vede nei corsi di analisi 1, noi non la studieremo, Per chi fosse interessato: LIBRO DI TESTO \cite{jech}, capitolo 10; NOTE DEL PROF. Di Nasso, 
fascicolo 4 \cite{diNasso_eti_2019_20}; LEZIONE 16 dell'a.a. 2020-21 \cite{mamino_eti_20_21}.

\subsection{\texorpdfstring{La cardinalità del continuo è $2^{\aleph_0}$}{2 alla aleph-zero}}
Torniamo ad una questione più strettamente insiemistica.

\begin{theorem}[Cardinalità del continuo]
	$|\RR| = 2^{\aleph_0}$
\end{theorem}

Questo teorema ci dice, in un modo ancora diverso, che $\RR$ è più che numerabile - poiché $\aleph_0 < 2^{\aleph_0}$ (per \hyperref[cantor]{Cantor}) - ma, in più, caratterizza
anche esattamente la cardinalità di $\RR$.

\textcolor{MidnightBlue}{Prima della dimostrazione formale, vediamo intuitivamente perché il risultato è vero. Per definizione $\RR \subseteq \ps(\QQ)$, quindi si immerge nelle parti, da cui
$|\RR| \leq 2^{\aleph_0}$, mentre la disuguaglianza da dimostrare è $2^{\aleph_0} \leq |\RR|$.
Esibiamo quindi una funzione iniettiva $\ps(\omega) \rightarrow \RR$ \footnote{Ricordando che $|\ps(A)| \overset{\text{visto}}{=} |{}^{A}2| \overset{\text{op. card.}}{=} |2|^{|A|}$.} come segue:
\[ f : \ps(\omega) \rightarrow \RR : S \mapsto 0.a_0^Sa_1^Sa_2^Sa_3^S\ldots\, \footnote{Sarebbe la scrittura decimale.} \qquad \text{con} \; a_i^s = \begin{cases}
	0 &\text{\text{se $i \not\in S$}} \\
	1 &\text{\text{se $i \in S$}}
\end{cases}\footnote{Cioè restituisce un numero decimale fatto da soli 0 e 1, che ha gli 1 dove l'indice corrisponde ad una posizione sta nell'insieme $S$ e 0 se la posizione non lo
è (naturalmente se l'insieme è finito la sequenza sarà 0 da un certo punto, se non lo fosse non è detto, ad esempio presi i naturali pari avremo una sequenza infinita del tipo $0.1010101010\ldots$).}
	\]
per esempio $S = \{2,3,5,7,11,\ldots\}$ dà $f(S) = 0.001101010001\ldots$ è chiaro che:
\[ f(S) = f(T) \overset{\text{def.}}{\iff} \forall i \in \omega \; a_i^S = a_i^T \overset{\text{def.}}{\iff} \forall i \in \omega \; i \in S \leftrightarrow i \in T \overset{\text{estensionalità}}{\iff} S = T
	\]
Non è difficile formalizzare questa dimostrazione. Basterebbe definire $0.a_1a_2a_3\ldots$ come $\sum_{i = 0}^\infty a_i 10^{-i}$, poi $\sum_{i = 0}^\infty$
come $\sup\left\{\sum_{i = 0}^n\right\}$, poi $\sum_{i = 0}^n$ per ricorsione numerabile, poi dimostrare le proprietà aritmetiche rilevanti. Noi sfrutteremo la stessa idea, 
ma formulando la dimostrazione in termini di ordini.}

\subsection{Operazioni che coinvolgono la cardinalità del continuo}
Prima di dimostrare il teorema, sviluppiamo un po' di aritmetica della cardinalità $2^{\aleph_0}$. \textcolor{red}{Questi lemmi sono importanti, e serviranno per calcolare 
la cardinalità di insiemi concreti.}

\begin{remark}
	$(2^{\aleph_0})^{\aleph_0} = 2^{\aleph_0}$.
\end{remark}

\begin{proof}
	Basta osservare che per le proprietà delle operazioni sulla cardinalità $(2^{\aleph_0})^{\aleph_0} = 2^{\aleph_0 \cdot \aleph_0}$, e, ricordando
	che prodotto di numerabili è numerabile, si ottiene $2^{\aleph_0 \cdot \aleph_0} = 2^{\aleph_0}$.
\end{proof}

\begin{lemma}[Assorbimento della cardinalità al più continua]
	Siano $\alpha$, $\beta$ abbreviazioni per o ``finito'' o $\aleph_0$ o $2^{\aleph_0}$, allora:
	\[ \alpha + \beta = \alpha \cdot \beta = \max(\alpha,\beta)
		\]
	\textcolor{red}{eccetto il caso} $\alpha \cdot 0 = 0 \cdot \beta = 0$.
\end{lemma}

\begin{proof}
	Somme e prodotti di cardinalità finite sono finite (per il \hyperref[op_card_fin]{teorema}, e in questo caso l'enunciato del lemma è già soddisfatto perché
	nel caso di entrambe le cose finite ci interessa soltanto che tutte e tre le operazioni sopra diano cose finite, pertanto da ora possiamo assumere che una delle due 
	abbreviazioni non sia finita e procedere con la dimostrazione). Supponiamo quindi $\aleph_0 \leq \beta$ e, senza perdita di generalità, $\alpha < \beta$.
	Abbiamo:
	\begin{align*}
		&\beta = \beta + 0 \overset{\text{compatib. op. cardin.}}{\leq} \alpha + \beta \overset{\text{compatib. op. cardin. + Hp.}}{\leq} 2\beta = \beta \\
		&\beta = \beta \cdot 1 \overset{\text{compatib. op. cardin.}}{\leq} \alpha \cdot \beta \overset{\text{compatib. op. cardin + Hp.}}{\leq} \beta^2 = \beta
	\end{align*}
	dove l'ultima uguaglianza nel prodotto vale perché $\aleph_0^2 = \aleph_0$, e $2^{\aleph_0} \leq (2^{\aleph_0})^2 \leq (2^{\aleph_0})^{\aleph_0}  = 2^{\aleph_0}$ (quindi la cosa accade 
	per entrambi i possibili valori di $\beta$). Nel caso di $2\beta$, si osserva che $\aleph_0 \leq 2 \cdot \aleph_0 \leq \aleph_0 \cdot \aleph_0 = \aleph_0$ e $2^{\aleph_0} \leq 2 \cdot 2^{\aleph_0} \leq 2^{\aleph_0} \cdot 2^{\aleph_0} 
	= 2^{\aleph_0 + \aleph_0} = 2^{\aleph_0}$ (come al solito per le proprietà di compatibilità e dando per buone le disuguaglianze iniziali, che possono essere verificate scrivendo semplici mappe).\\
	Pertanto si conclude l'enunciato usando Cantor-Bernstein nella serie di disuguaglianze sopra, 
	che ci danno proprio la tesi (ricordando che avevamo scelto WLOG $\beta$ come massimo).
\end{proof}

\begin{lemma}[$\alpha^{\aleph_0} = 2^{\aleph_0}$]
	Se $2 \leq \alpha \leq 2^{\aleph_0}$\footnote{Per la disuguaglianza di \hyperref[cantor]{Cantor} nel mezzo c'è anche $\aleph_0$, dunque vale anche che $\aleph_0^{\aleph_0} = 2^{\aleph_0}$} allora $\alpha^{\aleph_0} = 2^{\aleph_0}$.
\end{lemma}

\begin{proof}
	È sufficiente osservare che:
	\[ 2^{\aleph_0} \leq \alpha^{\aleph_0} \leq (2^{\aleph_0})^{\aleph_0} \overset{\text{oss. sopra}}{=} 2^{\aleph_0}
		\]
	dove le disuguaglianze sono semplicemente l'ipotesi + \hyperref[compatibilità_operazioni_cardinalità]{l'osservazione sulla compatibilità} tra ordinamento e operazioni fra cardinalità (si conclude come al solito per \hyperref[CB]{Cator-Bernstein}).
\end{proof}

\subsection{Sottrarre un  numerabile dal continuo}
Ricordiamo un'osservazione riguardo al numerabile.

\begin{remark}[Numerabile - finito = numerabile]
	Sia $|A| = \aleph_0$ e $B \subseteq A$ con $|B|<\aleph_0$. Allora $|A \setminus B| = \aleph_0$.
\end{remark}

\begin{proof}
	Siccome $A \setminus B \subseteq A$, o $|A \setminus B| = \aleph_0$ o $|A \setminus B|<\aleph_0$ (cioè la sottrazione ci da $\omega$\footnote{Possiamo sempre assumere sia $\omega$ WLOG.} di un sottoinsieme ci dà 
	ancora un sottoinsieme di $\omega$, che quindi è al più numerabile e per una proposizione vista o è finito o è numerabile). Escludiamo che valga la seconda possibilità, se così fosse:
	\[ A = B \cup (A \setminus B)
		\]
	cioè un insieme numerabile è unione di insiemi finiti, dunque è finito\footnote{Per inclusione-esclusione $|A \cup B| \leq |A| + |B| = n + m \in \omega$.} che è assurdo\footnote{Volendo ogni cardinalità finita sta in $\omega$ [o un qualsiasi altro insieme numerabile], e quindi si immerge in lui, cosa che rende assurda l'uguaglianza trovata.}.
\end{proof}

Vale una proposizione analoga per $2^{\aleph_0}$.

\begin{lemma}[Continuo - al più numerabile = continuo]
	Sia $|A| = 2^{\aleph_0}$ e $B \subseteq A$ con $|B| \leq \aleph_0$, allora $|A \setminus B| = 2^{\aleph_0}$.
\end{lemma}

\begin{note}[Continuo - al più continuo (escluso) = continuo]
	Il lemma varrebbe anche rimpiazzando $|B| \leq \aleph_0$ con $|B| < 2^{\aleph_0}$, però, per ora, possiamo dimostrare solo l'asserto più debole sopra.
\end{note}

\begin{proof}
	Chiaramente $A\setminus B \subseteq A \implies |A\setminus B| \leq |A| = 2^{\aleph_0}$, basta quindi dimostrare la disuguaglianza opposta.
	Siccome $2^{\aleph_0}\cdot 2^{\aleph_0} = 2^{\aleph_0}$, esiste una bigezione:
	\[ f : A \rightarrow {}^{\omega}2 \times {}^{\omega}2
		\]
	sia $\pi : {}^{\omega}2 \times {}^{\omega}2 \rightarrow {}^{\omega}2 : (x,y) \mapsto x$ (è surgettiva ma non iniettiva). Siccome $B$ è al più numerabile:
	\[ |\pi \circ f[B]| \leq \aleph_0 < 2^{\aleph_0}
		\]
	in particolare $|f[B]| = |B| \leq \aleph_0$ perché $f$ bigezione, inoltre, essendo $f[B]$ al più numerabile e $\pi$ surgettiva, $\pi[f[B]]$ è al più numerabile (come visto nell'\hyperref[ex7.13]{esercizio}\footnote{$|f[B]| \leq \aleph_0$, $f[B] \overset{\pi}{\twoheadrightarrow} \pi[f[B]]$, quindi $|\pi[f[B]]| \leq \aleph_0$.}).
	\begin{figure}[H]
		\centering
		\includegraphics[width = 4cm]{immagini/continuo-numerabile.png}
	\end{figure}
	Quindi, in particolare $\pi \circ f[B] \ne {}^{\omega}2$. Possiamo quindi prendere $x \in {}^{\omega}2 \setminus \pi \circ f[B]$. Dire che $x \not \in \pi \circ f[B]$ significa
	che [le coppie con prima componente $x$ nel prodotto sono disgiunte da $f[B]$] $(\{x\} \times {}^{\omega}2) \cap f[B] = \emptyset$ (fondamentalmente, trovato l'$x$ in ${}^{\omega}2$,
	siamo tornati indietro con $\pi^{-1}$ \footnote{$\pi^{-1}(x) = \{x\} \times {}^{\omega}2$, 
	quindi $x \not\in \pi \circ f[B]$ diventa $\pi^{-1}(x) = (\{x\} \times {}^{\omega}2) \cap f[B] = \emptyset$, perché se l'intersezione fosse non vuota}).\\
	Quindi tornando indietro ad $A$ [via $f^{-1}$], $f^{-1}(\{x\} \times {}^{\omega}2) \cap B = \emptyset$, ossia $f^{-1}(\{x\} \times {}^{\omega}2) \subseteq A \setminus B$ [se 
	non sta in $B$ sta nel suo complementare in $A$], da cui $|f^{-1}(\{x\} \times {}^{\omega}2)| \leq |A \setminus B|$. Usando il fatto che $f$ è bigettiva:
	\[ |f^{-1}(\{x\} \times {}^{\omega}2)| \overset{\text{$f$ bigett.}}{=} |\{x\} \times {}^{\omega}2| = 1 \cdot 2^{\aleph_0} = 2^{\aleph_0}
		\]
	dunque abbiamo anche la disuguaglianza dal basso e quindi $|A \setminus B| = 2^{\aleph_0}$.
\end{proof}

Siamo finitamente pronti per dimostrare che $|\RR| = 2^{\aleph_0}$.

\begin{proof}
	Siccome $\RR \subseteq \ps(\QQ)$, la disuguaglianza $|\RR| \leq 2^{\aleph_0}$ è immediata. Per dimostrare la disuguaglianza opposta definiamo:
	\[ A \Mydef \{X \in \ps(\omega) | X \ne \emptyset \land |\omega \setminus X| \geq \aleph_0\}
		\]
	ossia i sottoinsiemi di $\omega$ non vuoti e \vocab{co-infiniti}.\\
	\textcolor{MidnightBlue}{\underline{Intuitivamente}: $X \in A$ rappresenta lo sviluppo in notazione binaria di un $x \in ]0,1[$ - $x = 0.a_1a_2a_3 \ldots$, $a_i = 1 \leftrightarrow i \in X$ - la condizione $X \ne \emptyset$ serve a escludere lo 0, la 
	condizione di co-infinitezza a escludere l'uno periodico.}\\
	Ci basta dimostrare che $|A| = 2^{\aleph_0}$ e che esiste $f : A \rightarrow \RR$ iniettiva. La prima cosa è facile:
	\[ A = \ps(\omega)\setminus (\{\emptyset\} \cup \underbrace{\{X \in \ps(\omega) : |\omega \setminus X|<\aleph_0\}}_{\Mydef S})
		\]
	L'insieme $S$ è in corrispondenza biunivoca con $\psf(\omega)$ tramite la funzione ``complementare rispetto a $\omega$'':
	\[ \ol \square : S \rightarrow \psf(\omega) : X \mapsto \ol X = \omega \setminus X
		\]
	Quindi $|S| = |\psf(\omega)| = \aleph_0$, e, di conseguenza\footnote{$|\{\emptyset\} \cup S| \leq |\emptyset| + |S| = 1 + \aleph_0 = \aleph_0$ e $S \subseteq (\{\emptyset\} \cup S)$, da cui formalmente
	si ottiene che l'unione è numerabile.} $|A| = |\ps(\omega)\setminus(\{\emptyset\}\cup S)| = 2^{\aleph_0}$, grazie al lemma precedente.
	Resta da costruire $f : A \rightarrow \RR$ iniettiva.\\
	Cominciamo col definire un ordine totale su $A$. Dati $X,Y \in A$ (cioè sottoinsiemi non vuoti e co-infiniti di $\omega$):
	\[ X <_A Y \Mydef \exists i \in \omega \; \underbrace{(i \cap X = i \cap Y)}_{\forall j \,<\, i \; j \,\in\, X \leftrightarrow j \,\in\, Y} \land \underbrace{(i \in Y \setminus X)}_{i \,\in\, Y\; \land\; i \,\not\in \,X}
		\]
	In altri termini, detto $i$ il minimo elemento della differenza simmetrica $X \Delta Y \Mydef (X \setminus Y) \cup (Y \setminus X)$ [che è ancora un sottoinsieme di $\omega$ per cui il minimo esiste], se $i \in Y$ - per cui, chiaramente, $i \not \in X$ - allora $X < Y$, se, invece $i \in X$ -
	per cui $i \not \in Y$ - allora $Y < X$ (in altri termini, presa la differenza simmetrica di due insiemi, chi dei due ha il minimo elemento è il maggiore). La verifica del fatto che questo è un ordine totale è immediata.\\
	Consideriamo $B \Mydef \psf(\omega) \setminus \{\emptyset\} \subseteq A$. Chiaramente $|B| = \aleph_0$. Dimostriamo ora che $B$ è denso in $A$.\\
	Dati $X,Y \in A$, con $X < Y$, sia $i := \min X \Delta Y$ (che c'è per quanto appena scritto e in particolare sta in $Y$) e $j > i$ minimo tale che $j \not \in X$, che esiste perché $X$ è co-infinito. Sia $Z \Mydef (X \cap j) \cup \{j\}$ (ricordiamo che siamo in $\omega$,
	quindi stiamo togliendo da $X$ tutte le cose maggiori o uguali a $j$ e poi stiamo riaggiungendo $j$),
	$Z$ è finito [perché intersezione con $j\in \omega$ + unione con singoletto], quindi appartiene a $B$ per definizione. Inoltre $j$ è il minimo elemento di $X \Delta Z$ [l'abbiamo preso come il più piccolo non in $x$ e poi lo abbiamo aggiunto a $Z$] e $j \in Z$, quindi [per come è definito l'ordine] $X < Z$, e, similmente,
	$i$ è il minimo di $Z \Delta Y$ e $i \in Y$, quindi di nuovo si ha $Z < Y$, pertanto $X<Z<Y$.
	\begin{figure}[H]
		\centering
		\includegraphics[width = 7.0cm]{immagini/cardinalità_R.png}
	\end{figure}
	Stabilito che $B$ è denso in $A$, $B$ è, in particolare, denso [numerabile e naturalmente illimitato rispetto all'ordine dato ad $A$\footnote{Segue dal fatto che $\omega$ è illimitato.}], quindi, c'è un isomorfismo di ordini $g : B \rightarrow \QQ$. Ora, siccome, nuovamente, $B$ è denso in $A$,
	la funzione:
	\[ h : A \rightarrow \; \text{sezioni [principali] su $B$} : X \mapsto B_X = \{Y \in B | Y < X\}
		\]
	è iniettiva [e sezioni di $B = \RR$]. Quindi $f : A \rightarrow \RR : X \mapsto g[h(X)]$ è una funzione iniettiva da $A$ a $\RR$ (per essere formali dovremmo comporre anche $\iota$ alla fine, per quanto osservato sul fatto che $\QQ\subseteq \RR$).
\end{proof}










\newpage
\section*{Stato del corso}
\addcontentsline{toc}{section}{Stato del corso}
È un dato di fatto - il primo teorema di incompletezza di Gödel - che ogni teoria \vocab{calcolabile} - i cui assiomi possano, cioè, essere elencati 
in maniera meccanica - è necessariamente incompleta. L'incompletezza non è quindi un difetto, o meglio, che lo sia oppure no è irrilevante, perché 
non può essere evitata.\\
Tuttavia, gli assiomi che abbiamo introdotto fino ad ora lasciano aperte lacune che sarebbe desiderabile colmare.

\begin{enumerate}[1.]
	\item Sarebbe ragionevole che questi insiemi esistessero [all'interno della teoria che stiamo costruendo]:
	\[ \{\emptyset, \{\emptyset\}, \{\{\emptyset\}\}, \{\{\{\emptyset\}\}\}, \ldots\}
		\]\[ \{\omega, s(\omega), s(s(\omega)), s(s(s(\omega))),\ldots\}
			\]
	Però gli assiomi 1-7 non bastano né per dimostrarne l'esistenza, né - e questo sarebbe disastroso - permettono di escluderla.
	\item Alcune questioni sulle cardinalità, come per esempio la confrontabilità, non possono essere decise sulla base dei solo assiomi 1-7.
	Inoltre ci mancano risultati desiderabili per via delle applicazioni, segnatamente il lemma di Zorn.
	\item Vi sono insiemi la cui esistenza vorremmo escludere. Per esempio vorremmo che l'equazione $X = \{X\}$ non avesse soluzioni, e farebbe comodo escludere 
	l'esistenza di qualcosa del tipo $Y = \{\{\{\{\{\ldots\}\}\}\}\}$ con infinite parentesi annidate. Il guaio qui non è grave, ma questi oggetti contraddicono, in parte, l'intuizione che vorremmo 
	concretizzare negli assiomi della teoria degli insiemi. Noi vorremmo \textbf{che un insieme fosse identificabile dalla sua struttura}. Mi spiego, per esempio $\emptyset$ è identificato dal fatto di non avere elementi,
	$\{\emptyset\}$ è identificato dal fatto di avere un solo elemento che non ha elementi etc. per tutti gli insiemi che conosciamo, ma cosa dire di $Y$? $Y$ ha un elemento $Y_1$, che ha un elemento $Y_2$,
	che ha \ldots e la stessa descrizione si potrebbe applicare anche a $Y_1$, e anche a $Y_2$ \ldots Sono tutti uguali? 
\end{enumerate}

Queste tre lacune saranno colmate dai tre assiomi che ancora ci mancano: rispettivamente l'assioma del rimpiazzamento, l'assioma della scelta e l'assioma di buona fondazione. La teoria risultante sarà,
inevitabilmente, incompleta - per esempio non decide il problema del continuo: l'esistenza di cardinalità intermedie fra $\aleph_0$ e $2^{\aleph_0}$ - ma è la fondazione meglio accettata della matematica.






\newpage
\section{I buoni ordinamenti}
Il nostro prossimo obiettivo è definire e studiare la classe dei \vocab{numeri ordinali}. Questa può essere pensata come la più vasta classe
- dotata di un ordinamento totale definito per mezzo di una formula - su cui sia corretto ragionare per induzione forte. Conteremo, quindi, sugli ordinali per formulare 
l'induzione e la ricorsione transfinita, procedimenti che superano la forza dimostrativa dell'induzione e della ricorsione aritmetica - per esempio permettendo di ottenere 
il teorema di Cantor-Lebesgue sugli insiemi di unicità.\\
Siccome l'induzione forte equivala al principio del minimo, studieremo i buoni ordini. In questa sezione, dimostreremo il risultato seguente.

\begin{theorem}[Tutti i buoni ordini sono ``totalmente ordinati'' fra loro]
	Siano $(A,<_A)$ e $(B,<_B)$\footnote{Nel seguito scriveremo semplicemente $(A,<)$ e $(B,<)$ per comodità.} insiemi bene ordinati, allora vale \textcolor{red}{una e una sola} delle seguenti:
	\begin{itemize}
		\item $(A,<_A)$ è isomorfo a un segmento iniziale proprio di $(B,<_B)$
		\item $(A,<_A)$ e $(B,<_B)$ sono isomorfi
		\item $(B,<_B)$ è isomorfo a un segmento iniziale di $(A,<_A)$
	\end{itemize}
\end{theorem}

Di fatto stiamo creando un'ordinamento totale tra buoni ordini con questo teorema, se definiamo:
\[ (A,<_A) \prec (B,<_B) \Mydef \exists C \; \text{$C$ segmento iniziale proprio di $(B,<_B)$ e} \; (A,<_A) \sim C
	\]
allora $\prec$ soddisfa le \textcolor{red}{proprietà formali di un ordinamento totale fra le classi di isomorfismo di buoni ordini}.
Definiamo altresì l'ordine largo associato:
\[ (A,<_A) \preceq (B,<_B) \Mydef ((A,<_A) \prec (B,<_B)) \lor ((A,<_A) \sim (B,<_B))
	\]
ossia ``$(A,<_A)$ è isomorfo a un segmento iniziale [proprio o meno] di $(B,<_B)$''.\\
Richiamiamo le definizioni fondamentali.

\begin{definition}[Buon ordinamento]
	$(A,<)$ è un \vocab{buon ordinamento} se ogni $B \subseteq A$ non vuoto ha un minimo elemento.
\end{definition}

\begin{definition}[Segmento inziale]
	Dato un ordine totale $(A,<)$, $B \subseteq A$ è un \vocab{segmento inziale} se [assorbe gli elementi più piccoli] $\forall b \in B \; \forall x \in A \; x < b \rightarrow x \in B$.
\end{definition}

\begin{definition}[Segmenti iniziali propri e principali]
	Il segmento iniziale $B$ è \vocab{proprio} se $B \ne A$. Il segmento iniziale $B$ è \vocab{principale} se [è della forma]:
	\[ B = A_a \Mydef \{x \in A | x < a\}
		\]
	per qualche $a \in A$, e, in questo caso, si dice  che è un \vocab{segmento iniziale principale determinato da $a$}.
\end{definition}

È chiaro che un segmento iniziale principale, $A_a$, è \underline{sempre} proprio, perché $a \not \in A_a$, e nel caso dei buoni ordini questa è una doppia implicazione (quindi se è proprio è 
anche principale).

\begin{proposition}[proprio $\implies$ principale nei buoni ordini]
	Ogni segmento iniziale proprio di un buon ordine è principale.
\end{proposition}

\begin{proof}
	Sia $(A,<)$ ben ordinato e $I \subsetneq A$ un segmento iniziale proprio. Consideriamo $a := \min_<(A \setminus I)$ (per l'ipotesi di buon ordinamento il minimo c'è). Allora $I = A_a$ (ovvero 
	il nostro segmento iniziale proprio è principale determinato da $a$).\\
	\textcolor{MidnightBlue}{Verifiche: vediamo i due contenimenti, $x \in A_a \overset{\text{def.}}{\implies} x < a \overset{\text{$a$ min. in $A\setminus B$}}{\implies} x \not \in A \setminus I \implies x \in I$
	(cioè se $x < a$, poiché $a$ è il minimo che sta nel complementare di $I$ rispetto ad $A$, $x$ che è più piccolo non può soddisfare la proprietà e quindi non sta nel complementare aka sta in $I$), dunque $A_a \subseteq I$.\\
	Viceversa, supponiamo per assurdo $x \in I$ e $x \not \in A_a$, la seconda equivale ad $a \leq x$ (per definizione di segmento iniziale 
	principale), ma allora, siccome $x \in I$, per definizione di segmento iniziale $a \in I$, ma per definizione $a$ era il minimo in $A \setminus I \implies$ non poteva essere in $I$, dunque assurdo, quindi $x \in I \implies x \in A_a$, da cui $I \subseteq A_a$.}
\end{proof}

\begin{exercise}[Buon ordine $\iff$ (proprio $\implies$ principale)]
	Dimostra che la proposizione precedente caratterizza i buoni ordini. Più precisamente, dato un ordine totale $(A,<)$, se ogni segmento iniziale proprio di $A$
	è principale, allora $A$ è bene ordinato da $<$.
\end{exercise}

\begin{soln}
	La proposizione appena vista ci fornisce già $\Longrightarrow$, dunque non ci resta che dimostrare la freccia opposta, ovvero se vale la proposizione su un ordine totale $(A,<)$, allora questo è un buon ordine.
	Sia $B \subseteq A$, $B \ne \emptyset$, vogliamo vedere che ha un minimo, $\forall x \in B$ sia $B_x$ il segmento iniziale principale determinato da un elemento di $B$, consideriamo:
	\[ \bigcap_{x \in B} B_x \,\footnote{Ricordiamo che: $\bigcap_{x \in B} B_x = \bigcap \{B_x | x \in B\}$.}
		\]
	osserviamo che l'intersezione di segmenti iniziali è ancora un segmento iniziale [ogni $x$ nell'intersezione sta in tutti i segmenti iniziali, quindi vale la solita proprietà], inoltre, tale segmento iniziale è necessariamente proprio
	(infatti, se ci sono almeno due elementi in $B$ l'intersezione dei segmenti iniziali principali taglia fuori l'elemento più grande), dunque \textbf{per ipotesi}, l'intersezione è un segmento iniziale principale. Sia $m \in B$ l'elemento tale che:
	\[ B_m = \{x \in B | x < m\} = \bigcap_{x \in B} B_x
		\]
	verifichiamo che $m$ è il minimo di $B$ [aka $B_m = \emptyset$]. Supponiamo per assurdo che esista $y < m$, ovvero $y \in B_m = \bigcap_{x \in B} B_x$, ciò equivale a $y < x$, $\forall x \in B$, compreso $y$ stesso, si ottiene cioè $y < y \; \lightning$.
	Dunque $m$ è il minimo e $B_m = \emptyset$.
\end{soln}

\begin{remark}[Finto buon ordine]
	In $\ZZ$ ogni segmento iniziale proprio è principale, come accade in $\omega$, tuttavia $\ZZ$ non è buon ordine. Ciò apparentemente contraddirebbe quanto appena dimostrato,
	tuttavia non è così, infatti, come visto nella dimostrazione sopra il vuoto è un segmento iniziale proprio, che in $\omega$ è principale [corrisponde a $\omega_0$], mentre in $\ZZ$ non c'è un elemento che lo determini come 
	segmento iniziale principale (pur essendo proprio), da ciò si vede che l'implicazione proprio $\implies$ principale, non si verifica in $\ZZ$, che non è un buon ordine, come già sapevamo. 
\end{remark}

\begin{lemma}[Le funzioni crescenti di un buon ordine stanno sopra la diagonale]
	Sia $(A,<)$ un buon ordinamento e $f : A \rightarrow A$ una funzione \textcolor{red}{strettamente} crescente - $\forall x,y \in A \; x < y \rightarrow f(x) < f(y)$ -, allora $\forall x \in A \; x \leq f(x)$.
\end{lemma}

\begin{proof}
	Per assurdo, assumiamo la negazione della tesi, $\exists x \in A \; x > f(x)$. Quindi l'insieme $B = \{x \in A | f(x) < x\}$ non è vuoto. Sia $k := \min B$.
	Allora $f(k) < k$ (perché elemento di $B$), e, siccome $f$ è crescente $f(\textcolor{red}{f(k)}) < \textcolor{red}{f(k)}$, per cui $f(k) \in B$ a sua volta (è [strettamente] più grande della sua immagine),
	e, ricordando che per ipotesi $f(k)<k$, contraddice la minimalità di $k$ e ci dà un assurdo.
\end{proof}

\begin{corollary}[Proprietà degli isomorfismi tra buoni ordinamenti]
	Valgono le seguenti:
	\begin{enumerate}[(1)]
		\item Un buon ordinamento \textcolor{red}{non} è isomorfo a un suo segmento iniziale proprio. \textcolor{orange}{(irriflessività)}
		\item L'identità è il solo isomorfismo fra un buon ordinamento e se stesso.
		\item Se $(A,<)$ e $(B,<)$\footnote{Ricordare che quelli sono $<_A$ e $<_B$.} sono buoni ordini isomorfi allora esiste un unico isomorfismo fra di essi.
	\end{enumerate}
\end{corollary}

\begin{proof}
	Dimostriamo singolarmente gli enunciati:
	\begin{enumerate}[(1)]
		\item Supponiamo che $(A,<)$ sia isomorfo al suo segmento iniziale proprio $A_a$, ordinato - si intende - dalla restrizione di $<$, e sia $f : A \rightarrow A_a$ un isomorfismo.
		Allora $f$ è crescente per definizione di isomorfismo. Tuttavia $f(a) < a$, poiché in arrivo $f(a) \in A_a$, contraddicendo il lemma sopra, quindi abbiamo un assurdo.
		\item Sia $f : A \rightarrow A$ un automorfismo del buon ordine $(A,<)$, dobbiamo dimostrare che $f = \id_A$. Se così non fosse, ci sarebbe almeno un $x \in A$ tale che $f(x) \ne x$.
		Se $f(x) < x$ stiamo contraddicendo il lemma perché $f$ deve essere crescente (in quanto isomorfismo di ordini). Se $x < f(x)$, vale la stessa considerazione di prima con $f^{-1}$, e quindi di nuovo un assurdo.
		\item Se $f : A \rightarrow B$ e $g : A \rightarrow B$ fossero due isomorfismi diversi, allora $g^{-1} \circ f : A \rightarrow A$ sarebbe un automorfismo di $A$ diverso dall'identità,
		contraddicendo il punto (2).	
	\end{enumerate}
\end{proof}

\begin{remark}[Transitività della ``relazione d'ordine'' tra buoni ordini]
	Siano $(A,<)$, $(B,<)$, $(C,<)$ buoni ordini. Allora:
	\[ (A,<) \preceq (B,<) \land (B,<) \preceq (C,<) \rightarrow (A,<) \preceq (C,<)
		\]
\end{remark}

\begin{proof}
	Siano $f : A \rightarrow B$ e $g : B \rightarrow C$ isomorfismi fra $A$ e un segmento iniziale di $B$ e fra $B$ e un segmento iniziale di $C$ rispettivamente.
	Dimostriamo che $g \circ f : A \rightarrow C$ è un isomorfismo fra $A$ e un segmento iniziale di $C$ [non necessariamente proprio]\footnote{Una definizione alternativa ed equivalente di minore o uguale tra buoni ordini, rispetto a quella data all'inizio, è 
	che un buon ordine sia isomorfo ad un segmento iniziale non necessariamente proprio dell'altro.}. Si ha che $g \circ f$ è crescente in quanto composizione di funzioni crescenti.
	Occorre verificare che $g \circ f[A]$ è un segmento iniziale di $C$.\\
	\textcolor{MidnightBlue}{Verifica: sia $g(f(a))$ un qualunque elemento di $g \circ f[A]$, se sia $x < g(f(a))$, dobbiamo verificare [per avere la definizione di segmento iniziale] che $x \in g \circ f[A]$.
	$g(f(a)) \in g[B]$ e [per ipotesi] $g[B]$ è segmento iniziale di $C$, quindi $g[B] \ni g(f(a)) > x \in g[B]$. Scriviamo $x = g(y)$. Ora, siccome $g$ è un isomorfismo, da $x < g(f(a))$ deduciamo $y < f(a) \in f[A]$.
	Siccome $f[A]$ è segmento iniziale di $B$, segue che $y \in f[A]$, quindi possiamo scrivere $y = f(z)$, per qualche $z \in A$. In definitiva abbiamo quindi $x = g(f(z)) \in g \circ f[A]$.}
\end{proof}

\begin{remark}[Antisimmetria della ``relazione d'ordine'' sui buoni ordini]
	Siano $(A,<)$ e $(B,<)$ buoni ordini, allora:
	\[ (A,<) \preceq (B,<) \land (B,<) \preceq (A,<) \rightarrow (A,<) \sim (B,<)
		\]
	dunque vale la proprietà antisimmetrica\footnote{I buoni ordini sono una classe, non un'insieme, dunque la relazione $\preceq$ (o $\prec$), volendo, è una relazione d'ordine su una classe, non su un insieme.}
\end{remark}

\begin{proof}
	Siano $f : A \rightarrow B$ e $g : B \rightarrow A$ isomorfismi fra $A$ e un segmento iniziale di $B$ e fra $B$ e un segmento iniziale di $A$.
	Ricordando la dimostrazione dell'osservazione precedente, $g \circ f$ è un isomorfismo fra $A$ e un \textbf{segmento iniziale} $g \circ f[A]$ di $A$. Ma per l'(1) del corollario 
	$g \circ f[A]$ non può essere un segmento iniziale \textbf{proprio}, quindi [deve essere tutto $A$] $g \circ f[A] = A$. Ma allora, per il (2) del medesimo corollario, $g \circ f = \id_A$.\\
	Ragionando simmetricamente $f \circ g = \id_B$, quindi $f$ è un isomorfismo fra $A$ e $B$, con inversa $g$.
\end{proof}

Possiamo finalmente passare alla dimostrazione d'ordine del teorema.

\begin{theorem}[Totalità della ``relazione d'ordine'' sui buoni ordini]
	Siano $(A,<)$ e $(B,<)$ insiemi ben ordinati, allora vale \textcolor{red}{una e una sola} delle seguenti:
	\[ (A,<) \prec (B,<) \qquad (A,<) \sim (B,<) \qquad (B,<) \prec (A,<)
		\]
\end{theorem}

\begin{wrapfigure}[12]{l}{2.82cm}
	\includegraphics[width = 2.80cm]{immagini/ordine_totale_buoni_ordini.png}
\end{wrapfigure}

\textcolor{MidnightBlue}{\underline{Idea}: Il teorema ci dice che vale al più una delle alternative, quindi la difficoltà risiede nel dimostrare che una si verifica. Molto vagamente potremmo ragionare così.
Identifichiamo, progressivamente, segmenti iniziali sempre più lunghi di $A$ e $B$. All'inizio identifichiamo il minimo di $A$ con il minimo di $B$, poi il secondo elemento di $A$ con il secondo elemento di $B$, etc. Fatti $\omega$
passaggi avremo identificato un segmento iniziale di $A$, diciamo $A_x$, isomorfo a $\omega$, con un $B_y$, anch'esso ovviamente isomorfo a $\omega$. Bene: continuiamo identificando $x$ con $y$. Quando potrebbe bloccarsi il procedimento?
Solo se, ad un certo punto, abbiamo identificato interamente uno dei due insiemi, con un segmento iniziale dell'altro - perché altrimenti, abbiamo identificato due segmenti iniziali $A_x$ e $B_y$ e possiamo continuare attaccando $x$ a $y$. È 
come la chiusura di una cerniera lampo : ad ogni istante c'è un prossimo dente.\\
\textcolor{red}{Questa discorso, però, non è una dimostrazione.} Se vogliamo, sarebbe un tentativo di costruire l'isomorfismo cercato per \textcolor{purple}{ricorsione transfinita}. Il guaio è che i numeri che permetterebbero di numerare i passaggi della costruzione,
gli \textcolor{purple}{ordinali}, sono appunto l'oggetto che stiamo tentando di costruire.}

\begin{proof}
	Per il teorema\footnote{Typo di Mamino.}, si può verificare al più una delle tre condizioni. Consideriamo ora $f$ definita come segue:
	\[ f = \{(a,b) \in A \times B | A_a \sim B_b\}
		\]
	Vogliamo dimostrare che $f$ è una funzione crescente, che $\Dom(f)$ è un segmento iniziale di $A$, e che $\Imm(f)$ è un segmento iniziale di $B$ (cioè $f$ manda segmenti iniziali in segmenti iniziali). Quindi $f$ è un isomorfismo fra un segmento iniziale di $A$ e uno di $B$.
	Infine dimostriamo che $\Dom(f) = A$ o $\Imm(f) = B$, e questo conclude la dimostrazione (perché se si verifica una delle due o tutte e due, abbiamo ottenuto la tesi del teorema). Procediamo ora con tutte le verifiche.
	\begin{itemize}
		\item[$\boxed{\text{$f$ è una funzione}}$] \textcolor{MidnightBlue}{Supponiamo per assurdo $(a,b) \in f$ e $(a,b') \in f$ con $b \ne b'$. Senza perdita di generalità supponiamo $b < b'$ (quindi $B_b$ s.i. proprio di $B_{b'}$), e, per la definizione data di $f$ ciò corrisponde a:
		\[ B_b \sim A_a \sim B_{b'}
			\]
		dunque $B_{b'}$ sarebbe isomorfo al suo segmento iniziale proprio $B_b$ $\lightning$ (a causa dell'(1) del corollario).}
		\item[$\boxed{\text{$f$ è crescente}}$] \textcolor{MidnightBlue}{Dati $a,a' \in A$, con $a<a'$, dobbiamo dimostrare $f(a) < f(a')$. Supponiamo, per assurdo $f(a') \leq f(a)$, abbiamo allora:
		\[ A_{a'} \sim B_{f(a')} \preceq B_{f(a)} \sim A_a
			\]
		dove i due isomorfismi, vengono semplicemente dalla definizione di $f$ (cioè manda s.i. in s.i. [isomorfi] in arrivo), e $B_{f(a')} \preceq B_{f(a)}$ segue da $B_{f(a')} \subseteq B_{f(a)}$, che vale perché stiamo supponendo $f(a') \leq f(a)$ per ipotesi assurda.\\
		Abbiamo quindi che $A_{a'}\preceq A_a$ [$\implies A_{a'}\subseteq A_a \implies a' \leq a$] che è assurdo perché $A_a$ è un segmento iniziale proprio di $A_{a'}$ [poiché avevamo assunto $a<a'$].}
		\item[$\boxed{\text{$\Dom(f)$ è s.i. di $A$}}$] \textcolor{MidnightBlue}{Sia $a \in \Dom(f)$ e $a' < a$, vogliamo dimostrare che $a' \in \Dom(f)$ (che quindi è un segmento iniziale). L'ipotesi $a \in \Dom(f)$ equivale a dire che esiste $b \in B$ tale che $A_a \sim B_b$,
		quindi, in particolare, $A_a \preceq B_b$ (per la definizione di $f$ abbiamo l'isomorfismo e per l'osservazione sull'antisimmetria possiamo indebolire la cosa a disuguaglianza).
		Da $a'<a$ segue come al solito che $A_{a'} \subsetneq A_a$, quindi [per definizione di $\prec$] $A_{a'} \prec A_a$.\\
		Per transitività abbiamo quindi $A_{a'}\prec B_b$ e, siccome ogni segmento iniziale [proprio] è principale [in un buon ordine], esiste $b' \in B_b$, tale che $A_{a'} \sim (B_b)_{b'}$ (stiamo usando la definizione di $\prec$).
		Si conclude osservando che $(B_b)_{b'} \sim B_{b'}$ (basta verificare i due contenimenti banali), quindi $A_{a'}\sim B_{b'} \iff f(a') = b' \iff (a',b') \in f \implies a' \in \Dom(f)$.}
		\item[$\boxed{\text{$\Imm(f)$ è s.i. di $B$}}$] \textcolor{MidnightBlue}{Dimostrazione simmetrica alla precedente.}
		\item[$\boxed{\begin{aligned}\text{$\Dom(f) = A$} \\ \text{o $\Imm(f) = B$}\end{aligned}}$] \textcolor{MidnightBlue}{Se così non fosse, per la terza verifica vista, $\Dom(f) = A_a$ e per la penultima $\Imm(f) = B_b$ \footnote{Stiamo negando un OR quindi
		l'unica possibilità è che siano entrambe false, dunque, visto quanto verificato sopra, abbiamo ottenuto che sono entrambi segmenti iniziali propri e quindi principali.}, per opportuni $a \in A$ e $b \in B$. Per la seconda verifica $f$ è crescente, quindi è un isomorfismo fra $\Dom(f) = A_a$ e $\Imm(f) = B_b$.
		Ma allora, per definizione di $f$, $A_a \sim B_b$, cioè $(a,b) \in f$. Quindi $a \in \Dom(f) = A_a \; \lightning$ (o anche $b \in \Imm(f) = B_b \; \lightning$).}
	\end{itemize}
\end{proof}

\begin{exercise}[Ogni sottoinsieme proprio di un buon ordine è isomorfo a un s.i. non necessariamente proprio]
	Sia $(A,<)$ un buon ordine e sia $B \subsetneq A$. Dimostra che $B \preceq A$, ma non necessariamente $B \prec A$.
\end{exercise}

\begin{soln}
	Osserviamo che $(B,<_{|B})$ è un buon ordine [eredita la totalità di $<$ e tutti i sottoinsiemi di $B$ sono anche sottoinsiemi di $A$, dunque c'è sempre un minimo],
	segue che, per il teorema precedente, $(B,<_{|B})$ è isomorfo a un segmento iniziale di $A$, ma non necessariamente proprio,
 	infatti nel caso di cardinalità infinita $B \subsetneq A \notimplies |B|<|A|$\footnote{Cioè non vale la Dedekind-finitezza.}, dunque soltanto $|B|\leq|A|$ (quindi il segmento iniziale in arrivo potrebbe anche essere proprio), e concludiamo [potendo la cardinalità anche essere la stessa che] $B \preceq A$.
\end{soln}

\begin{exercise}
	Sia $(A,<_A)$ un ordine totale con $A = \bigcup S$. Supponiamo che:
	\begin{enumerate}[1.]
		\item ogni $X \in S$ è un buon ordine con la restrizione $<_{A|X}$
		\item per ogni $X,Y \in S$, o $X$ è segmento iniziale di $Y$ o $Y$ è segmento iniziale di $X$
	\end{enumerate}
	Dimostra che allora $(A,<_A)$ è un buon ordine. Esibisci inoltre un controesempio eliminando la condizione 2.
\end{exercise}

\begin{soln}
	
\end{soln}

\subsection{Operazioni aritmetiche fra buoni ordinamenti}
Per ora, non abbiamo visto molti esempi di buoni ordini. Le operazione definite in questa sezione forniscono una prima sorgente di esempi concreti.
Nel seguito del corso, vedremo buoni ordini assai più versatile di quelli ottenibili con queste operazioni.

\begin{definition}[Somma di ordini totali]
	Dati $(A,<_A)$ e $(B,<_B)$ ordini totali. Definiamo la \vocab{somma di ordini totali} come:
	\[ (A,<_A) + (B,<_B) \Mydef (A \sqcup B, <_+)
		\]
	dove, ricordiamo che $A \sqcup B = (A \times\{0\}) \cup (B \times \{1\})$, e $<_+$ è definito da:
	\[ \begin{split}
		(x,y) <_+ (x',y') &\Mydef (y = 0 \land y' = 1) \\
						  &\lor (y = 0 \land y' = 0 \land x <_A x') \\
						  &\lor (y = 1 \land y' = 1 \land x <_B x')
	\end{split}
		\]
\end{definition}

\textcolor{MidnightBlue}{L'idea è che $(A,<_A) + (B,<_B)$ si ottiene attaccando $(B,<_B)$ in coda a $(A,<_A)$}.

\begin{figure}[H]
	\centering
	\includegraphics[width = 7.5cm]{immagini/somma_ordini_totali.png}
\end{figure}

Riproponiamo, per completezza, la definizione di prodotto lessicografico.

\begin{definition}[Prodotto di lessicografico]
	Siano $(A,<_A)$ e $(B,<_B)$ ordini totali. Definiamo il \vocab{prodotto del lessicografico}:
	\[ (A,<_A) \cdot (B,<_B) \Mydef (A \times B, <_\times)
		\]
	dove $<_\times$ è definito da:
	\[ (x,y) <_\times (x',y') \Mydef (y <_B y') \land (y = y' \land x <_A x')
		\]
\end{definition}

\textcolor{MidnightBlue}{L'idea di confrontare prima la seconda componente, deriva dal fatto che $(A,<_A) \cdot (B,<_B)$ sono 
tante copie di $(A,<_A)$ giustapposte, quanti sono gli elementi di $B$ (e quindi associate nello stesso ordine)}.

\begin{figure}[H]
	\centering
	\includegraphics[width = 10.5cm]{immagini/ordine_lessicografico.png}
\end{figure}

Per definire l'esponenziale ci serve la nozione di supporto.

\begin{definition}[Supporto di una funzione a un buon ordine]
	Dato un buon ordine $(B,<)$ e $f : A \rightarrow B$, il \vocab{supporto} di $f$ è:
	\[ \supp_B(f) \Mydef \{x \in A | f(x) \ne \min_{<_B} B\}
		\]
	(ometteremo il pedice $B$ quando è chiaro cosa sia $B$).
\end{definition}

\textcolor{MidnightBlue}{Il supporto è quindi l'insieme dei punti sull'insieme [qualsiasi] di partenza, sui quali $f$ verso un buon ordine non assume il minimo di quest'ultimo.}

\begin{definition}[Esponenziali di ordini totali]
	Dati $(A,<_A)$ e $(B,<_B)$ ordini totali, definiamo l'\vocab{esponenziale di ordini totali}:
	\[ (A,<_A)^{(B,<_B)} \Mydef (\{f \in {}^{B}A \, : \, |\supp_A f| < \aleph_0\},<_{\exp})
		\]
	dove l'insieme è quello delle funzioni a supporto finito (quindi che su un numero finito di punti non assumono il valore $\min_{<_B}B$), e l'ordine $<_{\exp}$ è definito da:
	\[ f <_{\exp} g \Mydef (f \ne g) \land (f(m) <_A g(m))
		 \]
	dove $m$ è il massimo valore in $B$ su cui $f$ e $g$ sono diverse [dunque stiamo confrontando l'immagine dell'elemento massimo (nell'unione dei supporti) su cui non sono uguali], $m := \max_{<_B}\{x \in B | f(x) \ne g(x)\}$.\footnote{In particolare si ha che le funzioni coincidono (in quanto a supporto finito),
	$\forall n \in B \; m <_B n \rightarrow f(n) = g(n)$, dunque stiamo confrontando l'ultimo valore su cui differiscono, e prendendo il massimo.}
\end{definition}

\textcolor{MidnightBlue}{L'idea è che una funzione $B \rightarrow A$ può essere vista come una specie di tupla con tante componenti quanti sono gli elementi di $B$\footnote{D'altronde abbiamo visto che $|{}^{B}A| = |A|^{|B|}$, il che ci fa notare che la definizione data di insieme di funzioni come una sorta di esponenziazione di un
insieme ad un altro, è coerente con quella di esponenziazione come prodotto [cartesiano] ripetuto un numero di volte pari alla cardinalità dell'esponente, da qui l'identificazione di ${}^{B}A$ con $\underbrace{A \times \ldots \times A}_{\text{$|B|$ volte}}$, che ci dà l'intuizione descritta (e che formalmente si traduce nell'insieme di funzioni).}.
Ordinare queste tuple lessicograficamente significa che vince la componente diversa più a destra [se definitivamente c'è il minimo in entrambe le tuple (per la finitezza del supporto non può essere diversamente), basta confrontare l'ultima componente dove sono diverse (questa cosa corrisponde a dire che entrambe le funzioni fanno definitivamente il
minimo)], ossia quella corrispondente all'elemento di $B$ più grande (morale: vince chi fa di più prima che $f$ e $g$ siano definitivamente uguali).}

\begin{exercise}
	Verificare che $(\omega,<)^{(\omega,<)} \sim (\NN[x],\prec)$, dove $\NN[x]$ denota l'insieme dei polinomi a coefficienti in $\NN$, e definiamo:
	\[ p \prec q \Mydef \exists N \in \NN \; \forall x \in \NN \; x > N \rightarrow p(x) < q(x)
		\]
	(ossia $p \prec q$ se $p(x)<q(x)$ da un certo punto in poi).
\end{exercise}

\begin{soln}
	
\end{soln}

\begin{proposition}[Somma, prodotto ed esponenziale di buoni ordini è un buon ordine]
	Se $(A,<_A)$ e $(B,<_B)$ sono buoni ordini, allora anche:
	\[ (A,<_A) + (B,<_B) \qquad (A,<_A) \cdot (B,<_B) \qquad (A,<_A)^{(B.<_B)}
		\]
	sono buoni ordini.
\end{proposition}

\begin{proof}
	Si tratta di banali verifiche, \textcolor{red}{eccetto la terza}.\\
	La relazione $<_{\exp}$ è irriflessiva per definizione [richiede $f \ne g$, dunque se sono uguali non sono in relazione]. Occorre, intanto, verificare la transitività.
	Assumiamo $f <_{\exp} g$ e $g <_{\exp} h$ [naturalmente $f,g \in {}^BA$] con:
	\begin{align*}
		&m_1 = \max_{<_B}\{x\in B | f(x) \ne g(x)\} \\
		&m_2 = \max_{<_B}\{x\in B | g(x) \ne h(x)\} \\
		&m_3 = \max_{<_B}\{x\in B | f(x) \ne h(x)\} \\
	\end{align*}
	Detto $m := \max(m_1,m_2)$, abbiamo [per ipotesi abbiamo le due disuguaglianze con $<_{\exp}$ e stiamo usando il massimo tra i due che rendono vere le disuguaglianze] $f(m) \leq_A g(m) \leq_A h(m)$, dove la prima disuguaglianza è
	stretta se $m = m_1$, e la seconda se $m = m_2$ (sempre per la definizione di $<_{\exp}$).  In ogni caso, almeno una delle due disuguaglianze è stretta e quindi abbiamo $f(m) <_A h(m)$.
	D'altro canto, se $m  <_B x$, allora $f(x) = g(x) = h(x)$ (cioè le funzioni, essendo a supporto finito, sono definitivamente costanti), quindi $m = m_3$ [cioè $m$ è proprio il più grande valore per cui $f<h$ prima che diventino definitivamente costanti, ovvero $m_3$] e la transitività è dimostrata.\\
	La relazione $<_{\exp}$ è un ordine totale perché, se $f \ne g$ [allora per definizione di ordine stretto totale vogliamo una delle due disuguaglianze strette], allora esiste $m = \max_{<_B}\{x \in B | f(x) \ne g(x)\}$, e o $f(m)<_A g(m)$ o $g(m) <_A f(m)$. Nel primo caso $f <_{\exp} g$, nel secondo $g <_{\exp} f$ (quindi prese due funzioni distinte, essendo entrambe a supporto finito, il sottoinsieme di $B$
	su cui sono distinte è non vuoto, ed in particolare ha un massimo [è un insieme finito e $B$ è totalmente ordinato], per tale massimo si verifica necessariamente una delle due disuguaglianze strette, pertanto $f$ e $g$ sono necessariamente in una delle due relazioni).\\
	Resta da verificare che l'ordine ottenuto esponenziando è buono. Per assurdo, supponiamo che non lo sia, allora, detto $S$ l'insieme delle funzioni $B \rightarrow A$ a supporto finito, abbiamo:
	\[ \underbrace{\exists f \in S \; \exists A \subseteq S (f \in A}_{\text{c'è un $A \subseteq S$ non vuoto}}\land\underbrace{\forall g \in A \; \exists h \in A \; h <_{\exp} g}_{\text{che non ha minimo}})
		\]
	Ora, fissiamo una $f \in S$ tale che $\exists A \subseteq S$ etc. con queste proprietà [possiamo farlo perché stiamo negando la tesi, quindi fissata una $f$ che non la rispetti, esiste il sottoinsieme etc.]: che $\max_{<_B}(\supp_A(f))$ sia minimo, e che, a parità di $m = \max_{<_B}(\supp_A(f))$ il valore di $f(m)$ sia minimo.
	Fissiamo ora $A$ in modo tale che $f \in A$ ed $A$ non abbia minimo. Il nostro scopo è costruire $\widetilde{A} \subseteq S$ che non ha minimo e contiene una funzione $\widetilde{f}$ con $\max_{<_B}(\supp_A(f)) <_B m$, in questo modo neghiamo la minimalità di $m$ ed otteniamo 
	la contraddizione voluta.\\
	Osserviamo, intanto che $A$ può essere ripartito in:
	\begin{align*}
		&A_1 = \{g \in A |\max_{<_B}(\supp_A(g)) <_B m\} \\
		&A_2 = \{g \in A |\max_{<_B}(\supp_A(g)) = m \land g(m) = f(m)\} \\
		&A_3 = \{g \in A |\max_{<_B}(\supp_A(g)) = m \land f(m) <_A g(m)\} \\
		&A_4 = \{g \in A |m <_B \max_{<_B}(\supp_A(g))\}
	\end{align*}
	e segue dalla definizione che le funzioni in $A_1$, sono $<_{\exp}$ di quelle in $A_2$, che sono $<_{\exp}$ etc. fino ad $A_4$.
	Però $A_1$ è vuoto, perché altrimenti, presa $f' \in A_1$, abbiamo $f' \in A$ e $\max(\supp_A(f))<m$ contro la minimalità di $m$. Allora $A_2$,
	che non è vuoto perché contiene $f$ nona ha minimo, se, infatti, l'avesse, questo dovrebbe essere anche minimo di $A$.\\
	Concentriamoci ora su $A_2$. Tutte le $g \in A_2$ assumono il medesimo valore su $m$, quindi, comparando due di queste funzioni con $<_{\exp}$, il valore assunto da entrambe su $m$ 
	è irrilevante. Ossia la funzione:
	\[ H : A_2 \rightarrow S : g \mapsto \widehat{g} \quad\text{con}\quad\widehat{g}(x) = \begin{cases}
		\min A &\text{se $x = m$}\\
		g(x) &\text{altrimenti}
	\end{cases}
		\]
	è strettamente crescente. Per cui $\widetilde{A} \Mydef H[A_2]$ non ha minimo.\\
	Ora, però, segue dalla definizione che, fissata $g \in A_2$, $\supp(\widehat{g}) = \supp(g) \setminus \{m\}$, quindi, ponendo $\widetilde{f} \Mydef \widehat{g}$, abbiamo $\max(\supp(\widehat{f}))<m$, e questo contraddice la minimalità di $m$.
\end{proof}

\begin{proposition}[Buona definizione delle operazioni tra ``classi di isomorfismo'' di buoni ordini - ovvero le operazioni tra buoni ordini sono definite modulo isomorfismo]
	Le operazioni aritmetiche sui buoni ordini \vocab{passano al quoziente modulo isomorfismi}. Ossia, dati due buoni ordinamenti $\mathcal A = (B,<_A)$
	e $\mathcal B = (B,<_B)$, e dati $\mathcal A' = (A',<_{A'}) \sim \mathcal A$ e $\mathcal{B}' = (B',<_{B'}) \sim \mathcal B$, si ha:
	\[ \mathcal{A} + \mathcal{B} \sim \mathcal{A}' + \mathcal{B'} \qquad \mathcal{A} \cdot \mathcal{B} \sim \mathcal{A}' \cdot \mathcal{B'} \qquad \mathcal{A}^{\mathcal{B}} \sim \mathcal{A}'^{\mathcal{B'}}
		\]
	quindi le operazioni fra buoni ordini sono equivalenti modulo l'essere isomorfi.\footnote{In altre parole le operazioni tra buoni ordini sono definite sulle classi di equivalenza di buoni ordini isomorfi, e la proposizione mostra che queste 
	operazioni sono ben definite.}
\end{proposition}

\begin{proof}
	Fissati gli isomorfismi $f : A \rightarrow A'$ e $g : B \rightarrow B'$, è facile scrivere esplicitamente gli isomorfismi richiesti. Per esempio, nel caso di $\mathcal{A}^{\mathcal{B}}$, si considera la restrizione alle funzioni a supporto finito di:
	\[ {}^{B}A \rightarrow {}^{B'}A' : h \mapsto f \circ h \circ g^{-1}
		\]
	in altre parole, l'isomorfismo richiesto per dimostrare la tesi, che è quello scritto sopra, è quello che fa commutare il diagramma seguente:
	\[\begin{tikzcd}
		B & A \\
		{B'} & {A'}
		\arrow["h", from=1-1, to=1-2]
		\arrow["{g^{-1}}", from=2-1, to=1-1]
		\arrow[from=2-1, to=2-2]
		\arrow["f", from=1-2, to=2-2]
	\end{tikzcd}\]
	andrebbe verificato che anche la nuova funzione $f \circ h \circ g^{-1}$ sia a supporto finito, ma questo segue dal fatto che $f$ e $g$ sono isomorfismi di ordini, quindi dove $h$ fa il minimo di $A$,
	allora $h'$ dovrà necessariamente fare il minimo, e viceversa, pertanto vale che $\supp_{A'}(f \circ h \circ g^{-1}) = g[\supp_A(f)]$ (e $g$ isomorfismo).
\end{proof}

\begin{exercise}[Buona definizione delle operazioni tra buoni ordini]
	Fare le altre verifiche della proposizione sopra.
\end{exercise}

\begin{proposition}[Proprietà delle operazioni sui buoni ordini]
	Siano $\mathcal{A} = (A,<_A)$, $\mathcal{B} = (B,<_B)$ e $\mathcal{C} = (C,<_C)$ buoni ordini. Allora:\footnote{Valgono in realtà anche l'esistenza e le proprietà degli elementi neutri per $\cdot$ e $+$.}
	\[\begin{split}
		\text{\textcolor{red}{associatività:}} &\quad (\mathcal A + \mathcal B) + \mathcal C \sim \mathcal A + (\mathcal B + \mathcal C) \quad (\mathcal A \cdot \mathcal B) \cdot \mathcal C \sim \mathcal A \cdot (\mathcal B \cdot \mathcal C)\\
		\text{\textcolor{red}{distributività a sinistra:}} &\quad  \mathcal A \cdot (\mathcal B + \mathcal C) \sim \mathcal A \cdot \mathcal B + \mathcal A \cdot \mathcal C \\
		\text{\textcolor{red}{proprietà delle potenze:}} &\quad {\mathcal A}^{\mathcal B + \mathcal C} \sim \mathcal A^{\mathcal B} \cdot {\mathcal A}^{\mathcal C} \qquad ({\mathcal A}^{\mathcal B})^{\mathcal{C}} \sim {\mathcal A}^{\mathcal B \cdot \mathcal C}
	\end{split}\]
\end{proposition}

\begin{proof}
	Facili verifiche.
\end{proof}

\begin{exercise}[Proprietà delle operazioni tra buoni ordini]
	Fare qualcuna delle verifiche delle proprietà sopra.
\end{exercise}

È importante notare che non tutte le proprietà delle operazioni aritmetiche su $\omega$ valgono per i buoni ordini.

\begin{exercise}[Proprietà \textcolor{red}{false} delle operazioni tra buoni ordini]
	Esibire controesempi alle seguenti:
	\textcolor{red}{\begin{align*}
		& \mathcal A + \mathcal B \sim \mathcal B + \mathcal A &(\mathcal A + \mathcal B) \cdot \mathcal C \sim \mathcal A \cdot \mathcal C + \mathcal B \cdot \mathcal C \\
		& \mathcal A \cdot \mathcal B \sim \mathcal B \cdot \mathcal A &(\mathcal A \cdot \mathcal B)^{\mathcal C} \sim \mathcal A^{\mathcal C} \cdot \mathcal B^{\mathcal C}
	\end{align*}}
	ovvero non valgono: \textcolor{red}{commutatività}, \textcolor{red}{distributività a destra} e \textcolor{red}{potenza di un prodotto}.
\end{exercise}

\begin{soln}
	Vediamo controesempi caso per caso.
	\begin{itemize}
		\item[\textcolor{red}{$\boxed{\text{commutatività $+$}}$}] Basta considerare $1+\omega$ e $\omega + 1$ (sia $1$ che $\omega$ sono buoni ordini), infatti abbiamo che:
		\begin{align*}
			1+\omega = (1 \sqcup \omega, <_+) \qquad \omega + 1 = (\omega \sqcup 1, <_+) 
		\end{align*}
		dove $1 \sqcup \omega = (1,0)\cup (\omega \times \{1\}) = \{(1,0),(0,1),(1,1),(2,1),\ldots\}$, con $<_+$ che è l'ordine dato dalla somma di buoni ordini, dunque $(1,0)<_+(n,1)$, $\forall n \in \omega$. Si vede facilmente
		quindi che $1 + \omega$ (oltre ad essere un buon ordine in quanto somma di buoni ordini) è superiormente illimitato e vale il principio del massimo, dunque $1+\omega \sim \omega$. Viceversa, dove $\omega \sqcup 1 = (\omega \times \{0\})\cup \{(1,1)\} = \{(1,1),(0,0),(1,0),(2,0),\ldots\}$,
		con $<_+$ che è sempre l'ordine dato dalla somma di buoni ordini, ma in questo caso si ha $(n,0) <_+ (1,1)$, $\forall n \in \omega$, dunque $\omega + 1$ è superiormente limitato, pertanto non può essere isomorfo ad $\omega$, dunque $1 + \omega \ne \omega + 1$.
	\end{itemize}
\end{soln}

Un altro tranello in cui si potrebbe cadere è credere che le operazioni sui buoni ordini generalizzino quelle sulle cardinalità [perché provando le operazioni con gli ordini finiti, valgono tutte le proprietà, comprese quelle false]. Questo è vero per le cardinalità finite, e anche in generale 
per somma è prodotto - come è ovvio dalla definizione - ma fallisce per l'esponenziale quando è infinito.\footnote{Un trucco è ricordare che le proprietà che valgono sono quelle con le parentesi a destra, perché ridefiniremo le operazioni per ricorsione transfinita in maniera analoga a quanto fatto per quelle in $\omega$ con la ricorsione numerabile,
ed in questo caso le parentesi saranno a destra.}

\begin{exercise}
	Dimostra che se $\mathcal A = (A,<_A)$ e $\mathcal B = (B,<_B)$ sono buoni ordini con $|A| = |B| = \aleph_0$ e $(C,<_C) = \mathcal A^{\mathcal B}$ allora $|C| = \aleph_0$.\footnote{Cioè per l'esponenziale di ordini valgono proprietà diverse rispetto a quelle classiche per le cardinalità (proprio perché le cose sono definite in maniera completamente diversa, e qui stiamo considerando solo alcune delle funzioni da $\omega$ in $\omega$),
	quindi ad esempio $|\omega^\omega| = \aleph_0$ (cioè la cardinalità dell'esponenziazione), mentre $|{}^\omega \omega| = |\omega|^{|\omega|} = \aleph_0^{\aleph_0} = 2^{\aleph_0}>\aleph_0$ (cioè la cardinalità delle funzioni da $\omega$ in $\omega$ [che abbiamo associato alle $\omega$-uple di elementi di $\omega$].)} \footnote{\underline{\textbf{Hint}}: ricordare che $\psf(\omega) = \aleph_0$ e pensare a come si possa identificare ciò con $\omega^\omega$.}
\end{exercise}

\subsection{Gli ordinali di Von Neumann}
In questa sezione definiremo gli ordinali di Von Neumann. L'idea che vogliamo concretizzare è che, siccome abbiamo visto che, 
a meno di isomorfismi, due buoni ordinamenti sono sempre l'uno nell'altro [abbiamo creato un ordine totale formale tra di essi basato su ciò], unendo fra loro tutti i buoni ordinamenti - o tutte le classe di isomorfismo 
di questi - dovrebbe potersi costruire un buon ordinamento più grande di tutti. Questa vasta struttura sarà inevitabilmente una classe propria:
la classe dei \vocab{numeri ordinali}, i cui elementi sono rappresentanti di tutte le possibili classi di isomorfismo di buoni ordini.\footnote{Vorremo anche fissare un rappresentate canonico per le ``classi di equivalenza'' dei buoni ordini, viste sopra a meno di isomorfismo, da cui l'idea di introdurre gli ordinali,
questa cosa ci richiederà qualcosa in più in termini di ipotesi e anche la necessità di introdurre un nuovo assioma, queste cose possono essere aggirate continuando a lavorare con i buoni ordini, ma il tutto verrebbe estremamente più pesante al livello di trattazione.}

\begin{definition}[Insieme transitivo]
	L'insieme $\alpha$ è \vocab{transitivo} se $\forall x \in \alpha \; x \subseteq \alpha$, o equivalentemente, se $\forall x \in \alpha \, \forall y \in x \; y \in \alpha$ (da cui il termine transitivo).
\end{definition}

\textcolor{MidnightBlue}{Ossia: diciamo che $\alpha$ è transitivo se gli elementi degli elementi di $\alpha$ sono, a loro volta, elementi di $\alpha$ (cioè se gli elementi
sono a loro volta insiemi di elementi).}\footnote{$\omega$ è un esempio di insieme transitivo, e naturalmente negli insiemi transitivi, così come $\omega$ gli elementi sono sottoinsiemi, ma non tutti i sottoinsiemi sono elementi.}

\begin{definition}[Ordinali di Von Neumann]
	L'insieme $\alpha$ è un \vocab{ordinale} se è \textcolor{red}{transitivo e bene ordinato dalla relazione di appartenenza}. Formalmente, l'insieme transitivo $\alpha$
	è un ordinale se $(\alpha, <_\alpha)$ è un buon ordine, con:
	\[ <_\alpha \Mydef \{(x,y) \in \alpha \times \alpha | x \in y\}\,\footnote{Esattamente come su $\omega$, $x < y \leftrightarrow x \in y \leftrightarrow (x,y) \in <$.}
		\]
	Denotiamo con $\Ord$ la classe degli ordinali\footnote{Tale classe contiene un elemento per ogni buon ordine, ad esempio, preso $(\omega,<)$, come classe di buoni ordini isomorfi a lui, prenderemo solo $\omega$ (il buon ordine transitivo e che ha come ordinamento proprio quello dato dall'appartenenza, e quindi ordinale), come
	rappresentante della ``classe di equivalenza'' nella classe dei buoni ordini (attenzione a non confondere i due significati del termine classe).}, per cui:
	\[ \alpha \in \Ord \Mydef \; \text{``$\alpha$ è transitivo e ben ordinato da $\in$''}
		\]
\end{definition}

\begin{example}[Esempi di ordinali]
	Alcuni esempi di ordinali già incontrati:
	\begin{itemize}
		\item $\omega$ è un ordinale
		\item gli elementi di $\omega$ sono ordinali
		\item $s(\omega) = \omega \cup \{\omega\}$ è un ordinale\footnote{E in generale il successore di un ordinale è un ordinale, ciò ci permette di descrivere bene gli elementi di $\omega$ senza l'assioma dell'infinito, ci basta prendere gli ordinali finiti, dove finito può essere espresso ad esempio con il principio del massimo,
		quindi elemento di $\omega$ = insieme ben ordinato, transitivo e con il principio del massimo.}
	\end{itemize}
\end{example}

\begin{remark}[$\Ord$ è \lbrack una classe\rbrack\;transitiva]
	\label{Ord_trans}
	Se $\alpha \in \Ord$ e $\beta \in \alpha$, allora $\beta \in \Ord$ e $\beta = \alpha_\beta$ (ovvero $\beta$ è il segmento iniziale principale [e in automatico proprio] di $\alpha$, determinato da $\beta$). In particolare la classe degli ordinali $\Ord$ è transitiva.\footnote{Cioè tutti gli ordinali sono a loro volta insiemi di ordinali di più piccoli.}
\end{remark}

\begin{proof}
	Siccome $\beta \in \alpha$, per la transitività di $\alpha$ [tutti gli elementi sono sottoinsiemi], $\beta \subseteq \alpha$, quindi $\beta$ è bene ordinato da $\in$ [ristretto come ordine a $\beta$].
	La transitività di $\beta$ segue dalla transitività della relazione di ordine $<_{\alpha}$. Prendiamo, infatti, $\delta \in \gamma$, con $\gamma \in \beta$. Dobbiamo 
	dimostrare che $\delta \in \beta$ (che è equivalente al dire $\gamma \subseteq \beta$, e che quindi i suoi elementi sono anche sottoinsiemi).\\
	Siccome $\gamma \in \beta$, per la transitività di $\alpha$, $\gamma \in \alpha$ [abbiamo usato che $\beta$ è anche un sottoinsieme di $\alpha$], e, da questo, nuovamente per la transitività di $\alpha$ [ora che abbiamo $\gamma \in \alpha$, essendo $\alpha$ transitivo, 
	abbiamo in automatico $\gamma \subseteq \alpha$], $\delta \in \alpha$ (e quindi anche $\delta \subseteq \alpha$\footnote{È una specie di bootstrap per chi conoscesse il termine dall'analisi.}). Ora $\delta,\gamma,\beta \in \alpha$ (e anche tutti sottoinsiemi), e abbiamo 
	l'ipotesi $\delta \in \gamma \land \gamma \in \beta$ (e vogliamo dimostrare $\gamma \subseteq \beta$), ossia [in termini di $<_\alpha$, che ora possiamo usare, perché sono tutti elementi di $\alpha$] $\delta <_\alpha \gamma \land \gamma <_\alpha \beta$, da cui [per transitività della relazione d'ordine] $\delta <_\alpha \beta$.
	Quest'ultima dice, appunto, che $\delta \in \beta$ (dunque $(\beta,<_{\alpha|\beta})$ è transitivo).
	Resta da dire che $\beta = \alpha_\beta$, e segue facilmente:
	\[ x \in \alpha_\beta \overset{\text{def. s.i.}}{\iff} x \in \alpha \land x <_\alpha \beta \overset{\text{def. $<_\alpha$}}{\iff} x \in \alpha \land x \in \beta
		\]
	Ora, $x \in \beta \rightarrow x \in \alpha$ per la transitività di $\alpha$, quindi l'AND si riduce a un solo termine:
	\[ x \in \alpha \land x \in \beta \iff x \in \beta
		\]
	e concludiamo che $x \in \alpha_\beta \leftrightarrow x \in \beta$, dunque per estensionalità, $\alpha_\beta = \beta$.
\end{proof}

La proposizione seguente ci dice che due ordinali non possono essere nella stessa classe di isomorfismo di buoni ordini [cioè per ogni classe di isomorfismo c'è \textbf{al più} un ordinale]. Vorremmo poi dimostrare che ogni classe 
di isomorfismo contiene almeno un ordinale [in modo da poter dire che in ogni classe ce n'è uno ed uno solo]. Enunciamo prima una semplice osservazione.

\begin{remark}[Gli isomorfismi tra ordini totali mantengono i s.i. principali]
	Se $f : A \rightarrow B$ è un isomorfismo fra $(A,<_A)$ e $(B,<_B)$, allora preso un qualunque $a \in A$ abbiamo $f[A_a] = B_{f(a)}$.
\end{remark}

\begin{proof}
	Basta semplicemente osservare che:
	\[\begin{split}
		x \in B_{f(a)} &\iff x <_B f(a) \\
					   &\iff f^{-1}(x) <_A a \\
					   &\iff f^{-1}(x) \in A_a \\
					   &\iff x \in f[A_a]
	\end{split}
		\]
	e si conclude per estensionalità.
\end{proof}

\begin{proposition}[Gli ordinali isomorfi sono proprio uguali]
	Dati $\alpha,\beta \in \Ord$, se $(\alpha,<_\alpha) \sim (\beta,<_\beta)$, allora $\alpha = \beta$.\footnote{La proposizione ha come conseguenza che per ogni classe di isomorfismo di buoni ordini, c'è \textbf{al più} un ordinale, perché se ce ne fosse più di uno (posto che per ora non sappiamo nemmeno se ce ne sia uno) sarebbero esattamente uguali.}
\end{proposition}

\begin{proof}
	Sia $f : \alpha \rightarrow \beta$ un isomorfismo. Ci basta dimostrare che $\forall \gamma \in \alpha \; f(\gamma) = \gamma$ [cioè che $f = \id_\alpha$]. Sia, per assurdo,
	$\gamma$ il minimo elemento di $\alpha$ tale che $f(\gamma) \ne \gamma$. Allora:
	\[ \gamma \overset{\text{\hyperref[Ord_trans]{Oss. sull'$\in$ degli ord.}}}{=} \alpha_\gamma \overset{(\star)}{=} f[\alpha_\gamma] \overset{\text{Oss. sopra}}{=} \beta_{f(\gamma)} \overset{\text{\hyperref[Ord_trans]{Oss. sull'$\in$ degli ord.}}}{=} f(\gamma)\;\lightning
		\]
	dove $(\star)$ è vero in quanto, abbiamo preso $\gamma$ come il più piccolo ordinale per cui $f$ non è l'identità, ma $\alpha_\gamma$ è fatto da cose strettamente più piccole di $\gamma$, dunque $f[\alpha_\gamma] = \alpha_\gamma$.
\end{proof}

Possiamo ora chiederci [avendo modo di ordinare questi particolari buoni ordini, detti ordinali] come si rifletta l'ordinamento totale delle classi di isomorfismo di buoni ordini, dato dalla relazione 
``essere segmento iniziale di'', sugli ordinali. La risposta è che diventa la relazione di appartenenza (cioè gli ordinali sono proprio tutti ordinati dall'appartenenza, d'altronde abbiamo già visto come sono ordinati i buoni ordini,
nel caso di questa particolare classe di buoni ordini transitivi, abbiamo appena visto che non ci sono classi di isomorfismo ma direttamente uguaglianze, è naturale quindi che l'essere isomorfo ad un segmento iniziale di un altro buon ordine, diventi in questo caso essere proprio esattamente quel segmento iniziale,
quindi, venuto via l'isomorfismo abbiamo direttamente l'appartenenza come ordine).

\textcolor{MidnightBlue}{Morale: ci piacerebbe ordinare gli ordinali usando $\in$, gli ordinali in quanto buoni ordini sono già ordinati dalla ``relazione d'ordine sulle classi di isomorfismo di buoni ordini'', ebbene si scopre proprio che il fatto che i buoni ordini siano ordinati totalmente da $\prec$ è equivalente al 
fatto che gli ordinali delle corrispondenti classi di isomorfismo [che per ora non sappiamo esserci, ma se ci sono, sono unici] sono ordinati totalmente secondo $\in$.}

\begin{theorem}[Gli ordinali sono totalmente ordinati dalla ``relazione'' di appartenenza]
	Dati $\alpha,\beta \in\Ord$, vale \textcolor{red}{una e una sola} delle seguenti:\footnote{Siamo in una classe, dunque ordinare
	gli ordinali è impreciso perché non abbiamo mai parlato di relazioni sulle classi, tuttavia, li stiamo ordinando nello stesso senso [ed a partire proprio] dall'ordinamento
	dei buoni ordini, usando in questo caso l'appartenenza.}
	\begin{align*}
		& \alpha \in \beta \;\text{che vale \textcolor{orange}{se e solo se} $(\alpha,<_\alpha) \prec (\beta,<_\beta)$} \\
		& \alpha = \beta \;\text{che vale \textcolor{orange}{se e solo se} $(\alpha,<_\alpha) \sim (\beta,<_\beta)$} \\
		& \beta \in \alpha \;\text{che vale \textcolor{orange}{se e solo se} $(\beta,<_\beta) \prec (\alpha,<_\alpha)$}
	\end{align*}
	(l'implicazione $\Longleftarrow$ tra i due fatti in mezzo la abbiamo già dimostrata con la proposizione precedente).
\end{theorem}

\textcolor{MidnightBlue}{Notazione: d'ora in poi porremo per comodità: $\alpha \prec \beta \Mydef (\alpha,<_\alpha) \prec (\beta,<_\beta)$, e analogamente $\alpha \sim \beta$ e $\beta \prec \alpha$.}

\begin{proof}
	Dimostriamo il primo caso [il terzo sarà simmetrico a questo]. Se $\alpha \prec \beta$ allora, per definizione di $\prec$, esiste $\gamma \in \beta$ tale che $\alpha \sim \beta_{\gamma}$. Però, $\beta$ è un ordinale e
	per quanto visto in un'\hyperref[Ord_trans]{osservazione precedente}, si ha $\beta_\gamma = \gamma$, da cui $\alpha \sim \gamma$, ma dalla proposizione precedente sappiamo che due ordinali isomorfi sono proprio uguali, ovvero $\alpha \sim \gamma \rightarrow \alpha = \gamma$.\\
	Abbiamo quindi che $\alpha = \gamma \in \beta$ [e ciò dimostra che se vale l'ordinamento come buon ordine vale l'appartenenza di ordinali]. D'altro canto, se $\alpha \in \beta$, allora $\alpha = \beta_\alpha$ (sempre per la solita \hyperref[Ord_trans]{osservazione}), quindi 
	$\alpha \prec \beta$ (esattamente per la definizione di $\prec$ visto che $\alpha$ diventa proprio un segmento iniziale di $\beta$).\\
	Se $\alpha \sim \beta$, come detto, la proposizione precedente ci assicura l'uguaglianza, il viceversa è immediato per la definizione di isomorfismo tra ordini totali.
\end{proof}

\begin{notation}[Ordine della classe degli ordinali]
	Dati $\alpha,\beta \in \Ord$, avendo dimostrato che l'appartenenza è una ``relazione di ordine totale'' per gli ordinali, quando si parla di ordinali useremo la notazione:
	\[ \alpha < \beta \Mydef \alpha \in \beta
		\]
	Infatti il teorema precedente ci dice che la relazione $<$ gode delle proprietà di un ordine totale stretto sulla classe degli ordinali.
\end{notation}

\begin{exercise}[Gli ordinali finiti sono tutti e soli quelli di $\omega$]
	Dimostra che $\alpha$ è un ordinale finito se e solo se $\alpha \in \omega$.
\end{exercise}

\begin{proposition}[Ordine largo sulla classe degli ordinali]
	Siano $\alpha,\beta \in \Ord$, allora:
	\[ \alpha \leq \beta \leftrightarrow \alpha \subseteq \beta
		\]
	con $\alpha \leq \beta \Mydef \alpha < \beta \lor \alpha = \beta$.
\end{proposition}

\begin{proof}
	Vediamo le due implicazioni:
	\begin{itemize}
		\item[$\boxed{\longrightarrow}$] Se $\alpha < \beta$ allora $\alpha \in \beta$ per la definizione di ordine sugli ordinali, quindi $\alpha \subseteq \beta$ per la transitività di $\beta$.
		Se $\alpha = \beta$, allora in particolare $\alpha \subseteq \beta$.
		\item[$\boxed{\longleftarrow}$] Dato $\alpha \subseteq \beta$, supponiamo per assurdo che $\beta < \alpha$. Allora $\beta \in \alpha$, ma per ipotesi si ha $\beta \in \alpha \subseteq \beta$.
		Per la transitività di $\beta$ [nel senso che gli elementi degli elementi sono elementi], si ha $\beta \in \beta\;\lightning$ [per definizione (se $\beta \in \beta$ allora $\beta < \beta \; \lightning$) o anche perché abbiamo dimostrato con la proposizione precedente che è un ordine stretto e totale sugli ordinali, dunque irriflessiva].\footnote{Segnalo che sulle dispense originali di Mamino c'è un divertente riferimento a buona fondazione.}
	\end{itemize}
\end{proof}

Ricordiamo che $s(\alpha) \Mydef \alpha \cup \{\alpha\}$. La proposizione segue dice che $s(\alpha)$ è, a buon diritto, il successore di $\alpha$, anche quando $\alpha$
è un ordinale.

\begin{proposition}[Il successore è un ordinale]
	Dato $\alpha \in \Ord$, $s(\alpha)$ è il minimo ordinale $> \alpha$.
\end{proposition}

\begin{proof}
	Occorre inizialmente verificare che $s(\alpha)$ è un ordinale.
	\begin{itemize}
		\item[$\boxed{\text{transitività}}$] Se $\beta \in s(\alpha)$, o $\beta \in \alpha$ o $\beta = \alpha$ (per la definizione di successore). Nel secondo caso è ovviamente transitivo, nel ugualmente per quanto visto nell'\hyperref[Ord_trans]{osservazione}.
		\item[$\boxed{\text{buon ordine}}$] Siccome $s(\alpha)$ è un insieme di ordinali, $\in$ è un ordine totale su $s(\alpha)$ [per quanto già dimostrato]. Dato $X \subseteq s(\alpha) = \alpha \cup \{\alpha\}$, con $X \ne \emptyset$, abbiamo che o $X = \alpha$ o $X \cap \alpha = \emptyset$ [$\alpha$ è un elemento dell'insieme unione ed è a sua volta un insieme di ordinali].
		Nel primo caso $X$ ha chiaramente un minimo, nel secondo, $\min(X \cap \alpha)$ è il minimo di $X$ [gli elementi di $X$ fuori dall'intersezione sono ordinali di $\alpha$ e quindi più piccoli].
	\end{itemize}
	Supponiamo, ora $\alpha < \beta$, dobbiamo dimostrare $s(\alpha) \leq \beta$. Sappiamo che $\alpha < \beta \implies \alpha \in \beta \implies \alpha \subseteq \beta$, allora dalla prima implicazione si ottiene $\{\alpha\} \subseteq \beta$ [transitività di $\beta$], e ciò, unito alla seconda implicazione, dà $s(\alpha) = \alpha \cup \{\alpha\}\subseteq \beta$, pertanto [per la proposizione sopra] $s(\alpha) \leq \beta$.
\end{proof}

\begin{corollary}[Successore del primo termine in una disuguaglianza tra ordinali]
	$\forall \alpha,\beta \in\Ord \; \beta \leq \alpha \leftrightarrow \beta < s(\alpha)$.
\end{corollary}

\begin{proposition}[Proprietà degli insiemi di ordinali]
	Dato un insieme di ordinali $X$:
	\begin{enumerate}[1.]
		\item Se $X \ne \emptyset$, allora esiste il minimo di $X$, detto \vocab{$\min X$}, inoltre $\min X = \bigcap X$.
		\item Esiste il minimo dei maggioranti di $X$\footnote{I maggioranti di un insieme di ordinali sono definiti allo stesso modo di quanto visto per i reali, ovvero sono gli $\alpha \in \Ord$ tali che $\forall \beta \in X \; \beta \leq \alpha$.}, detto \vocab{$\sup X$}, inoltre $\sup X = \bigcup X$.
		\item C'è un ordinale che non appartiene a $X$.\footnote{Questa cosa ci garantisce che non esiste un insieme di tutti gli ordinali, perché ci sarebbe sempre un ordinale fuori.}
	\end{enumerate}
\end{proposition}

\begin{proof}
	Vediamo i vari punti.
	\begin{enumerate}[1.]
		\item Dimostriamo, prima, che il minimo esiste. Sia $\alpha \in X$ fissato, che c'è perché supponiamo $X \ne \emptyset$. Consideriamo $\mu \Mydef \min_{<_{s(\alpha)}}(X \cap s(\alpha))$.
		Questo esiste perché $X \cap s(\alpha) \ne \emptyset$ in quanto $\alpha$ vi appartiene [e l'intersezione è sottoinsieme di $s(\alpha)$ che è ben ordinato, avendo dimostrato sopra che $s(\alpha)$ è un ordinale].\\
		Vediamo che $\mu = \min X$. Infatti, preso $\beta \in X$, se $\beta \leq \alpha$ allora [per il corollario sopra $\beta \in s(\alpha)$] $\beta \in s(\alpha) \cap X$, dunque $\beta \leq \mu$ [cioè partecipa alla scelta per il minimo nell'intersezione]. Se però $\alpha < \beta$ (cioè $s(\alpha) \leq \beta$), allora 
		$\mu < s(\alpha) \leq \beta$, dove la prima disuguaglianza è stretta perché c'è anche $\alpha$ in $s(\alpha) \cap X$ (e quindi $\mu$ è minimo per tutti gli ordinali in $X$ che sono maggiori o uguali ad $\alpha$).\\
		Ora verifichiamo che $\mu = \bigcap X$. Chiaramente $\forall \gamma \in X \; \mu \leq \gamma$, quindi $\forall \gamma \in X \; \mu \subseteq \gamma$, cioè $\mu$ è un sottoinsieme di ogni elemento di $X$, dunque è un sottoinsieme degli elementi comuni degli elementi, ossia $\mu \subseteq \bigcap X$. D'altro
		canto $\mu \in X$, quindi [poiché si prendono gli elementi comuni anche a $\mu$, necessariamente l'intersezione è un sottoinsieme] $\bigcap X \subseteq \mu$.
		\item Dimostriamo in primis che $\bigcup X$ è un ordinale.
		\begin{itemize}
			\item[$\boxed{\text{transitività}}$] Dato $\alpha \in \bigcup X$, esiste $\beta \in X$ tale che $\alpha \in \beta$. Per transitività di $\beta$ si ha $\alpha \subseteq\beta$. Da cui [$\alpha$ è un sottoinsieme degli elementi di $\beta$, quindi unendo questi elementi ad altri, $\alpha$ rimarrà ancora un sottoinsieme dei nuovi elementi] $\alpha \subseteq X$.
			\item[$\boxed{\text{buon ordine}}$] Ogni $\alpha \in \bigcup X$ appartiene a qualche $\beta \in X$, ed è, quindi, un ordinale (per la solita \hyperref[Ord_trans]{osservazione}). Stabilito che $\bigcup X$ è un insieme di ordinali, è chiaro che ogni suo sottoinsieme non vuoto è un insieme non vuoto di ordinali. Quindi ha minimo per il punto 1.
		\end{itemize}
		Dimostriamo ora che $\sigma \in \Ord$ è un maggiorante per $X$ se e solo se $\bigcup X \leq \sigma$ (in questo modo sappiamo che $\bigcup X$ è più piccolo di tutti i maggioranti, e si vede facilmente che è a sua volta un maggiorante in quanto $\forall x \in X$ si ha che un elemento di un insieme è naturalmente un sottoinsieme della sua unione [in altre parole, unendo
		un insieme stiamo prendendo tutti gli elementi degli elementi, dunque un elemento dell'insieme iniziale sarà un sottoinsieme dell'unione, in quanto ne abbiamo preso gli elementi nell'unione], pertanto segue $x \subseteq \bigcup X \leftrightarrow x \leq \bigcup X$).
		\[ \underbrace{\forall \alpha \in X \; \alpha \leq \sigma}_{\text{$\sigma$ è un maggiorante}} \iff \forall \alpha \in X \; \alpha \subseteq \sigma \iff \bigcup X \subseteq \sigma \iff \bigcup X \leq \sigma 
			\]
		dove l'equivalenza centrale deriva dal fatto che se tutti gli elementi di $X$ sono contenuti in $\sigma$, allora i loro elementi appartengono a $\sigma$, e dunque ovviamente la loro unione [degli elementi degli elementi] è un sottoinsieme di $\sigma$.
		\item Basta considerare $s(\sup X)$, per il 2. sappiamo che l'estremo superiore di $X$ esiste, e dalle proprietà viste sugli ordinali, sappiamo che il successore di un ordinale è il minimo ordinale più grande, dunque, in questo caso, per definizione di estremo superiore, necessariamente non sta nell'insieme.
	\end{enumerate}
\end{proof}

\begin{corollary}[Gli insiemi di ordinali transitivi sono ordinali]
	Un insieme di ordinali è un ordinale se e solo se è transitivo.
\end{corollary}

\begin{proof}
	Per il 2. della proposizione precedente sappiamo che ogni insieme di ordinali è ben ordinato, dunque la definizione di ordinale in questo caso si riduce al richiedere la transitività dell'insieme.
\end{proof}

\begin{corollary}[Paradosso di Burali-Forti]
	$\Ord$ è una classe propria.
\end{corollary}

\textcolor{MidnightBlue}{Ossia non esiste l'insieme di tutti gli ordinali.}

\begin{proof}
	Per il punto 3. della proposizione sulle proprietà degli insiemi di ordinali, se $\Ord$ fosse un insieme, esisterebbe un ordinale che non vi appartiene, che è assurdo.
\end{proof}


\pagebreak
\begin{note}[Cosa c'è di paradossale nel paradosso di Burali-Forti?]
	Nel 1897, \href{https://it.wikipedia.org/wiki/Cesare_Burali-Forti}{\textcolor{purple}{Cesare Burali-Forti}} era assolutamente convinto della esistenza dell'insieme
	di tutti gli ordinali - definiti allora come le classi di isomorfismo dei buoni ordini - quello che non sapeva è se la relazione $\prec$ fosse un ordine totale su queste classi.
	\begin{figure}[H]
		\centering
		\includegraphics[width = 7.5cm]{immagini/Burali_Forti.png}
	\end{figure}
	Burali-Forti credette di poter negare la totalità dell'ordine $\prec$ ragionando per assurdo. Se $\prec$ fosse un ordine totale, si può dimostrare che sarebbe buono [esattamente come abbiamo visto sopra, un insieme di ordinali è sempre ben ordinato], ma 
	allora $\Omega \Mydef [(\Ord,\prec)]$, la classe di isomorfismo di $(\Ord,\prec)$, sarebbe [a sua volta una classe di isomorfismo di un buon ordine e quindi] uno dei membri della classe $\Ord$ stessa, e, considerando il suo successore $s(\Omega)$,
	avremmo $\Omega \prec s(\Omega)$, ma anche $s(\Omega) \prec \Omega$ [per definizione], perché $s(\Omega) \in \Ord\;\lightning$.\\
	Il guaio è che, nello stesso anno, Cantor pubblicò una dimostrazione del fatto che la relazione $\prec$ è totale - esattamente l'argomento dei segmenti iniziali isomorfi che abbiamo illustrato 
	nel corso. Come è stata risolta la contraddizione? Concludendo che l'insieme di tutti gli ordinali esiste? \textcolor{red}{No}. Sfortunatamente Burali-Forti 
	aveva capito male la definizione di buon ordine, e ancora così, forse, nessuno se ne sarebbe accorto, ma, quel che è peggio, aveva tentato di correggerla, facendo, in realtà un pasticcio.
	La contraddizione è stata quindi imputata, da Burali-Forti e da Cantor, al bisticcio di definizioni ed il paradosso è stato dimenticato. Cinque anni dopo, \href{https://en.wikipedia.org/wiki/Bertrand_Russell}{\textcolor{purple}{Russell}}
	si rese conto del fatto che l'assurdo sussiste anche se si usa la definizione correttezza di buon ordine, e fu così che il paradosso di Burali-Forti acquisì il suo nome. E tutti vissero felici e contenti.
\end{note}

\subsection{L'assioma del rimpiazzamento}
Gli ordinali di Von Neumann sono eleganti, ma quanti ne abbiamo di questi arnesi? Si può dimostrare che, assumendo i soli assiomi 1-7, il gran totale degli ordinali potrebbe essere:
\[ \textcolor{purple}{\Ord \overset{?}{=} \underbrace{\{\emptyset, s(\emptyset),\ldots,s^n(\emptyset),\ldots,\omega,s(\omega),\ldots,s^n(\omega),\ldots\}}_{\textnormal{\textcolor{MidnightBlue}{in realtà, questo si chiamerà $\omega + \omega$}}}}
	\]
la classe degli ordinali raggiungibili a partire da $\emptyset$ o da $\omega$ con un numero finito di applicazioni della mappa successore.

\begin{exercise}
	Dimostra che la classe descritta sopra è effettivamente una classe, ossia è definita da una formula.
\end{exercise}

\begin{soln}
	Chiamiamo $O = \{\emptyset, s(\emptyset),\ldots,s^n(\emptyset),\ldots,\omega,s(\omega),\ldots,s^n(\omega),\ldots\}$, allora la formula che descrive $O$ è:
	\[ \forall x \; x \in O \leftrightarrow ((x = \emptyset) \lor (x = \omega) \lor (\exists n \, n \in \omega \land (x = s^n(\emptyset) \lor x = s^n(\omega))))
		\]
	avendo una formula, abbiamo che $O$ è una classe.
\end{soln}

Se vogliamo poter rispondere alla domanda ``quanti ordinali esistono?'' occorre un nuovo assioma: l'assioma del rimpiazzamento. Sotto questa ipotesi addizionale, la risposta sarà
``tutti quelli che potrebbero esistere'', ossia avremo un ordinale per ogni classe di isomorfismo di buoni ordini (che è proprio quello che vorremmo avendo dimostrato che per ogni classe ce n'è al più uno).
Per formulare l'assioma, ci serve il concetto di funzione classe.

\begin{definition}[Funzione classe]
	Date due classi $A$ e $B$ una \vocab{funzione classe} da $A$ a $B$ è una formula insiemistica $\varphi(x,y)$ tale che:
	\[ \forall x \in A \;\exists \textcolor{red}{!}\,y \in B \; \varphi(x,y)
		\]
\end{definition}

\textcolor{MidnightBlue}{Ossia, una funzione classe è una proprietà, espressa nel linguaggio della teoria degli insiemi, che ad ogni $x \in A$ [$=$ elemento che soddisfa la formula che definisce $A$] associa un \textcolor{red}{unico} $y \in B$ [$=$ elemento che soddisfa la formula che definisce $B$].}

\begin{notation}[Funzione classe]
	Possiamo denotare una funzione classe $\varphi(x,y)$ da $A$ a $B$ mediante la notazione più familiare:
	\[ F : A \rightarrow B
		\]
	In questo caso, la scrittura $y = F(x)$ è una semplice abbreviazione:
	\[ y = F(x) \Mydef y \in B \land \varphi(x,y)
		\]
\end{notation}

\begin{example}[Esempi di funzioni classe]
	Le seguenti sono funzioni classe $V \rightarrow V$:
	\[ F_1(x) = x \qquad F_2(x) = \{x\} \qquad F_3(x) = \ps(x) \qquad F_4(x) = s(x)
		\]
	La funzione classe $F_5(x) = \sup(x \cap \Ord)$, con $x \cap \Ord \Mydef \{\alpha \in x | \alpha \in \Ord\}$, è $V \rightarrow \Ord$.
\end{example}

\begin{axiom}[Assioma del rimpiazzamento]
	\label{ax8}
	Se $A$ è un \textcolor{LimeGreen}{insieme} e $F : V \rightarrow V$ è una funzione classe, allora $F[A] \Mydef \{F(x) | x \in A\}$ è un \textcolor{LimeGreen}{insieme}.\footnote{Come per la separazione, anche questo è uno \vocab{schema di assiomi}, uno per ogni possibile funzione classe $F$.}
	\[ \forall A \; \exists B \; \forall y \; y \in B \leftrightarrow \exists x \in A \; y = F(x)
		\]
	(cioè per ogni insieme [che ricordiamo essere le uniche variabili del nostro linguaggio] esiste un insieme i cui elementi sono immagini di quelli di $A$ rispetto alla funzione classe $F$).
\end{axiom}

\begin{proposition}[Unicità del rimpiazzo]
	Data una funzione classe $F : V \rightarrow V$ vale che:
	\[ \forall A \; \exists\,\textcolor{red}{!} B \; \forall y \; y \in B \leftrightarrow \exists x \in A \; y = F[x]
		\]
\end{proposition}

\begin{proof}
	Estensionalità.
\end{proof}

\begin{remark}[Rimpiazzamento da insieme a classe]
	Dato un \textcolor{red}{insieme} $A$ e una funzione classe $G : \textcolor{red}{A} \rightarrow V$, esiste ed è unico l'\textcolor{red}{insieme} $G[A]$ tale che:
	\[ \forall y \; y \in G[A] \leftrightarrow \exists x \in A \; y = G(x)
		\]
	In altre parole, l'assioma del rimpiazzamento vale anche con una funzione classe che va da un insieme ad una classe.
\end{remark}

\begin{proof}
	Ci basta semplicemente applicare l'\hyperref[ax8]{assioma del rimpiazzamento} appena enunciato, applicato alla funzione classe $F : V \rightarrow V$ definita come:
	\[ y = F(x) \Mydef (x \in A \land y = G(x)) \lor (x \not \in A \land y = \emptyset)
		\]
	ossia [in termini meno formali]:
	\[ F(x) \Mydef \begin{cases}
		G(x) &\text{se $x \in A$} \\
		\emptyset &\text{altrimenti}
	\end{cases}
		\]
	infatti se $x \in A$ si ha $G(x) = F(x)$, altrimenti c'è il vuoto, per cui $G[A] = F[A]$\footnote{Per la precisione $F[A]$, con $F : V \rightarrow V$, dà l'insieme $G[A] \cup \emptyset = G[A]$}.
\end{proof}

\begin{exercise}[Esistenza del prodotto cartesiano via rimpiazzamento]
	Dimostra che, dati due insiemi $A$ e $B$, esiste il loro prodotto cartesiano $A \times B$, usando l'assioma del rimpiazzamento ma \textcolor{red}{senza usare l'assioma delle parti}.
\end{exercise}

\begin{soln}
	[FALSA, DA SISTEMARE]
	Ci basta considerare la funzione classe $F : \ps(\ps(A \cup B)) \rightarrow V$ definita come:
	\[ F(z) \Mydef \begin{cases}
		(x,y) &\exists x \in A \; \exists y \in B \; z = (x,y) \\
		\emptyset &\text{altrimenti}
	\end{cases}
		\]
	a questo punto $F[\ps(\ps(A \cup B))]$ è un insieme ed è proprio uguale a $A \times B$.
\end{soln}

\begin{theorem}[Ogni buon ordine è isomorfo ad un unico ordianle]
	Dato un buon ordine $(A,<)$, esiste un unico ordinale $\alpha$ tale che $(A,<)\sim\alpha$.\footnote{Questo conclude il discorso sull'identificazione tra ordinali e buoni ordini, infatti prima abbiamo dimostrato che per ogni classe 
	di isomorfismo di buoni ordini c'è al più un ordinale, e ora che ce n'è sempre uno, che è appunto unico.}
\end{theorem}

\begin{proof}
	L'unicità segue per quanto abbiamo già visto, cioè $\alpha \sim \alpha' \rightarrow \alpha = \alpha'$. Basta quindi da dimostrare l'esistenza di $\alpha$. Sia:
	\[ A' = \{x \in A | \exists \gamma \in \Ord \; A_x \sim \gamma\}
		\]
	ovvero gli elementi nel buon ordine i cui segmenti iniziali sono isomorfi ad un ordinale (dopo questa dimostrazione potremo assumere che ce n'è uno per segmento iniziale, ma per ora, non lo sappiamo).
	Consideriamo la funzione classe $F : A' \rightarrow \Ord$:
	\[ F(x) = \text{l'unico $\gamma \in \Ord$ tale che $A_x \sim \gamma$}
		\]
	(cioè quella che associa ad ogni elemento di $A$ l'ordinale isomorfo al suo segmento iniziale), l'unicità vale perché:
	\[ A_x \sim \gamma \land A_x \sim \gamma' \implies \gamma \sim \gamma' \implies \gamma = \gamma'
		\]
	Vogliamo dimostrare che $\alpha\Mydef F[A'] \sim (A,<)$ (in altre parole l'immagine della funzione classe di $A'$ [che è un insieme per rimpiazzamento] è proprio l'ordinale corrispondente alla classe di isomorfismo del buon ordine $(A,<)$).\\
	Dimostriamo dunque che $\alpha$ è un ordinale, $A'$ è un segmento iniziale di $A$ e $\alpha \sim A'$. Infine concludiamo dimostrando che $A' = A$ e quindi si ha proprio che $\alpha \sim A$.
	\begin{itemize}
		\item[$\boxed{\text{$\alpha$ è un ordinale}}$] $\alpha$ è definito come l'insieme degli ordinali isomorfi ai segmenti iniziali corrispondenti agli elementi di $A'$, dunque è un insieme di ordinali, e per quanto visto, ci basta dimostrare che è transitivo affinché sia un ordinale a sua volta.
		Supponiamo $\beta < \gamma \in \alpha$, dobbiamo dimostrare che $\beta \in \alpha$ [cioè che $\beta$ è un sottoinsieme di $\alpha$].\\ Sia $f : \gamma \rightarrow A_x$ un isomorfismo [che abbiamo perché $\gamma \in \alpha$ significa che è isomorfismo a qualche segmento iniziale principale di un $x \in A$], allora
		$f_{|\beta} : \beta \rightarrow A_{f(\beta)}$ è un isomorfismo [la restrizione dell'isomorfismo deve necessariamente essere ancora iniettiva e preservare l'ordinamento, dunque l'unica cosa da osservare è che $f[\beta] = A_{f(\beta)}$, che è vero in quanto, dato $x \in A$ con $x \in f[\beta]$ e,
		preso $y \in A$, con $y <_A x$, allora $f^{-1}(y) < f^{-1}(x)$ e $f^{-1}(x) \in \beta$ (perché $f$ isomorfismo), dunque per transitività $f^{-1}(y) \in \beta$, per cui
		tornando indietro con $f$ si ottiene che $y \in f[\beta]$, dunque l'immagine di $\beta$ via $f$ è proprio il segmento iniziale determinato su $A$ da $f(\beta)$], quindi $\beta$ è l'ordinale corrispondente al segmento iniziale su $A$ determinato da $f(\beta)$, ovvero $\beta = F(f(\beta)) = F(A_{f(\beta)})$, pertanto sta in $\alpha = F[A']$.
		\item[$\boxed{\text{$A'$ s.i. di $A$}}$] Se $y < x \in A'$, allora esiste un isomorfismo $f : A_x \rightarrow \gamma$, quindi $f_{|A_y} : A_y \rightarrow \gamma_{f(y)}$ è un isomorfismo [poiché gli isomorfismi di ordini totali mandano segmenti iniziali in segmenti iniziali si ha che $f(A_y) = \gamma_{A_y}$, inoltre come sopra nella restrizione si
		mantengono iniettività e ordinamento] e, siccome [per le solite proprietà degli ordinali] $\gamma_{f(y)}  = f(y)$, abbiamo quindi proprio per $f$ ristretta che $\gamma_{A_y} = A_y \sim f(y)$.
		Per cui, per definizione di $A'$, essendo $A_y$ isomorfo ad un ordinale, si ha proprio $y \in A'$\footnote{Typo di Mamino.}, dunque $A'$ è un segmento iniziale di $A$.
		\item[$\boxed{\alpha\sim A'}$] Sia $f : A' \rightarrow \alpha$, la funzione, tra insiemi, definita da $f(x) = F(x)$, che esiste per l'\hyperref[ax8]{assioma di separazione} applicato ad $A' \times \alpha$ (quindi con la funzione classe abbiamo ottenuto l'insieme $\alpha$, ed ora possiamo proprio prendere una funzione tra i due insiemi, che faccia le stesse cose che faceva $F$).
		Dimostriamo quindi che $f$ è un isomorfismo di ordini.\\
		La surgettività è immediata perché, per costruzione, abbiamo che $\alpha = F[A'] = f[A']$. Verifichiamo la monotonia:
		\[ x < y \overset{\text{def. s.i.}}{\implies} A_x \prec A_y 
			\]
		ora, essendo che $f$ è definita tramite $F$, associa ad ogni elemento di $A$ l'ordinale a cui è isomorfo il suo segmento iniziale, cioè $f(x) \sim A_x$ e $f(y) \sim A_y$, dunque,
		avendo la relazione $\prec$ in partenza (che ci ricordiamo definita mediante isomorfismo), componendo gli isomorfismi troviamo che $f(x) \textcolor{red}{\prec} f(y)$, ovvero $f(x)$ è un segmento iniziale proprio di $f(y)$ mediante l'isomorfismo,
		ma, essendo $f(x)$ e $f(y)$ ordinali, sappiamo che la relazione d'ordine dei buoni ordini, coincide su di essi con l'appartenenza, pertanto $f(x) < f(y)$ (e con la monotonia stretta dimostrata abbiamo gratis l'iniettività).
		\item[$\boxed{A' = A}$] Se così non fosse $A'$ [che per 3. è un s.i.] sarebbe un segmento iniziale proprio di $A$, avremmo quindi $A' = A_k$ per qualche $k \in A$.\\
		Ma allora, per i punti 1-3, $A_k = A' \sim \alpha \in \Ord$, quindi, per definizione stessa di $A'$, $k \in A'$ [perché $k \in A$ sarebbe un elemento il cui s.i. principale è isomorfo ad un ordinale]. Però questo contraddice $A' = A_k \; \lightning$ (perché un s.i. principale non può contenere l'elemento da cui è determinato).
	\end{itemize}
\end{proof}

Una conseguenza del risultato precedente è che possiamo definire le operazioni sugli ordinali come semplice riflesso di quelle sui buoni ordini (perché a questo punto abbiamo una corrispondenza esatta tra classi di isomorfismo di buoni ordini e ordinali).

\begin{definition}[Operazioni sugli ordinali - v.1]
	Dati $\alpha,\beta \in\Ord$, definiamo \vocab{$\alpha + \beta$}, \vocab{$\alpha\cdot\beta$}, \vocab{$\alpha^\beta$} come, rispettivamente, l'unico ordinale tale che:
	\[ \textcolor{purple}{\alpha + \beta} \sim (\alpha,<_\alpha)+(\beta,<_\beta) \qquad \textcolor{purple}{\alpha \cdot \beta} \sim (\alpha,<_\alpha) \cdot (\beta,<_\beta)
		\]\[ \textcolor{purple}{\alpha^\beta} \sim (\alpha,<_\alpha)^{(\beta,<_\beta)}
			\]
\end{definition}

\begin{exercise}
	Dimostra che l'insieme introdotto all'inizio della sezione è effettivamente $\omega + \omega$, ossia, più precisamente:
	\[ \forall x \; x \in \omega + \omega \leftrightarrow (\exists m \in \omega \; x = m) \lor (\exists n \in \omega \; x = \omega +n)
		\]
\end{exercise}

\subsection{Induzione e ricorsione transfinite}
Il piatto forte di questa sezione è una seconda applicazione dell'assioma del rimpiazzamento: il teorema di ricorsione transfinita. Questo risultato sarà più chiaro a chi ha, in precedenza, risolto il seguente esercizio.

\begin{exercise}
	Dimostra che esiste un insieme $A$ tale che:
	\[ \forall x \; x \in A \leftrightarrow x = \emptyset \lor \exists y \in A \; x = \{y\}
		\]
	\textcolor{MidnightBlue}{ossia, in sostanza dimostra che esiste:}
	\[ \textcolor{MidnightBlue}{\{\emptyset,\{\emptyset\},\{\{\emptyset\}\},\{\{\{\emptyset\}\}\},\ldots\}}
		\]
\end{exercise}

L'idea per risolvere questo esercizio è contenuta nella dimostrazione del teorema di ricorsione \textcolor{red}{numerabile}, che abbiamo già visto. Attenzione, però,
che questo teorema non si può applicare dire alla situazione dell'esercizio.

\begin{soln}
	
\end{soln}

\begin{proposition}[Induzione transfinita - v.1]
	\label{induz_transf1}
	Data una formula insiemistica $\varphi(x)$. Se vale [l'ipotesi dell'induzione]\footnote{Come nell'induzione normale, il difficile sta nel mostrare che vale il passo induttivo, rappresentato dall'implicazione nell'ipotesi, poi il teorema assicura la veridicità dell'enunciato.}:
	\[ \forall \alpha \in \Ord(\forall \beta < \alpha \; \varphi(\beta))\rightarrow \varphi(\alpha)
		\]
	[ovvero se per ogni ordinale, sapere che la formula è vera per gli ordinali più piccoli, rende vera la formula per l'ordinale stesso], allora $\forall \alpha \in \Ord \; \varphi(\alpha)$.
\end{proposition}

\textcolor{MidnightBlue}{In termini di classi, rappresentando con $C$ la classe definita dalla formula $\varphi(x)$, abbiamo che se vale $\forall\alpha\in\Ord \;(\forall \beta < \alpha \; \varphi(\beta))\rightarrow \alpha \in C$\footnote{Ricordiamo
che avere $\alpha$ in una classe corrisponde al fatto che $\alpha$ soddisfa la formula per mezzo della quale è definita tale classe.}, allora $\forall \alpha \in \Ord \; \alpha \in C$, oppure, in forma più coincisa:
\[ (\forall \alpha \in \Ord (\alpha \subseteq C \rightarrow \alpha \in C)) \rightarrow \Ord \subseteq C
	\]
cioè, assumendo che per tutti gli ordinali valga che $\forall \beta < \alpha \; \beta \in C \iff \forall \beta \in \alpha \; \beta \in C \iff \alpha \subseteq C$ implica $\alpha \in C$ [cioè se tutti gli elementi di un ordinale stanno in $C$, allora anche l'ordinale stesso lo è], allora la classe degli ordinali è contenuta in $C$, ovvero tutti gli ordinali soddisfano la formula che definisce $C$.}

\begin{proof}
	Per assurdo, assumiamo [la negazione della tesi] $\neg \varphi(\alpha)$\footnote{Formalmente $\neg(\forall \alpha \in\Ord\;\varphi(\alpha)) = \exists \alpha \in \Ord \;\neg\varphi(\alpha)$.}, e consideriamo un $\alpha$ per cui la formula è falsa.
	Essendo l'ipotesi vera e [per ipotesi assurda] $\varphi(\alpha)$ falsa, l'antecedente dell'ipotesi [che come formula insiemistica è un'implicazione materiale] deve essere necessariamente falso, cioè vale $\neg (\forall \beta < \alpha \; \varphi(\beta))$, ovvero $\exists \beta < \alpha \; \neg \varphi(\beta)$.\\
	Quindi il ragionamento per assurdo ci ha portato a dire che esiste almeno un $\beta < \alpha$ per cui la proposizione è falsa, in particolare il sottoinsieme di $\alpha$ dei $\beta$ per cui $\varphi$ è falsa è non vuoto, quindi possiamo considerare\footnote{Perché
	ora possiamo scrivere un insieme non vuoto da cui, per separazione, prendere il minimo. Prima sarebbe stato il minimo $\alpha$ preso su qualcosa definito per separazione sulla classe degli ordinali, e poteva essere problematico, così invece non abbiamo alcun problema.} $\beta_0 := \min \{\beta \in \alpha | \neg\varphi(\beta)\}$, che esiste in quanto $\alpha$ è un buon ordine.
	Ora, usando $\beta_0$ nell'ipotesi [come fatto all'inizio con $\alpha$] (essendo un altro ordinale per cui la formula è falsa), si ottiene che $\exists \beta < \beta_0 \; \neg \varphi(\beta)$ (cioè l'antecedente dell'ipotesi ci procura un'ordinale più piccolo per cui la formula è falsa), contro la minimalità di $\beta_0$, che è assurdo.
\end{proof}

\begin{note}[L'induzione transfinita è uno schema di teoremi]
	Il principio di induzione transfinita non è, letteralmente, un teorema della teoria degli insiemi, quanto piuttosto uno scherma di teoremi - o metateorema - che ci permette 
	di costruire un diverso teorema per ogni possibile formula $\varphi$ [non essendo le classi oggetti della teoria degli insiemi non possono essere quantificate con i quantificatori soliti, quindi l'induzione può
	essere enunciata solo per una formula fissata ogni volta, e non per tutte le formule].
\end{note}

C'è una chiara analogia fra la forma precedente del principio di induzione transfinita e la forma forte dell'induzione aritmetica.
A volte, però, è comodo esprimere l'induzione transfinita in una forma che meglio ricorda il principio di induzione di Peano.

\begin{definition}[Ordinale successore]
	Diciamo che $\alpha \in \Ord$ è un \vocab{ordinale successore} se $\exists \beta \in \Ord \; \alpha = s(\beta)$. Un ordinale $\alpha \textcolor{red}{>0}$\footnote{Quindi tutti gli ordinali limite devono contenere lo 0, e lo 0 NON è un ordinale limite a sua volta.}
	che non è successore è detto \vocab{ordinale limite}.
\end{definition}

\begin{remark}[Un ordinale è successore se e solo se ha max]
	Un ordinale $\alpha$ è successore se e solo se ha un massimo elemento.\footnote{Equivalentemente questo ci dice anche che un ordinale è non successore [quindi limite tranne nel caso 0] se e solo se non ha un massimo}
\end{remark}

\begin{proof}
	$\beta$ è il massimo di $\alpha$ \textcolor{red}{se e solo se} $\alpha$ è il minimo ordinale $>\beta$, poiché:
	\[ \begin{split}
		\text{$\beta$ è il massimo di $\alpha$} \iff & \forall x \in \alpha \; x \leq \beta \land \beta \in \alpha\\
												\overset{\text{trans. $\alpha$}}{\iff} & \forall x \in \alpha \; x \leq \beta \land \beta \subsetneq \alpha\\
												\overset{(\star)}{\iff} & \beta \subsetneq \alpha \land \forall \gamma \in \Ord \; \beta \subseteq \gamma \rightarrow \alpha \subseteq \gamma
	\end{split}
		\]
	dove $(\star)$ vale perché, se vale che se $\beta$ è contenuto in un ordinale, allora anche $\alpha$ lo è, significa che se tutti gli elementi di $\beta$ stanno in $\gamma$, allora anche tutti quelli
	di $\alpha$ stanno in $\gamma$, ovvero $\alpha \subseteq \beta \leftrightarrow \forall x \in \alpha \; x \subseteq \alpha \rightarrow x \subseteq \beta \leftrightarrow x \leq \beta$,
	dunque $\forall x \in \alpha \; x \leq \beta$ e quindi $\beta$ è un massimo [perché $\beta \in \alpha$]. Viceversa, $\forall x \in \alpha \, x \leq \beta \leftrightarrow x \subseteq \beta \implies x \subseteq \gamma$, ma questo, per ogni $x$, equivale a $\alpha \subseteq \gamma$.\\
	Quando dimostrato è equivalente a dire che $\alpha$ è il minimo ordinale più grande di $\beta$. Si conclude osservando che ciò è vero \textcolor{red}{se e solo se} $\alpha = s(\beta)$ [per quanto visto sul successore di un ordinale, questo è per definizione il
	più piccolo ordinale più grande (a cui cioè appartiene l'ordinale iniziale)].
\end{proof}

\begin{proposition}[Induzione transfinita - v.2]
	\label{induz_transf2}
	Sia $\varphi(x)$ una formula insiemistica. Se:
	\begin{itemize}
		\item $\varphi(0)$ \textcolor{orange}{(caso base)}
		\item $\forall \gamma \in \Ord \; \varphi(\gamma) \rightarrow \varphi(s(\gamma))$ \textcolor{orange}{(caso successore)}
		\item per ogni ordinale limite $\lambda$, $(\forall \beta < \lambda \; \varphi(\beta)) \rightarrow \varphi(\lambda)$\footnote{Cioè vale anche un passo induttivo [forte] per gli ordinali che non sono successori.} \textcolor{orange}{(caso limite)}
	\end{itemize}
	allora $\forall \alpha \in \Ord \; \varphi(\alpha)$.
\end{proposition}

\begin{proof}
	Basta verificare l'ipotesi della \hyperref[induz_transf1]{prima forma dell'induzione transfinita}, per avere in automatico la veridicità della formula, dunque, fissato $\alpha \in \Ord$ bisogna mostrare che vale:
	\[ (\forall \beta < \alpha \; \varphi(\beta)) \rightarrow \varphi(\alpha)
		\]
	Se $\alpha$ è limite o $0$ abbiamo questa formula tout court [nel caso di 0 la formula sopra è sempre vera, nel caso degli ordinali limite abbiamo assunto che è vera nelle ipotesi]. Ci resta da vedere che la formula sopra è vera nel caso in cui $\alpha = s(\gamma)$, in questo caso abbiamo:
	\[ \gamma < \alpha \overset{\text{antecedente Hp. induttiva}}{\rightarrow} \varphi(\gamma) \overset{\text{Hp. prop.}}{\rightarrow} \varphi(\alpha)
		\]
	dunque anche in questo caso vale l'ipotesi della prima forma dell'induzione transfinita, che quindi vale $\forall \alpha \in \Ord$, pertanto per la prima forma vale $\varphi(x)$ per tutti gli $x \in \Ord$.
\end{proof}

Ora possiamo dimostrare il teorema di ricorsione transfinita. Faremo uso della notazione seguente.

\begin{notation}[Restrizione di una funzione classe]
	Data una funzione classe $F : A \rightarrow B$ e un insieme $X \subseteq A$ esiste la funzione [di insiemi]\footnote{In partenza ci stiamo restringendo ad un insieme, ed anche in arrivo, infatti $F[X]$ è un'insieme per \hyperref[ax8]{rimpiazzamento}.}:
	\[ f = F_{|X} : X \rightarrow F[X] : a \mapsto F[a]
		\]
	(che è in automatico surgettiva).
\end{notation}


\begin{theorem}[Ricorsione transfinita - v.1]
	\label{ric_transf1}
	Data una funzione classe $G : V \rightarrow V$\footnote{Anche in questo caso, questo è uno \vocab{schema di teoremi}, uno per ogni formula $G$, per la solita ragione che le formule delle classi non sono oggetti della teoria degli insiemi e quindi non sono quantificabili [inoltre la dimostrazione sarà uno schema di dimostrazioni].} esiste
	un'unica\footnote{Dove l'unicità va intesa nel senso seguente: date $F_1,F_2$ come sopra,
	vale $\forall \alpha \in \Ord \; F_1(\alpha) = F_2 (\alpha)$, cioè date due formule che rispettano entrambe quanto scritto, queste danno la stessa cosa [anziché richiedere che la formula sia unica, visto che le formule possono essere scritte in tanti modi equivalenti].} funzione $F : \Ord \rightarrow V$ tale che:
	\[ \forall \alpha \in \Ord \; F(\alpha) = G(F_{|\alpha})\footnote{Si osserva che $F_{|\alpha}$ è proprio una funzione tra insiemi $\alpha \rightarrow F[\alpha]$, come osservato sopra, per \hyperref[ax8]{rimpiazzamento}.}
		\]
\end{theorem}

\begin{proof}
	
\end{proof}

Come per l'induzione, possiamo esprimere la ricorsione transfinita separando i casi zero, successore e limite.

\begin{definition}[Prodotto cartesiano  di classi]
	Date due classi $A,B$ definiamo la \vocab{classe prodotto cartesiano} $A \times B$ come:
	\[ x \in A \times B \Mydef \exists a \in A \; \exists b \in B \; x = (a,b)
		\]
	(cioè $x$ è uguale a una coppia di elementi ciascuno in una classe, ovvero ciascuno soddisfa un predicato).
\end{definition}

\begin{corollary}[Ricorsione transfinita - v.2]
	\label{ric_transf2}
	Date le funzioni classe $G_1 : \Ord \times V \rightarrow V$ e $G_2 : V \rightarrow V$. Detto $x_0$ un insieme, esista un'unica funzione classe $F$ tale che:
	\begin{align*}
		& F(0) = x_0 \\
		& \forall \alpha \in \Ord \; F(s(\alpha)) = G_1(\alpha,F(\alpha)) \\
		& \forall \lambda \in \Ord \; \text{$\lambda$ limite} \rightarrow F(\lambda) = G_2(F_{|\lambda})
	\end{align*}
\end{corollary}

\begin{proof}
	Ci basta applicare il \hyperref[ric_transf1]{teorema di ricorsione transfinita v.1}, e per farlo, non dobbiamo far altro che definire una funzione classe $G : \Ord \rightarrow \Ord$, rispetto a cui $F(\alpha) = G(F_{|\alpha})$, ed il teorema ci assicura esistenza ed unicità.
	Possiamo esibire $G$ nel modo seguente:
	\[ G(f) = \begin{cases}
		\emptyset &\text{se $f$ NON è una funzione con $\Dom(f) \in \Ord$} \\
		x_0 &\text{se $f = \emptyset$} \\
		G_1(\alpha,f(\alpha)) &\text{se $\Dom(f) = \alpha + 1$ per qualche $\alpha \in \Ord$} \\
		G_2(f) &\text{altrimenti}
	\end{cases}
		\]
	(dove abbiamo definito $G(f)$, come $G$ di una certa troncata di $F$).
\end{proof}

\begin{corollary}[Operazioni tra ordinali (definizione ricorsiva)]
	Esistono le funzioni (classe) di somma, prodotto e potenza di ordinali, così definite:\footnote{Ricordiamo che l'estremo superiore di un insieme di ordinali esiste sempre ed è l'unione dell'insieme.}
	\begin{align*}
		&\alpha + 0 = \alpha &\quad &\alpha \cdot 0 = 0 \\
		&\alpha + s(\beta) = s(\alpha + \beta)  &\quad &\alpha \cdot s(\beta) = \alpha \cdot \beta + \alpha \\
		&\alpha^\lambda = \sup\{\alpha + \beta | \beta < \lambda\} &\quad &\alpha\cdot \lambda = \sup\{\alpha \cdot \beta | \beta < \lambda\}
	\end{align*}
	\begin{align*}
		&\alpha^0 = 1 \\
		&\alpha^{s(\beta)} = \alpha^\beta \cdot \alpha \\
		&\alpha^{\lambda} = \sup\{\alpha^\beta | \beta < \lambda\}
	\end{align*}
\end{corollary}

Ossia, le operazioni aritmetiche sugli ordinali si possono definire in modo analogo alle operazioni aritmetiche su $\omega$ nei casi 0 e successore,
\vocab{estendendole con continuità} nel caso limite.

\begin{definition}[Continuità]
	Una funzione classe $F : \Ord \rightarrow \Ord$ mai decrescente - $\alpha < \beta \rightarrow F(a) \leq F(\beta)$ - si dice \vocab{continua} se, per ogni ordinale limite $\lambda$ vale $F(\lambda) = \sup F[\lambda]$.\footnote{L'idea è la stessa dell'estensione continua di una funzione fuori dal suo dominio, ovvero quella 
	di far valere la funzione subito fuori l'estremo superiore dell'immagine dell'insieme subito prima.}
\end{definition}
\pagebreak
\begin{notation}[Sulle definizioni ricorsive di funzioni classe]
	Sarebbe corretto osservare che, letteralmente, il teorema di ricorsione transfinita non pare sufficiente a garantire l'esistenza, per esempio, della funzione classe $+ : \Ord \times \Ord$.
	Il problema è che, fissato $\alpha$, possiamo costruire ricorsivamente la funzione classe $``\alpha+'' : \Ord \rightarrow \Ord$, ma abbiamo, a quanto pare, una diversa funzione per ogni possibile $\alpha$ [perché stiamo costruendo una funzione classe da $\Ord$ a $\Ord$ fissato il primo termine, e in questo caso, essendo la funzione una classe, 
	essa cambia qualsiasi sia il primo termine fissato della somma, e come già visto le classi non possono essere quantificate all'interno della teoria degli insiemi, pertanto, abbiamo necessità di una funzione diversa per ogni fissato ordinale $\alpha$]. Ci sono due vie d'uscita da questo impasse.\\
	La più solida è, forse, dimostrare una versione parametrica del teorema, in cui sia $G$ sia $F$ hanno un argomento in più, un parametro, per accomodare $\alpha$.
	Questa è una operazione del tutto elementare, ma aggiunge burocrazia alla dimostrazione, che è già abbastanza complicata.\\
	La seconda strada è, tuttavia, osservare che il teorema si trova già in forma parametrica, anche se non si vede. Una funzione classe non è, infatti, altro che una formula insiemistica - con determinate proprietà - e nulla vieta 
	che questa formula contenga una variabile libera $\alpha$. Il teorema di \hyperref[ric_transf1]{ricorsione transfinita} dice che, se una certa formula - quella che definisce $G$ - è una funzione classe, allora un'altra formula - quella di $F$ - scritta esplicitamente nella 
	dimostrazione è anch'essa una funzione classe. Ebbene se la formula per $G$ ha una variabile libera $\alpha$ [e nulla ci vieta di inserirla], questa variabile comparirà altresì nella formula di $F$, ed avremo così, in realtà, una funzione classe di due argomenti: $\alpha$ e l'argomento di $F$.\\
	Comunque sia, questa dei parametri è una sottigliezza che, al livello del nostro corso, si può trascurare. Sono sicuro che, chiunque sia giunto a padroneggiare la materia abbastanza da rendersi conto del problema, capirà anche che la sua soluzione 
	non presenta difficoltà.
\end{notation}

\begin{proposition}[Le definizioni ricorsive delle operazioni tra ordinali sono equivalenti alle definizioni mediante le operazioni tra buoni ordini]
	Vale che:
	\[ \alpha + \beta \sim (\alpha,<_\alpha) + (\beta,<_\beta) \qquad \alpha \cdot \beta \sim (\alpha,<_\alpha) \cdot (\beta,<_\beta) 
		\]\[ \alpha^\beta \sim (\alpha,<_\alpha)^{(\beta,<_\beta) }
			\]
	ossia: che si definiscano le operazioni sugli ordinali per ricorsione o che lo si faccia mediante le corrispondenti operazioni sui buoni ordini, il risultato è il medesimo.
\end{proposition}

\begin{proof}
	Si procede per \hyperref[induz_transf2]{induzione transfinita v.2} su $\beta$ (in modo da avere i casi esattamente corrispondenti alla definizione ricorsiva).\\
	\textcolor{purple}{$\alpha + \beta \sim (\alpha,<_\alpha) + (\beta,<_\beta)$}
	\begin{itemize}
		\item[$\boxed{\beta = 0}$] Consideriamo $\alpha + 0$ e $(\alpha,<_\alpha) + (0,<_0)$, per la definizione ricorsiva sappiamo che $\alpha + 0 = \alpha$, mentre, per la definizione di somma sui buoni ordini abbiamo che:
		\[ (\alpha,<_\alpha) + (0,<_0) = (\alpha \sqcup 0, <_+) = ((\alpha \times \{0\}) \cup (\underbrace{\emptyset \times \{1\}}_{= \emptyset}),<_+) = (\alpha \times \{0\},<_+)
			\]
		con $(a,b) <_+ (a',b') = (b = 0 \land b' = 1) \lor ((b = 0 \land b' = 0) \land a <_\alpha a') \lor ((b = 1 \land b' = 1) \land a <_\alpha a')$, ma, visto che ci rimane solo $\alpha \times \{0\}$, non ci possono essere coppie ordinate con seconda componente 1, dunque $<_+$ si riduce al secondo caso [ovvero a confrontare solo coppie con seconda componente $0$],
		pertanto è immediato che $(\alpha \times \{0\},<_+) \sim (\alpha,<_\alpha)$\footnote{Formalmente l'isomorfismo manda semplicemente $(\alpha \ni) x \mapsto (x,0) (\in \alpha \times \{0\})$, è facile vedere che è iniettiva e surgettiva, ed è strettamente monotona in quanto, dati $x <_\alpha y$, si ha che $(x,0) <_+ (y,0) \equiv ((0 = 0) \land x <_\alpha y)$, che è vero per ipotesi.}.
		A questo punto sappiamo già che $\alpha \sim (\alpha,<_\alpha)$, perché semplicemente stiamo considerando l'uno come buon ordine e l'altro come ordinale, ma sono proprio la stessa cosa [isomorfismo di buoni ordini o no].
		\item[$\boxed{\beta = s(\gamma)}$] Assumiamo come ipotesi induttiva che $\alpha + \beta \sim (\alpha,<_\alpha) + (\beta,<_\beta)$ e dimostriamo che $\alpha + s(\beta) = (\alpha,<_\alpha) + (s(\beta),<_{s(\beta)})$. Per la definizione ricorsiva $\alpha + s(\beta) = s(\alpha + \beta)$,
		e per ipotesi induttiva:
		\[ s(\alpha + \beta) \sim s((\alpha,<_\alpha) + (\beta,<_\beta))
			\]
		(abbiamo semplicemente applicato il successore al LHS e al RHS), osserviamo ora che:
		\[ s((\alpha,<_\alpha) + (\beta,<_\beta)) = ((\alpha,<_\alpha) + (\beta,<_\beta)) + (1,<) \,\footnote{Andrebbe verificato.}
			\]
		a questo punto, si applica la proprietà associativa della somma dei buoni ordini, ottenendo $(\alpha,<_\alpha) + ((\beta,<_\beta) + (1,<))$, e, di nuovo per la verifica [non fatta], si ottiene $(\alpha,<_\alpha) + (s(\beta),<_{s(\beta)})$. A questo punto si ottiene l'isomorfismo voluto.
		\item[$\boxed{\text{$\beta = \lambda$ limite}}$] Vogliamo dimostrare che $\alpha + \lambda \sim (\alpha,<_\alpha) + (\lambda,<_\lambda)$, con $\lambda$ ordinale limite. Per definizione di somma tra buoni ordini abbiamo:
		\[ (\alpha,<_\alpha) + (\lambda,<_\lambda) = (\alpha \sqcup \lambda,<_+)
			\]
		che come sappiamo è un nuovo buon ordine [in particolare una classe di isomorfismo di buoni ordini], pertanto possiamo considerare l'ordinale associato alla classe di isomorfismo $\gamma \sim (\alpha \sqcup \lambda,<_+)$ e l'isomorfismo $f : \alpha \sqcup \lambda \rightarrow \gamma$.
		Vogliamo calcolare $\alpha + \lambda = \sup\{\alpha + \beta | \beta < \lambda\} = \bigcup \{\alpha + \beta | \beta < \lambda\}$ (la somma è definita ricorsivamente così + abbiamo visto che l'estremo superiore di un insieme di ordinali è l'unione dell'insieme). Dunque la tesi  iniziale da dimostrare si riduce a verificare che\footnote{Avendo dimostrato che la somma tra buoni ordini è isomorfa all'ordinale $\gamma$, possiamo sfruttare la transitività dell'isomorfismo e scrivere la tesi come $\gamma \sim \alpha + \lambda$, e, dovendo 
		al RHS essere un'ordinale [per la definizione ricorsiva], si ha proprio l'uguaglianza (per quanto osservato sul fatto che ordinali isomorfi sono proprio uguali), dunque dobbiamo verificare esattamente che $\gamma = \alpha + \lambda$.} $\gamma = \alpha + \lambda = \bigcup\{\alpha + \beta | \beta < \lambda\}$.\\
		Ora, per ipotesi induttiva (nel caso limite in un'induzione transfinita, ricordiamo che assumiamo l'ipotesi induttiva per tutti gli ordinali più piccoli di quello limite), se $\beta < \lambda$, allora vale la somma $\alpha + \beta \sim (\alpha \sqcup \beta,<_+)$, e siccome $\alpha \sqcup \beta$ è un segmento iniziale di $\alpha \sqcup \lambda$ ($\beta < \lambda \leftrightarrow \beta \in \lambda$,
		dunque $\alpha \sqcup \beta \hookrightarrow \alpha \sqcup \lambda$ con un'immersione strettamente monotona), si ha che $f[\alpha \sqcup \beta] = \alpha + \beta$ (perché $f$ è l'isomorfismo che associa l'ordinale $\alpha \sqcup \lambda$ al suo ordinale $\gamma$, dunque manda il segmento iniziale $\alpha \sqcup \beta$ nel segmento iniziale $\gamma_{\alpha \sqcup \beta}$, ma $\alpha \sqcup \beta \sim \alpha + \beta$ per 
		ipotesi induttiva, quindi $\gamma_{\alpha \sqcup \beta} \sim \gamma_{\alpha + \beta} \overset{\text{prop. ordinali}}{=} \alpha + \beta$, e poiché ordinali isomorfi sono uguali si ha proprio che $f[\alpha \sqcup \beta] = \gamma_{\alpha \sqcup \beta} = \alpha + \beta$).
		Ora, siccome $\lambda$ è limite, $\lambda = \bigcup \{\beta | \beta < \lambda\}$, quindi si vede che:
		\[ \alpha \sqcup \lambda \overset{\text{$\lambda$ limite}}{=} \bigcup \{\alpha \sqcup \beta | \beta < \lambda\} \\
			\]
		da cui la tesi:
		\[ \begin{split}
			\gamma \overset{\text{def. $f$}}{=} f[\alpha \sqcup \lambda] \overset{\text{appena visto}}{=}& \bigcup \{f[\alpha \sqcup \beta] | \beta < \lambda\} \\
																		 \overset{\text{oss. sopra}}{=}& \bigcup \{\alpha + \lambda | \beta < \lambda\} \overset{\text{def. ord. limite}}{=} \alpha + \lambda
		\end{split}
			\]
	\end{itemize}
	Per le altre verifiche (che sono circa sulla stessa linea), riportiamo solo il caso successore dell'induzione transfinita.\\
	\textcolor{purple}{$\alpha \cdot \beta \sim (\alpha,<_\alpha) \cdot (\beta,<_\beta)$}
	\begin{itemize}
		\item[$\boxed{\text{$\beta = \lambda$ limite}}$] Si procede come prima, prendendo $\gamma \sim (\alpha,<_\alpha) \cdot (\lambda,<_\lambda)$ e $f : \alpha \times \lambda \rightarrow \gamma$ isomorfismo.
		Nuovamente $\lambda = \bigcup \{\beta | \beta < \lambda\} \implies \alpha \cdot \lambda = \bigcup\{\alpha \cdot \beta | \beta < \lambda\}$, e per ipotesi induttiva si ha che $\alpha \cdot \beta \sim \alpha \times \beta$ [come buon ordine],
		da cui $f[\alpha \times \beta] = \alpha \cdot \beta$ [con un ragionamento analogo a quanto visto sopra]. Infine si conclude con:
		\[ \begin{split}
			\gamma = f[\alpha \times \lambda] &= \bigcup\{f[\alpha \times \beta] | \beta < \lambda\} \\
											  &= \bigcup\{\alpha \cdot \beta | \beta < \lambda\} = \alpha \cdot \beta
		\end{split}
			\]
		e dal fatto che $\alpha \cdot \lambda = \gamma \sim (\alpha,<_\alpha) \cdot (\lambda,<_\lambda)$, si conclude che $\alpha \cdot \lambda \sim (\alpha,<_\alpha) \cdot (\lambda,<_\lambda)$.
	\end{itemize}
	\textcolor{purple}{$\alpha^\beta \sim (\alpha,<_\alpha)^{(\beta,<_\beta)}$}
	\begin{itemize}
		\item[$\boxed{\text{$\beta = \lambda$ limite}}$] In questo caso, si ripropone il ragionamento dei due casi precedente, con un leggero problema tecnico. Dato $\beta < \lambda$, nei due casi precedente, abbiamo usato il fatto che $\alpha \sqcup \beta$ e $\alpha \times \beta$ sono,
		rispettivamente, segmenti iniziali di $\alpha \sqcup \lambda$ e $\alpha \times \lambda$, che poi scriviamo come unione, appunto, di questi sottoinsiemi.\\
		Il guaio, adesso, è che l'insieme delle funzioni $\beta \rightarrow \alpha$ a supporto finito non è neppure sottoinsieme dell'insieme delle funzioni $\lambda \rightarrow \alpha$ a supporto finito. La soluzione è semplice, detti:
		\begin{align*}
			& SF(\square \rightarrow \alpha) \equiv \{g : \square \rightarrow \alpha \; \text{a supporto finito}\} \\
			& EXT_\beta^\alpha : SF(\beta \rightarrow \alpha) \rightarrow SF(\lambda \rightarrow \alpha) : g \mapsto h \qquad\text{con $h_{|\beta} = g$ e $\forall \delta \in \lambda\setminus\beta \; h(\delta) = 0$}
		\end{align*}
		ossia $EXT$ è l'operatore che estende una $g : \beta \rightarrow \alpha$ con 0 su $\alpha \setminus\beta$.\\
		È chiaro che... (DA COMPLETARE)
	\end{itemize}
\end{proof}

Per la proposizione predente, la definizione ricorsiva delle operazioni aritmetiche fra ordinali equivale a quella basata sulle operazioni fra buoni ordini. Quella \textcolor{purple}{ricorsiva} è una \vocab{definizione intensionale} - il termine 
è parente più prossimo di intendere che di inteso - ossia specifica le proprietà che caratterizzano un certo oggetto, in questo caso le operazioni ordinali. L'altra [quella basta sulla costruzione di nuovi \textcolor{purple}{buoni ordini}] è una \vocab{definizione estensionale} -
ossia descrive l'oggetto definito. Generalmente, la difficoltà con le definizioni intensionali è dimostrare che il definendo esiste, con le definizioni estensionali è, invece, ricavarne le proprietà.

\newpage
\section{Aritmetica ordinale e forma normale di Cantor}
In questa sezione studieremo nel dettaglio le proprietà delle operazioni aritmetiche fra gli ordinali. Il risultato principale sarà che ogni ordinale $\alpha$ si scrive, in modo unico, nella forma:
\[ \alpha = \omega^{\beta_1}\cdot k_1 + \omega^{\beta_2}\cdot k_2 + \ldots + \omega^{\beta_n}\cdot k_n
	\]
con $n \in \omega$, $k_1,k_2,\ldots,k_n \in \omega\setminus\{0\}$ e $\beta_1 > \beta_2 > \ldots > \beta_n$ (ordinali). Con queste forme normali di Cantor è possibile calcolare le operazioni aritmetiche in modo esplicito.

\begin{note}
	Per procederemo con ordine, assumeremo la definizione ricorsiva delle operazioni ordinali e procederemo unicamente da quella.
\end{note}

\begin{proposition}[Monotonia delle operazioni fra ordinali]
	Le funzioni $(\alpha,\beta)\mapsto \alpha + \beta$, $(\alpha,\beta) \mapsto \alpha \cdot \beta$ e $(\alpha,\beta) \alpha^\beta$ sono \textcolor{red}{strettamente crescenti nel secondo argomento} - per $\alpha \cdot \beta$ assumendo $\alpha \ne 0$,
	per $\alpha^\beta$ assumendo $1<\alpha$ - e \textcolor{red}{ma decrescenti nel primo argomento}.
\end{proposition}

Per dimostrare la proposizione ci serviranno queste note.

\begin{note}[Condizione sufficiente per la disuguaglianza tra gli estremi superiori]
	Dati due insiemi di ordinali $X,Y$ non vuoti vale che:\footnote{Moralmente: se posso dominare ogni elemento di $X$ con un elemento di $Y$, allora vale la disuguaglianza tra gli estremi superiori.}
	\[ \forall \alpha \in X \; \exists \beta \in Y \; \alpha \leq \beta \rightarrow \sup X \leq \sup Y
		\]
\end{note}

\begin{proof}
	Basta osservare che se $\gamma$ è un maggiorante di $Y$, allora è un maggiorante di $X$.
	Infatti, preso $\alpha \in X$ per ipotesi esiste $\beta \in Y$ con $\alpha \leq \beta$, e, siccome $\beta \leq \gamma \implies \alpha \leq \gamma$.\\
	Ora $\sup Y$ è un maggiorante di $Y$ [quindi in automatico, per quanto appena visto, domina tutti gli elementi di $X$], dunque $\sup Y$ è un maggiorante di $X$, pertanto è maggiore o uguale a $\sup X$ [per definizione].
\end{proof}

\begin{note}[Il successore è crescente]
	La funzione classe $\alpha \mapsto s(\alpha)$ è crescente.
\end{note}

\begin{proof}
	$\alpha < \beta \leftrightarrow s(\alpha) \leq \beta \leftrightarrow s(\alpha) < s(\beta)$ [dove entrambe le equivalenza corrispondono alle osservazioni sul successore di uno dei due termini di una disuguaglianza].
\end{proof}

Possiamo quindi dimostrare la proposizione.

\begin{proof}
	Vediamo le due richieste nel caso della somma separatamente.\\
	\textcolor{purple}{$\beta \mapsto \alpha + \beta$ è strettamente crescente}\\
	Dobbiamo dire che dati $\beta < \gamma$, vale che $\alpha + \beta < \alpha + \gamma$. Procediamo per \hyperref[induz_transf2]{induzione transfinita v.2} su $\gamma$.
	\begin{itemize}
		\item[$\boxed{\text{caso $\gamma = 0$}}$] Vera a vuoto [$\beta < 0 \leftrightarrow \beta \in \emptyset$].
		\item[$\boxed{\text{caso $\gamma = s(\delta)$}}$] Per ipotesi induttiva abbiamo che $\beta < \delta \rightarrow \beta + \alpha < \beta + \delta$. Preso $\beta < s(\delta)$, questo è equivalente a  $\beta \leq \delta$, da cui si ha:
		\[ \alpha + \beta \leq \alpha + \delta \,\textcolor{red}{<}\footnote{Questa disuguaglianza è letteralmente la definizione di $<$ come appartenenza, che col successore è ovvia.}\, s(\alpha + \delta) \overset{\text{def. ric.}}{=} \alpha + s(\delta) = \alpha + \gamma
			\]
		dove la prima disuguaglianza si ha perché o $\beta = \delta$ o $\beta < \delta$. Nel primo caso naturalmente $\alpha + \beta = \alpha + \gamma$, nel secondo vale l'ipotesi induttiva, cioè $\alpha + \beta < \alpha + \delta$.
		\item[$\boxed{\text{caso $\gamma = \lambda$ limite}}$] Dato $\beta < \lambda$, abbiamo $s(\beta) < \lambda$ [successore di una disuguaglianza + ordinale limite, quindi non può essere uguale], dunque, possiamo applicare l'ipotesi induttiva sia a $\beta$ che a $s(\beta)$ e 
		ottenere:
		\[ \alpha + \beta < \alpha + s(\beta) \leq \sup\{\alpha + \delta | \delta < \lambda\} = \alpha + \lambda
			\]
		dove il minore o uguale vale per la definizione di $\sup$ (e per l'osservazione iniziale, cioè prendere $\beta$ sotto $\lambda$, e di conseguenza anche il suo successore sta sotto $\lambda$).
	\end{itemize}
	\textcolor{purple}{$\alpha \mapsto \alpha + \beta$ è non decrescente}\\
	Dobbiamo dire che $\alpha < \gamma$, allora $\alpha + \beta \leq \gamma + \beta$. Procediamo ancora una volta per \hyperref[induz_transf2]{induzione transfinita v.2} su $\beta$.
	\begin{itemize}
		\item[$\boxed{\text{caso $\beta = 0$}}$] Banale per le definizioni delle operazioni [sia ricorsiva sia coi buoni ordini].
		\item[$\boxed{\text{caso $\beta = s(\delta)$}}$] Per ipotesi induttiva, vale che $\alpha < \gamma \rightarrow \alpha + \delta \leq \gamma + \delta$, per l'osservazione precedente ci basta applicare la funzione successore a LHS e RHS per ottenere:
		\[ \alpha + \beta = \alpha + s(\delta) = s(\alpha + \delta) \leq s(\gamma + \delta) = \gamma + s(\delta) = \gamma + \beta
			\]
		\item[$\boxed{\text{caso $\beta = \lambda$ limite}}$] Dobbiamo dimostrare che:
		\[ \alpha + \lambda = \sup \{\alpha + \delta | \delta < \lambda\} \leq \sup\{\gamma + \delta | \delta < \lambda\} = \gamma + \lambda
			\]
		Basta applicare la prima nota, osservando che vale $\alpha + \delta \leq \gamma + \delta$ per ipotesi induttiva.
	\end{itemize}
	Le dimostrazioni per il prodotto e l'esponenziale ripetono pedissequamente lo schema delle precedenti, restano quindi per \underline{esercizio}. Unica osservazione:
	nel passo induttivo del prodotto si deve usare il risultato per la somma, e nel passo induttivo dell'esponenziale si deve usare il prodotto.
\end{proof}

\begin{exercise}
	Le ipotesi che $\alpha \ne 0$ per il prodotto e $1 < \alpha$ per l'esponenziale dove sono usate?
\end{exercise}

\begin{remark}[Controesempio alla stretta crescenza della prima componente]
	Basta considerare $0 + \omega$ e $1 + \omega$, infatti, $\omega$ è ordinale limite, dunque:
	\begin{align*}
		& 0 + \omega = \sup \{0 + n | n < \omega\} = \sup \{n | n < \omega\} = \bigcup \{n | n < \omega\} = \omega \\
		& 1 + \omega = \sup \{1 + n | n < \omega\} = \sup \{s(n) | n < \omega\} = \bigcup \{s(n) | n < \omega\} = \omega
	\end{align*}
	quindi la somma con $\omega$ dà lo stesso risultato, ma $0 < 1$, dunque la somma non è strettamente crescente nella prima componente.
\end{remark}

\begin{proposition}[Proprietà delle operazioni fra ordinali]
	Dati $\alpha,\beta,\gamma \in \Ord$ valgono le seguenti proprietà:
	\[\begin{split}
		\text{\textcolor{red}{associatività:}} &\quad (\alpha + \beta) + \gamma = \alpha + (\beta + \gamma) \quad (\alpha \cdot \beta) \cdot \gamma = \alpha \cdot (\beta \cdot \gamma)\\
		\text{\textcolor{red}{distributività a sinistra:}} &\quad  \alpha \cdot (\beta + \gamma) = \alpha \cdot \beta + \alpha \cdot \gamma \\
		\text{\textcolor{red}{proprietà delle potenze:}} &\quad {\alpha}^{\beta + \gamma} = \alpha^{\beta} \cdot \alpha^{\gamma} \qquad ({\alpha}^{\beta})^{\gamma} = {\alpha}^{\beta \cdot \gamma}
	\end{split}\]
\end{proposition}

\begin{note}
	Abbiamo già asserito la proposizione corrispondente per i buoni ordinamenti (notare gli uguali al posto dei simboli di isomorfismo in questo caso), ma lasciando la dimostrazione per esercizio.\\
	Lasceremo comunque parte della dimostrazione per esercizio, ma non invano: è un esercizio più facile.
\end{note}

\begin{remark}[$\sup X \not \in X \implies \sup X$ ordinale limite]
	Dato $X \subseteq \Ord$, se $\sup X \not \in X$, allora $\sup X$ è limite.
\end{remark}

\begin{proof}
	Se per assurdo $\sup X = s(\alpha)$, siccome $\sup X$ non è elemento di $X$, ciò equivale a $\forall \beta \in X \; \beta < \sup X$, quindi abbiamo 
	anche $\forall \beta \in x \; \beta \leq \alpha < \sup X$ [non può accadere mai che $\beta > \alpha$, perché $s(\alpha)$ è il più piccolo ordinale più grande di $\alpha$ come
	abbiamo visto, in tal caso dovrebbe accadere per forza che $s(\alpha) \leq \beta \implies s(\alpha) \in X$ (in ogni caso), ma $s(\alpha) = \sup X$, quindi è assurdo],
	per cui $\alpha$ è un maggiorante di $X$ più piccolo di $\sup X \; \lightning$.
\end{proof}

\begin{remark}[Le operazioni tra ordinali sono continue - ovvero commutanto con il sup a destra]
	Dato $X \subseteq \Ord$ e $\alpha \in \Ord$ vale che [indipendentemente dal fatto che $\sup X$ sia o meno in $X$]:
	\begin{align*}
		\alpha + \sup X &= \sup\{\alpha + \beta | \beta \in X\} \\
		\alpha \cdot \sup X &= \sup \{\alpha \cdot \beta | \beta \in X\} \\
		\alpha^{\sup X} &= \sup\{\alpha^\beta | \beta \in X\}
	\end{align*}
	(quindi quando si ha un ordinale limite, che sappiamo essere sempre della forma $\lambda = \sup\{\beta | \beta < \lambda\}$, vale in automatico quanto scritto sopra).
\end{remark}

\begin{proof}
	Le dimostrazioni sono uguali. Vediamo la prima. Se $\sup X \in X$, l'enunciato è immediato, infatti, come visto, se un insieme di ordinali ha un massimo, allora è in automatico un ordinale [successore], quindi ai LHS diventano le normali operazioni fra ordinali,
	mentre al RHS gli estremi superiori diventano proprio i massimi, che si raggiungono appunto con $\sup X = \max X$ nelle operazioni.\\
	Supponiamo dunque che $\lambda = \sup X \not \in X$ [per quanto visto nell'osservazione sopra sappiamo quindi che $\sup X$ è un ordinale limite]. Dobbiamo dimostrare che:
	\[ \alpha + \lambda \overset{\text{definizione}}{=} \sup\underbrace{\{\alpha + \gamma | \gamma < \lambda\}}_{A} = \sup\underbrace{\{\alpha + \beta | \beta \in X\}}_{B}
		\]
	Dobbiamo verificare la seconda uguaglianza, basta dire che dato $\gamma_1 < \lambda$ esiste $\beta_1 \in X$ con $\alpha + \gamma_1 \leq \alpha + \beta_1$ [dunque $\sup A \leq \sup B$], e, viceversa, dato $\beta_2 \in X$ esiste $\gamma_2 < \lambda$ con $\alpha + \beta_2 \leq \alpha + \gamma_2$ [dunque $\sup A \geq \sup B$] (stiamo usando il lemma visto prima sulla disuguaglianza dei sup).\\
	Preso $\gamma_1 < \lambda = \sup X$, $\gamma_1$\footnote{Typo di Mamino.} non è un maggiorante di $X$, quindi esiste $\beta_1 \in X$ con $\gamma_1 < \beta_1$ (sarebbe la caratterizzazione del $\sup$),
	per la monotonia stretta sulla seconda componente, segue $\alpha + \gamma_1 \leq \alpha + \beta_1$ [e la disuguaglianza stretta implica quella larga].\\
	Preso $\beta_2 \in X$, siccome $\lambda = \sup X$ è limite, $\beta_2 < s(\beta_2) < \lambda$ [successore + $\lambda$ limite], quindi, ponendo $\gamma_2 = s(\beta_2)$, sempre per la stretta monotonia nella prima componente, si ha $\alpha + \beta_2 < \alpha + \gamma_2$ [e ancora una volta la disuguaglianza stretta implica quella larga].
\end{proof}

Possiamo quindi dimostrare la proposizione sulle proprietà delle operazioni tra ordinali.

\begin{proof}
	Sono tutte facili induzioni su $\gamma$. Vediamo la prima, le altre restano come \underline{esercizio}. Conviene affrontarle nell'ordine in cui sono scritte, sinistra - destra, alto-basso.\\
	Dimostriamo \textcolor{purple}{$(\alpha + \beta) + \gamma = \alpha + (\beta + \gamma)$} per induzione su $\gamma$.
	\begin{itemize}
		\item[$\boxed{\text{caso $\gamma = 0$}}$] Si vede immediatamente $(\alpha + \beta) + 0 \overset{\text{def. ricors.}}{=} \alpha + \beta \overset{\text{def. ricors.}}{=} \alpha + (\beta + 0)$.
		\item[$\boxed{\text{caso $\gamma = s(\delta)$}}$] Segue dall'ipotesi induttiva e dalla definizione ricorsiva della somma ordinale:
		\[\begin{split}
			(\alpha + \beta) + s(\delta) \overset{\text{def. ricors.}}{=}& s((\alpha + \beta) + \delta) \\
										 \overset{\text{Hp. indutt.}}{=}& s(\alpha + (\beta + \delta)) \\
										 \overset{\text{def. ricors.}}{=}& \alpha + s(\beta + \delta) \\
										 \overset{\text{def. ricors.}}{=}& \alpha + (\beta + s(\delta))
		\end{split}
			\]
		\item[$\boxed{\text{caso $\gamma = \lambda$ limite}}$] Ancora una volta segue dall'ipotesi induttiva e dalla definizione ricorsiva della somma nel caso limite:
		\[ \begin{split}
			(\alpha + \beta) + \lambda \overset{\text{def. ricors.}}{=}& \sup\{(\alpha + \beta) + \delta | \delta < \lambda\} \\
									   \overset{\text{Hp. indutt.}}{=}& \sup\{\alpha + (\beta + \delta) | \delta < \lambda\} \\
																	 =\quad\;\, &\alpha + (\sup\{\beta + \delta | \delta < \lambda\}) \\
										\overset{\text{def. ricors.}}{=}& \alpha + (\beta + \lambda)
			\end{split}
			\]
	\end{itemize}	
\end{proof}



\subsection{Sottrazione e divisione euclidea}
Introduciamo, ora, due lemmi che serviranno per calcolare la formale normale di Cantor: la sottrazione e la divisione di ordinali.

\begin{lemma}[Sottrazione di ordinali]
	Dati $\alpha, \gamma \in \Ord$, con $\alpha \leq \gamma$, esiste un unico $\beta \in \Ord$ tale che $\alpha + \beta = \gamma$.
\end{lemma}

\textcolor{MidnightBlue}{Intuitivamente $\gamma \sim \alpha \sqcup (\gamma\setminus\alpha)$ [per la somma di ordinali definita mediante i buoni ordini], dove $\gamma\setminus\alpha = \{\delta \in \gamma | \delta \not \in \alpha\}$}.
\begin{figure}[H]
	\centering
	\includegraphics[width = 7.0cm]{immagini/sottrazione_ordinali.png}
\end{figure}

\textcolor{MidnightBlue}{Vediamo ora una dimostrazione formale.}

\begin{proof}
	Abbiamo l'unicità perché la funzione $+$ è crescente nel secondo argomento, come visto nel lemma, dunque è iniettiva, e quindi $\beta$ è unico. Dimostriamo l'esistenza. Se $\alpha = \gamma$ l'enunciato è ovvio [basta usare l'ordinale 0]. Assumiamo quindi $\alpha < \gamma$.\\
	Consideriamo il minimo $\delta$ tale che [la somma supera $\gamma$] $\gamma < \alpha + \delta$ - $\delta$ esiste poiché $\gamma \textcolor{red}{<} s(\gamma) \leq \alpha + s(\gamma)$ [la seconda disuguaglianza è la debole monotonia sulla prima componente, dove al LHS usiamo 0 e al RHS $\alpha$, ciò dimostra che l'insieme di cui stiamo prendendo il minimo è non vuoto].
	Se $\delta$ è successore, sia $\delta = s(\beta)$, allora [essendo $\delta$ minimo per cui $\alpha + \beta > \gamma$, si ha $\alpha + \beta \leq \gamma$] e $\alpha + \beta \leq \gamma < s(\alpha + \beta) = \alpha + s(\beta) = \alpha + \delta$ (che quindi equivale alla cosa presa per ipotesi), da cui segue:
	\[ \alpha + \beta \leq \gamma < s(\alpha + \beta) \implies  \alpha + \beta \leq \gamma \leq \alpha + \beta
		\]
	E quindi [le due disuguaglianze corrispondono a due contenimenti, dunque l'uguaglianza\footnote{Abbiamo un doppio contenimento vero, quindi per tavola di verità  dell'AND devono essere necessariamente veri gli uguali.}] $\gamma = \alpha + \beta$.
	Ci basta quindi mostrare che $\delta$ non è limite (ovviamente non è 0). Se lo fosse avremmo $\gamma \overset{\text{def. $\delta$}}{<} \alpha + \delta \overset{\text{def. ric.}}{=} \sup\{\alpha + \varepsilon | \varepsilon < \delta\}$, ma allora, non essendo l'insieme non vuoto, perché vale quanto appena scritto esiste $\varepsilon < \delta$
	tale che $\gamma < \alpha + \varepsilon$ [il sup prende l'elemento più piccolo tra i più grandi di tutto l'insieme, quindi basta prendere $\varepsilon$ nell'insieme] contro la minimalità di $\delta \;\lightning$.
\end{proof}

\begin{lemma}[Divisione euclidea di ordinali]
	Dati $\alpha,\gamma \in \Ord$, con $\alpha \ne 0$, esistono e sono unici $\beta, \rho \in \Ord$ tali che $\rho < \alpha$ e $\alpha \cdot \beta + \rho = \gamma$.
\end{lemma}

\begin{proof}
	Verifichiamo esistenza e unicità separatamente.
	\begin{itemize}
		\item[$\boxed{\text{unicità}}$] Per il lemma precedente, fissato $\beta$, $\rho$ è unico [per unicità della differenza appunto]. Dobbiamo quindi dimostrare solo l'unicità di $\beta$. Supponiamo per assurdo:
		\[ \alpha \cdot \beta + \rho = \alpha \cdot \beta' + \rho' \qquad\text{con (WLOG) $\beta < \beta'$ e $\rho,\rho'<\alpha$}
			\]
		allora, per la stretta monotonia nella seconda componente della somma, vale che:
		\[ \begin{split}
			\alpha \cdot \beta +\rho\;\textcolor{red}{<}\;& \alpha \cdot \beta + \alpha \\
									  \overset{\text{def. ric.}}{=}& \alpha \cdot s(\beta) \\
									  \overset{\beta < \beta'}{\leq}& \alpha \cdot \beta' \\
									  \overset{\text{2ª comp.}}{\leq}& \alpha \cdot \beta' + \rho' \\
									  \overset{\text{Hp. ass.}}{=}& \alpha \cdot \beta + \rho \quad \textcolor{red}{\lightning}
		\end{split}
			\]
		\item[$\boxed{\text{esistenza}}$] Come nella dimostrazione del lemma precedente, consideriamo il minimo $\delta$ tale che $\gamma < \alpha \cdot \delta$ - che esiste in quanto $\gamma \leq \alpha \cdot \gamma \; \textcolor{red}{<} \alpha \cdot s(\alpha)$ [nella prima disuguaglianza c'è la debole monotonia della prima componente 
		con $1$ e $\alpha$, nell'altra, quella stretta della seconda componente con $\gamma < s(\gamma)$, quindi abbiamo preso $\delta$ come minimo di un insieme non vuoto]. Se $\delta$ è successore, $\delta = s(\beta)$, allora [$\alpha \cdot \beta$, essendo $\beta < \delta$, non può essere strettamente più grande di $\gamma$] $\alpha \cdot \beta \leq \gamma$, quindi [essendo 
		$\alpha \cdot \beta \leq \gamma$], per il lemma precedente, esiste $\rho$ tale che $\alpha \cdot \beta + \rho = \gamma$.
		Per concludere il caso $\delta$ successore, ci basta quindi dimostrare che $\rho < \alpha$ [così da avere tutta la tesi]. Se, per assurdo, fosse $\alpha \leq \rho$, allora si avrebbe:
		\[ \begin{split}
			\gamma &\;\overset{s(\beta) = \delta}{\textcolor{red}{<}}\; \alpha \cdot s(\beta) \\
									     &= \alpha \cdot \beta + \alpha \\
										 &\leq \alpha \cdot \beta + \rho = \gamma \quad \textcolor{red}{\lightning} 
		\end{split}
			\]
		(dove nell'ultima disuguaglianza si è usato appunto che $\alpha \leq \rho$, e quindi si ha la monotonia [debole, data la disuguaglianza debole] sulla seconda componente).\\
		Dobbiamo infine escludere che $\delta$ sia limite. Se lo fosse, avremmo $\gamma < \alpha \cdot \delta \overset{\text{def. ric.}}{=} \sup\{\alpha \cdot \varepsilon | \varepsilon < \delta\}$, ma 
		allora esisterebbe $\varepsilon < \delta$ tale che $\gamma < \alpha \cdot \varepsilon$ [per la solita storia che $\delta$ è il sup di questo insieme], contro la minimalità di $\delta$.
	\end{itemize}
\end{proof}

\subsection{La forma normale di Cantor}

\begin{theorem}[Forma normale di Cantor]
	Ogni ordinale $\alpha$ può essere espresso in maniera unica come somma \textcolor{red}{finita} del tipo:
	\[ \alpha = \omega^{\beta_1} \cdot k_1 + \omega^{\beta_2} \cdot k_2 + \ldots + \omega^{\beta_n} \cdot k_n
		\]
	con $\beta_1 > \beta_2 > \ldots > \beta_n$ ordinali, $k_1,k_2,\ldots,k_n \in \omega\setminus\{0\}$ e $n \in \omega\setminus\{0\}$.
\end{theorem}

\begin{proof}
	Dividiamo la dimostrazione in esistenza ed unicità.
	\begin{itemize}
		\item[$\boxed{\text{esistenza}}$] Per induzione transfinita, supponiamo che ogni ordinale $< \alpha$ abbia una forma normale, dobbiamo dimostrare che $\alpha$ ha una forma normale. Sia $\gamma$ il minimo tale che 
		$\alpha < \omega^\gamma$, che c'è perché [l'insieme su cui prendiamo il minimo è non vuoto in quanto vale sempre almeno che] $\alpha < \omega^{s(\alpha)}$. Come nei lemmi precedenti, consideriamo il caso $\gamma$ successore.
		Sia $\gamma = s(\beta_1)$, allora, per il lemma sulla divisione euclidea, dato che $\omega^{\beta_1} < \alpha$ [$\gamma$ era il minimo per cui $\alpha$ era minore...], possiamo fare la divisione euclidea ottenendo:
		\[ \alpha = \omega^{\beta_1} \cdot k_1 + \rho \qquad \text{con $\rho < \omega^{\beta_1}$}
			\]
		Osserviamo intanto che $0 < k_1 < \omega$. Infatti:
		\begin{align*}
			& 0 = k_1 \implies \alpha = \rho < \omega^{\beta_1} \qquad \text{contro la minimalità di $\gamma\;\lightning$} \\
			& \omega \leq k_1 \implies \omega^{\gamma} = \omega^{s(\beta_1)} \overset{\text{2ª comp. prodotto}}{\leq} \omega^{\beta_1} \cdot k_1 + \rho = \alpha \; \lightning
		\end{align*}
		(dove il secondo assurdo c'è in quanto avevamo preso $\alpha < \omega^\gamma$). Ora $\rho < \omega^{\beta_1} \leq \alpha$ [lemma divisione + $\beta_1<\gamma$], quindi, per ipotesi induttiva, $\rho$ si può scrivere come somma finita:
		\[ \rho = \omega^{\beta_2} \cdot k_2 + \ldots + \omega^{\beta_n} \cdot k_n
			\]
		Siccome $\rho < \omega^{\beta_1}$, abbiamo $\beta_2 < \beta_1$, quindi, sostituendo $\rho$ nell'espressione iniziale si ottiene:
		\[ \alpha = \omega^{\beta_1} \cdot k_1 + \rho = \omega^{\beta_1} \cdot k_1 + \omega^{\beta_2} \cdot k_2 + \ldots + \omega^{\beta_n} \cdot k_n
			\]
		che è una forma normale. Come al solito non resta che escludere quindi che $\gamma$ sia limite, e ciò lo si può fare, osservando che, altrimenti, $\alpha < \omega^\gamma = \sup\{\omega^{\varepsilon} | \varepsilon < \gamma\}$, e questo implica che $\alpha < \omega^{\varepsilon}$, con $\varepsilon < \gamma$ [essendo $\gamma$ il 
		sup dell'insieme], contro la minimalità di $\gamma$.
		\item[$\boxed{\text{unicità}}$] Sia $\alpha$ minimo che non ha un'unica forma normale. Supponiamo che vi siano due forme normali:
		\[ \begin{split}
			\alpha &= \omega^{\beta_1} \cdot k_1 + \omega^{\beta_2} \cdot k_2 + \ldots + \omega^{\beta_n} \cdot k_n \\
				   &= \omega^{\beta_1'} \cdot k_1' + \omega^{\beta_2'} \cdot k_2' + \ldots + \omega^{\beta_{n'}'} \cdot k_{n'}'
		\end{split}
			\]
		(notare che abbiamo usato anche $n'$ al posto di $n$, perché la lunghezza della forma normale può essere diversa). Ci basta dire che $\beta_1 = \beta_1'$ e $k_1 = k_1'$, infatti allora:
		\[ \omega^{\beta_2} \cdot k_2 + \ldots + \omega^{\beta_n} \cdot k_n = \omega^{\beta_2'} \cdot k_2' + \ldots + \omega^{\beta_{n'}'} \cdot k_{n'}'\, \textcolor{purple}{<\, \omega^{\beta_1}} 
			\]
		contro la minimalità di $\alpha$. Supponiamo $\beta_1 = \beta_1'$, allora $\omega^{\beta_1} \cdot k_1 + \overbrace{\ldots}^{\textcolor{purple}{<\,\omega^{\beta_1}}} = \omega^{\beta_1} \cdot k_1' + \overbrace{\ldots}^{\textcolor{purple}{<\, \omega^{\beta_1}}}$ quindi $k_1 = k_1'$ per la divisione euclidea.\\
		Se infine $\beta_1 < \beta_1'$ abbiamo:
		\[ \omega^{\beta_1} \cdot k_1 + \omega^{\beta_2} \cdot k_2 + \ldots + \omega^{\beta_n} \cdot k_n \, \textcolor{purple}{<\, \omega^{s(\beta_1)}} \leq \omega^{\beta_1'} \cdot k_1' + \ldots \; \lightning
			\]
	\end{itemize}
\end{proof}

\begin{exercise}
	Dimostrare le disuguaglianza in viola (sono tutte uguali).
\end{exercise}

\subsection{Punti fissi e \texorpdfstring{$\varepsilon$}{epsilon}-numbers}
Si potrebbe credere che il teorema precedente, applicato ricorsivamente agli esponenti $\beta_1,\ldots,\beta_n$, implichi che ogni ordinale
si possa scrivere sotto forma di un'espressione finita composta di somme, prodotti e potenze delle costanti $0,1,2,\ldots,\omega$. Tipo questa:
\[ \omega^{\omega^4 \cdot 7 + \omega^2 \cdot 1}\cdot 9 + \omega^{75} + 9
	\]
Effettivamente, se valesse $\textcolor{red}{\alpha > \beta_1}>\beta_2>\ldots>\beta_n$ per ogni $\alpha$, allora questa conclusione sarebbe corretta.\\
Però è possibile esibire un ordinale $\varepsilon_0$ - e, in realtà, un'intera classe propria di ordinali come questo - tale che $\varepsilon_0 = \omega^{\varepsilon_0}$\footnote{Notare che in questo caso non vale che $\alpha > \beta_1$}.
La forma normale di Cantor di $\varepsilon_0$ è quindi, chiaramente, $\omega^{\varepsilon_0}$, e procedere ricorsivamente sull'esponente $\omega^{\omega^{\varepsilon_0}},\omega^{\omega^{\omega^{\varepsilon_0}}}$, etc. non conduce ad un'espressione finita, intuitivamente verrebbe una cosa del tipo:
\[ \varepsilon_0 = \underbrace{\omega^{\omega^{\omega^{\omega^{\iddots}}}}}_{\text{$\omega$ volte}}
	\]
La proposizione seguente è interessante di per sé, ma, in particolare, ci permetterà di dimostrare l'esistenza degli \vocab{$\varepsilon$-numbers}.

\begin{proposition}[Ogni funzione ordinale continua ha un punto fisso $\geq \alpha$]
	Sia $F : \Ord \rightarrow \Ord$ una funzione classe [debolmente] crescente e continua - ossia $F(\lambda) = \sup F[\lambda]$ per $\lambda$ limite. Allora, per ogni $\alpha \in \Ord$, $F$ ha un punto fisso $\geq \alpha$. Ossia:
	\[ \exists \pi \in \Ord \; \alpha \leq \pi \land F(\pi) = \pi
		\]
\end{proposition}

\begin{proof}
	Definiamo per ricorsione transfinita (stiamo usando la ricorsione per mezzo di una funzione classe) $\pi_0 = \alpha$, $\pi_{s(n)} = F(\pi_n)$ per $n \in \omega$.\\
	Se $F(0) = 0$, allora $\pi = 0$ è un punto fisso.\\
	Se $0 < F(0)$, allora, induzione, $\forall n \in \omega \;\pi_n < \pi_{s(n)}$ per cui la funziona $n \mapsto \pi_n$ è crescente. Di conseguenza:
	\[ \pi \Mydef \sup\{\pi_n | n \in \omega\}
		\]
	è limite (perché $\pi \not \in \{\pi_n | n \in \omega\}$, dunque vale il lemma sugli insiemi di ordinali in cui il sup non appartiene all'insieme). Quindi:
	\[ F(\pi) = \sup F[\pi] = \sup \{F(\pi_n) | n \in \omega\} = \sup\{\pi_{s(n)} | n \in \omega\} = \pi
		\]
	(la dimostrazione ci mostra anche che i punti fissi sono una classe propria di ordinali [se fossero un insieme ci sarebbe un sup, e gli ordinali sarebbero segmenti iniziali dell'ordinale che necessariamente ne viene fuori]).
\end{proof}

\begin{example}
	Sia $\varepsilon$ un punto fisso di $x \mapsto \omega^x$, allora la forma normale di $\varepsilon$ è $\varepsilon = \omega^\varepsilon$.
\end{example}

\begin{exercise}
	Sia $\varepsilon_0 = \sup\{1,\omega,\omega^{\omega},\omega^{\omega^{\omega}}, \omega^{\omega^{\omega^{\omega}}},\ldots\}$. Formalmente definiamo per ricorsione $\alpha_0 = 1$, $\alpha_{n+1}=\omega^{\alpha_n}$,
	allora $\varepsilon_0 = \sup\{\alpha_n | n \in \omega\}$. Dimostrare che $\varepsilon_0$ è il più piccolo punto fisso della funziona $x \mapsto \omega^x$.
\end{exercise}

\begin{exercise}
	Sia $F : \Ord \rightarrow \Ord$ crescente e continua. Allora esiste $G : \Ord \rightarrow \Ord$ crescente tale che:
	\[ \forall \alpha \in \Ord \; F(\alpha) = \alpha \leftrightarrow \exists \beta \in \Ord \; \alpha = G(\beta)
		\]
	(ossia $F$ ha una classe propria di punti fissi).
\end{exercise}

\begin{exercise}
	La $G$ dell'esercizio precedente è univocamente determinata da $F$ ed è continua.
\end{exercise}

\begin{definition}[$\varepsilon$-numbers]
	Se negli esercizi precedenti $F(\alpha) = \omega^\alpha$, allora $\varepsilon_{\alpha} \Mydef G(\alpha)$\footnote{E quindi ce n'è uno per ogni ordinale.}.
\end{definition}

\textcolor{MidnightBlue}{L'$\alpha$-esimo $\varepsilon$-number è L'$\alpha$-esimo punto fisso di $x \mapsto \omega^x$.}\\
I primi due esercizi seguenti saranno assai più facili quando, usando l'assioma della scelta, dimostreremo che un insieme numerabile di insiemi numerabili è numerabile.\footnote{Ricordare che l'avevamo già dimostrato dando per buono di avere una successione di enumerazioni,
ma anche il quel caso l'enumerazione ce la si può procurare solo con scelta.}

\begin{exercise}[\textcolor{red}{$\star$ Difficile senza leggere l'idea sotto}]
	$|\varepsilon_0| = \aleph_0$.\footnote{\underline{\textbf{Idea}}: dimostrare che $\alpha \in \varepsilon_0$ se e solo se $\alpha$ può essere scritto a partire da $0,1,\omega$, applicando le operazioni di somma, prodotto, ed esponente ordinale un numero finito di volte.}
\end{exercise}

\begin{exercise}[\textcolor{red}{$\star \star$ Ostico}]
	Sia $\zeta_0$ minimo tale che $\varepsilon_{\zeta_0} = \zeta_0$, allora $|\zeta_0| = \aleph_0$.
\end{exercise}

\begin{exercise}
	Sia $\omega$ un qualunque ordinale $\geq 2$. Ogni ordinale $\alpha$ si scrive in modo unico come somma finita:
	\[ \alpha = \gamma^{\beta_1}\cdot k_1 + \ldots \gamma^{\beta_n}\cdot k_n
		\]
	con $\beta_1 > \beta_2 > \ldots > \beta_n$ (ordinali) e $k_1,\ldots,k_n < \gamma$.
\end{exercise}

\subsection{Operazioni in forma normale di Cantor}
È facile ridurre l'aritmetica ordinale, in forma normale di Cantor, ad una piccola collezione di regole meccaniche.
Nel contesto del corso, queste regole hanno un'importanza limitata, è però utile sapere che ci sono, ed avere un'idea del loro aspetto.
Il lemma seguente è un caso particolare, ma è semplice e vale la pena ricordarlo.

\begin{lemma}[Assorbimento a destra dell'ordinale più grande]
	Siano $\alpha,\beta,\gamma \in \Ord$ tali che $\alpha < \omega^{\beta} \leq \gamma$\footnote{Naturalmente $\beta>0$.}, allora:
	\[ \alpha + \textcolor{orange}{\gamma} = \textcolor{orange}{\gamma}
		\]
\end{lemma}

\textcolor{MidnightBlue}{Ossia fare $\alpha + \gamma$ assorbe tutti gli $\alpha$ abbastanza piccoli, ossia quelli minori di qualche potenza di $\omega$ che sia a sua volta minore o uguale a $\gamma$.}\footnote{È falso 
tuttavia che $\textcolor{red}{\gamma + \alpha = \gamma}$, infatti per la stretta monotonia nella seconda componente della somma non si può avere $\gamma + 0$ al RHS, perché $0 < \alpha$.}

\begin{proof}
	Ci basta dimostrare che $\alpha + \gamma \leq \gamma$ (l'altra disuguaglianza è automatica per debole monotonia della prima componente, $\textcolor{MidnightBlue}{0} + \gamma \leq \textcolor{MidnightBlue}{\alpha} + \gamma$\footnote{Naturalmente escludiamo il caso $\alpha = 0$, perché in questo caso si ottiene ancora banalmente la tesi.}).\\
	Scrivendo $\alpha$ in forma normale di Cantor otteniamo:
	\[ \alpha = \omega^{\beta_1} \cdot k_1 + \ldots + \omega^{\beta_n} \cdot k_n
		\]
	con gli ordinali $\beta_1 > \beta_2 > \ldots > \beta_n$. Quindi si ha che $\alpha \leq \omega^{\beta_1} \cdot k$, per qualche $k \in \omega$, infatti [iterando la debole monotonia sulla prima componente sia di somma che del prodotto, dal termine $n$-esimo al secondo, si ottiene]:
	\[ \alpha = \omega^{\beta_1} \cdot k_1 + \ldots + \omega^{\beta_n} \cdot k_n \leq \alpha = \omega^{\beta_1} \cdot k_1 + \omega^{\beta_{\textcolor{red}{1}}} \cdot k_2 + \ldots + \omega^{\beta_{\textcolor{red}{n}}} \cdot k_n = \omega^{\beta_1}\cdot (k_1 + \ldots + k_n)
		\]
	con $k := k_1 + \ldots + k_n$. Ora, da [l'ipotesi] $\alpha < \omega^\beta$, deduciamo $\beta_1 < \beta$ [basta fare il confronto tra la forma normale di $\alpha$ e la disuguaglianza per ipotesi usando la monotonia delle potenze],
	quindi $s(\beta_1) \leq \beta$ e possiamo scrivere [usando la sottrazione, visto che il primo termine sta ancora in $\gamma$ che] $\gamma = \omega^{s(\beta_1)} + \gamma' = \omega^{\beta_1} \cdot \omega + \gamma'$, $\gamma' < \gamma$.
	Da cui:
	\[ \alpha + \gamma \overset{\text{sopra + monot.}}{\leq} \omega^{\beta_1} \cdot k + \omega^{\beta_1} \cdot \omega + \gamma = \omega^{\beta_1} (k + \omega) \;\textcolor{red}{=}\; \omega^{\beta_1} \cdot \omega + \gamma' = \gamma
		\]
	dove l'uguaglianza in \textcolor{red}{rosso} segue da $k + \omega = \omega$ [ricordiamo $k \in \omega\setminus\{0\}$], che a sua volta segue da:
	\[ k + \omega = \sup\{k + n | n < \omega\} = \omega
		\]
\end{proof}

\begin{proposition}[Regole di calcolo in forma normale di Cantor]
	Per le somme ($c \ne 0$, $d \ne 0$) vale che:
	\[ \omega^{\alpha} \cdot c + \omega^\beta \cdot d = \begin{cases}
		\omega^\beta \cdot d &\text{se $\alpha < \beta$} \\
		\omega^\alpha \cdot (c + d) &\text{se $\alpha = \beta$} \\
		\omega^{\alpha} \cdot c + \omega^\beta \cdot d &\text{se $\beta < \alpha$}\footnote{È già in forma normale, poiché $\beta < \alpha$, le operazioni cancellano solo cose scritte con $\omega$ con esponenti non tutti in ordine strettamente decrescente, ovvero non già completamente in forma normale.}
	\end{cases}
		\]
	Per i prodotti si applica la proprietà distributiva, e poi le regole seguenti:
	\begin{align*}
		\beta > 0 \rightarrow (\omega^{\alpha_1} \cdot k_1 + \ldots \omega^{\alpha_2} \cdot k_2 + \ldots) \cdot \omega^{\beta} &= \omega^{\alpha_1 + \beta} \\
		n \in \omega\setminus\{0\} \rightarrow (\omega^{\alpha_1} \cdot k_1 + \ldots \omega^{\alpha_2} \cdot k_2 + \ldots) \cdot n &= \omega^{\alpha_1} \cdot k_1\textcolor{red}{n} + \ldots \omega^{\alpha_2} \cdot k_2 + \ldots
	\end{align*}
	Per le potenza si usano $\alpha^{\beta + \gamma} = \alpha^\beta \cdot \alpha^\gamma$ e $\alpha^{\beta \cdot n} = (\alpha^\beta)^n$, poi:
	\begin{align*}
		k \in \omega\setminus\{0\}\qquad\qquad k^{\omega^{1 + \alpha}} &= \omega^{\omega^{\alpha}} \\
		\beta > 0 \land \alpha_1 > 0 \rightarrow (\omega^{\alpha_1} \cdot k_1 + \ldots \omega^{\alpha_2} \cdot k_2 + \ldots)^{\omega^\beta} &= \omega^{\alpha_1 \cdot \omega^\beta}
	\end{align*}
\end{proposition}

\begin{proof}
	La regole per la somma sono immediate: la prima è il lemma precedente, infatti se $\alpha < \beta$, allora $\omega^\alpha \cdot c < \textcolor{purple}{\omega^{s(\alpha)}} \leq \omega^{\beta} \cdot d$, e quindi nella somma si salva solo il termine di destra;
	la seconda è la proprietà distributiva a sinistra valida per tutti gli ordinali, e la terza è la stessa forma normale di Cantor, che per ipotesi non può essere semplificata ulteriormente.\\
	Per dimostrare che:
	\[ \textcolor{purple}{\beta > 0 \rightarrow (\omega^{\alpha_1}\cdot k_1 + \omega^{\alpha_2} \cdot k_2 + \ldots) \cdot \omega^{\beta} = \omega^{\alpha_1 + \beta}}
		\]
	osserviamo intanto il caso particolare $n \cdot \omega = \omega$ per $n \in \omega\setminus\{0\}$:
	\[ \omega \leq n \cdot \omega = \sup\{n \cdot i | i \in \omega\} \leq \sup\{j | j\footnote{Typo di Mamino.} \in \omega\} \leq \omega
		\]
	(la prima disuguaglianza è la solita debole monotonia sulla prima componente del prodotto, la seconda disuguaglianza è il lemma sulla disuguaglianza dei sup\footnote{Se per ogni elemento del primo insieme ne trovo sempre uno del secondo che
	lo domina, allora c'è la disuguaglianza tra gli estremi superiori.}, in particolare il lemma si può applicare al contrario e ottenere proprio uguaglianza dei sup, infine, l'ultima è proprio un'uguaglianza data dal fatto che $\omega$ è limite e quindi uguale al sup delle cose più piccole).
	Ora, scrivendo $\beta = 1 + \gamma$ (abbiamo supposto $\beta > 0$, quindi vale il lemma sulla sottrazione, ed otteniamo un'unico $\gamma \in \Ord$ per cui vale quella somma), si ottiene:\footnote{Typo Mamino al primo uguale dopo le tre disuguaglianze.}
	\[ \begin{split}
		\textcolor{orange}{\omega^{\alpha_1 + \beta}} &= \omega^{\alpha_1}\omega^\beta \\
								  &\leq \textcolor{MidnightBlue}{(\omega^{\alpha_1} \cdot k_1 + \omega^{\alpha_2} \cdot k_2 + \ldots) \cdot \omega^{\beta}} \\
								  &\leq (\omega^{\alpha_1} \cdot k_1 + \omega^{\alpha_2} \cdot k_2 + \ldots + \textcolor{red}{\omega^{\alpha_1}}) \cdot \omega^{\beta} \\
								  &\leq \omega^{\alpha_1}(k_1+1)\cdot \omega^{\beta} \\
		   						  &= \omega^{\alpha_1}\underbrace{(k_1 + 1)\omega}_{\omega}\omega^{\gamma}\\
		   						  &= \omega^{\alpha_1} \cdot \omega \cdot \omega^{\gamma} \\
								  &= \textcolor{orange}{\omega^{\alpha_1 + \beta}}
 	\end{split}
		\]
	dove: la prima uguaglianza sono le proprietà delle potenze degli ordinali; la seconda disuguaglianza è la debole monotonia della prima componente del prodotto; nella terza abbiamo aggiunto $\omega^{\alpha_1}$ alla fine,
	ed è la stretta monotonia sulla seconda componente della somma, essendo $0 < \omega^{\alpha_1}$, unita alla debole monotonia sulla prima componente del prodotto totale, da cui la disuguaglianza larga; la quarta è la regola della somma, infatti per l'ipotesi sulla forma normale di Cantor,
	avendo aggiunto $\omega^{\alpha_1}$ alla fine, i termini vengono cancellati [sarebbe il lemma sopra applicato alle coppie da destra verso sinistra man mano], si ottiene $\omega^{\alpha_1} \cdot k_1 + \omega^{\alpha_1}$ e infine si usa la distributività a sinistra;
	per la quinta uguaglianza stiamo usando quanto visto sopra, cioè $\beta = 1 + \gamma$ e le solite proprietà delle potenze; la sesta uguaglianza è il caso particolare visto sopra [$n \cdot \omega = \omega$], essendo $k_1 + 1 \in \omega$; infine, nell'ultima uguaglianza usiamo ancora che $\beta = 1 + \gamma$.\\
	La seconda regola, del prodotto di un ordinale in forma normale e un naturale:
	\[ \textcolor{purple}{n \in \omega\setminus\{0\} \rightarrow (\omega^{\alpha_1} \cdot k_1 + \ldots \omega^{\alpha_2} \cdot k_2 + \ldots) \cdot n = \omega^{\alpha_1} \cdot k_1\textcolor{red}{n} + \ldots \omega^{\alpha_2} \cdot k_2 + \ldots}
		\]
	si ottiene per induzione su $n$. La prima per il prodotto invece è immediata:
	\[ \begin{split}
		\textcolor{purple}{k^{\omega^{1+\alpha}}} &= k^{\omega \cdot \omega^{\alpha}} \\
												  &= (k^{\omega})^{\omega^{\alpha}} \\
												  &= (\sup\{k^{n} | n \in \omega\})^{\omega^{\alpha}} \\
												  &= \omega^{\omega^{\alpha}}
		\end{split}
		\]
	sono solo la definizione ricorsiva della potenza nel caso limite e le proprietà delle potenze degli ordinali, l'unica cosa degna di nota da osservare è che l'estremo superiore di quell'insieme, 
	per il solito lemma [usato per una doppia disuguaglianza], è uguale all'estremo superiore ad esempio di $\{n | n \in \omega\}$, per questo motivo si vede che è $\omega$ stesso.\\
	Per dimostrare infine l'ultima regola sulle potenze di ordinali in forma normale:
	\[ \textcolor{purple}{\beta > 0 \land \alpha_1 > 0 \rightarrow (\omega^{\alpha_1} \cdot k_1 + \omega^{\alpha_2} \cdot k_2 + \ldots)^{\omega^\beta} = \omega^{\alpha_1 \cdot \omega^{\beta}}}
		\]
	partiamo dal caso particolare $(\omega^{\alpha} \cdot k)^{\omega} = \omega^{\alpha \cdot \omega}$:
	\[ \begin{split}
		\textcolor{orange}{\omega^{\alpha \cdot \omega}} &\leq \textcolor{MidnightBlue}{(\omega^{\alpha} \cdot k)^{\omega}} \\
														 &= \sup{(\omega^\alpha \cdot k)^n | n \in \omega} \\
														 &= \sup\{\omega^{\alpha \cdot n}\cdot k^n | n \in \omega\} \\
														 &\leq \sup\{\omega^{\alpha \cdot (n + 1) | n \in \omega}\} \\
														 &\leq (\omega^{\alpha})^\omega = \textcolor{orange}{\omega^{\alpha \cdot \omega}}
	\end{split}
		\]
	dove: la prima disuguaglianza è la solita monotonia, applicata al prodotto interno; la seconda uguaglianza è la definizione ricorsiva di potenza di un ordinale nel caso limite;
	la terza uguaglianza deriva dal fatto che $n \cdot \omega = \omega$ per quanto visto, quindi, facendo la potenza tutti i $k$ davanti agli $\omega^\alpha$ scompaiono e rimane solo l'ultimo;
	la quarta disuguaglianza è la stretta monotonia sulla seconda componente del prodotto, usando $k < \omega^\alpha$, e poi sono semplicemente le proprietà delle potenze degli ordinali;
	la quinta disuguaglianza è in realtà un uguaglianza per la definizione ricorsiva delle potenze nel caso limite, e, infine l'ultima uguaglianza è data dalle proprietà delle potenze degli ordinali.\\
	Siccome $\beta > 0$, allora $\beta \geq 1$, dunque possiamo scrivere $\beta = 1 + \gamma$, per un unico $\gamma \in \Ord$, dunque abbiamo $\omega^\beta = \omega \cdot \omega^\gamma$ \footnote{Typo di Mamino che si porta dietro tutto il conto.}, da cui:
	\[ \begin{split}
		\textcolor{orange}{\omega^{\alpha_1 \cdot \omega^\beta}} &\leq \textcolor{MidnightBlue}{(\omega^{\alpha_1} \cdot k_1 + \omega^{\alpha_2} \cdot k_2 + \ldots)^{\omega^\beta}} \\
															   &\leq (\omega^{\alpha_1} \cdot (k_1 + 1))^{\omega\cdot \omega^{\gamma}} \\
															   &= ((\omega^{\alpha_1} \cdot (k_1 + 1))^{\omega})^{\omega^{\gamma}} \\
															   &= (\omega^{\alpha_1 \cdot \omega})^{\omega^\gamma} \\
															   &= \omega^{\alpha_1 \cdot \omega \cdot \omega^\gamma} \\
															   &= \textcolor{orange}{\omega^{\alpha_1 \cdot \omega^\beta}}
	\end{split}
		\]
	dove: la prima disuguaglianza è debole monotonia sulla base della potenza [ovvero il primo argomento]; per la seconda ci basta aggiungere ai termini della somma $\omega^{\alpha_1}$ alla fine (o sostituirlo all'ultimo termine, è indifferente) e poi [dopo aver usato la regola per la somma],
	si ha la monotonia sulla seconda componente e sulla base della potenza; la terza uguaglianza sono le proprietà delle potenze; la quarta uguaglianza è il caso particolare; la quinta sono di nuovo le proprietà delle potenze di ordinali, e, infine la sesta era il fatto che $\omega^\beta = \omega^{1 + \gamma}$.
\end{proof}

\begin{example}[Operazioni tra ordinali in forma normale di Cantor]
	Elenchiamo alcuni esempi usando le proprietà appena viste:
	\begin{itemize}
		\item $(\omega + 1)^2 = (\omega + 1)(\omega + 1) = (\omega + 1)\cdot \omega + (\omega + 1) \cdot 1 = \omega^2 + \omega + 1$, le uniche cose usate sono la distributività a sinistra del prodotto di ordinali, la prima regola per il prodotto di ordinali e volendo la seconda nel caso di prodotto per $1$ [che in teoria abbiamo già gratis come elemento neutro dalle regole generali per gli ordinali].
		\item $(\omega + 1)^2 \cdot n = (\omega^2 + \omega + 1) \cdot n = \omega^2 \cdot n + \omega + 1$, dove $n \in \omega\setminus\{0\}$. In questo caso abbiamo combinato semplicemente il risultato sopra con la seconda regola per il prodotto di ordinali in forma normale.
		\item $(\omega + 1)^2 \cdot \omega = (\omega^2 + \omega + 1) \cdot \omega = \omega^3$, come sopra, ma usando la prima regola per il prodotto.
		\item $(\omega + 1)^3 = (\omega^2 + \omega + 1) \cdot (\omega + 1) = (\omega^2 + \omega + 1) \cdot \omega + \omega^2 + \omega + 1 = \omega^3 + \omega^2 + \omega + 1$, abbiamo usato distributività e seconda regola per il prodotto.
		\item $(\omega + 1)^n = \omega^n + \omega^{n - 1} + \ldots + 1 = \sum_{i = n}^0 \omega^i$\footnote{Notare la somma al contrario, perché l'ordine conta in forma normale di Cantor.}, $n \in \omega$. Lo si vede per induzione, i casi base sono fatti sopra (il caso 0 è il caso base della definizione ricorsiva di potenza ordinale), dunque possiamo procedere per induzione e fare il passo induttivo:
		\[ (\omega + 1)^{n + 1} = (\omega + 1)^n \cdot (\omega + 1) \overset{\text{Hp. indutt.}}{=} \left(\sum_{i = n}^n \omega^i\right) \cdot (\omega + 1) = \left(\sum_{i = n}^n \omega^i\right) \cdot \omega + \sum_{i = n}^n \omega^i
			\]
		e usando la prima regola per il prodotto si ottiene:
		\[ \omega^{n + 1} + \sum_{i = n}^n \omega^i = \sum_{i = n + 1}^n \omega^i
			\]
		che è proprio la tesi nel caso successore.
		\item $(\omega + 1)^\omega = \omega^\omega$, usando la seconda regola per le potenze.
		\item $(2 \cdot \omega^2 + \omega \cdot 3 + 7)^3$ \footnote{Dall'\href{https://ciovil.li/eti20/exam05.pdf}{\textcolor{purple}{esame del 27-1-2020}}.}, osserviamo che $2 \cdot \omega^2 = 2 \cdot \omega \cdot \omega = (2 \cdot \omega) \cdot \omega = \omega \cdot \omega = \omega^2$, per l'osservazione fatta prima, secondo cui $n \cdot \omega = \omega$, per $n \in \omega\setminus\{0\}$.
		Da qui si può procedere cone le regole che conosciamo, calcoliamo per comodità prima il quadrato:
		\[ \begin{split}
			(\omega^2 + \omega \cdot 3 + 7)^2 &= (\omega^2 + \omega \cdot 3 + 7) \cdot (\omega^2 + \omega \cdot 3 + 7) \\
											  &= (\omega^2 + \omega \cdot 3 + 7) \cdot \omega^2 + (\omega^2 + \omega \cdot 3 + 7) \cdot \omega \cdot 3 \\
											  &+ (\omega^2 + \omega \cdot 3 + 7) \cdot 7 \\
											  &= \omega^4 + \omega^3 \cdot 3 + \omega^2\cdot 7 + \omega \cdot 3 + 7
		\end{split}
			\]
		iterando ancora una volta la distributività, le regole per il prodotto [e ricordando che quest'ultimo è associativo], si ottiene il risultato:
		\[ (\omega^2 + \omega \cdot 3 + 7)^3 = \omega^6 + \omega^5 \cdot 3 + \omega^4 \cdot 7 + \omega^3 \cdot 3 + \omega^2\cdot 7 + \omega \cdot 3 + 7
			\]
	\end{itemize}
\end{example}

\pagebreak
\section{Gli aleph}
In questa sezione costruiremo una funzione classe dagli ordinali in sé, $\alpha \mapsto \omega_\alpha$, la cui immagine contiene precisamente un ordinale per ogni cardinalità infinita.
Definiremo la scrittura $|X| = \aleph_\alpha$, come $|X| = |\omega_\alpha|$ (o molto più semplicemente $|\omega_\alpha| \Mydef \aleph_\alpha$). Indagheremo inoltre l'aritmetica, che è molto semplice, di somme e prodotti di cardinalità: $\aleph_\alpha + \aleph_\beta = \aleph_\alpha \cdot \aleph_\beta = \aleph_{\max(\alpha,\beta)}$.
Tratteremo, invece, in seguito l'esponenziale di cardinalità, che non è affatto semplice.\\
Formalmente, in realtà, dimostreremo che ogni cardinalità infinita \textcolor{purple}{che sia la cardinalità di qualche ordinale} è un aleph. Resterà quindi da dimostrare che ogni cardinalità è la cardinalità di qualche ordinale, ma per farlo occorre l'assioma della scelta.
Le cardinalità degli ordinali fanno comodo, per esempio, perché sono confrontabili.

\begin{remark}[Confronto cardinalità degli ordinali]
	Dati $\alpha,\beta \in \Ord$, o $|\alpha| < |\beta|$ o $|\alpha| = |\beta|$ o $|\beta| < |\alpha|$.
\end{remark}

\begin{proof}
	Basta osservare che, data la totalità della relazione d'ordine tra gli ordinali, o $\alpha \subseteq \beta$ o $\beta \subseteq \alpha$, quindi o $|\alpha| \leq |\beta|$ o $|\beta| \leq |\alpha|$.
\end{proof}

Ad ogni cardinalità associamo un rappresentante canonico: il minimo ordinale di quella cardinalità.

\begin{definition}[Ordinale iniziale]
	$\alpha \in \Ord$ è un \vocab{ordinale iniziale} se $\forall \beta < \alpha \; |\beta| < |\alpha|$.
\end{definition}

\begin{exercise}
	Dimostrare che se $\alpha$ è un ordinale iniziale, allora $\alpha$ è limite.
\end{exercise}

\subsection{Teorema di Hartogs}
Il nostro scopo è, ora, dimostrare che gli ordinali iniziali sono una classe propria, e quindi enumerarli per mezzo di una funzione classe $\Ord \rightarrow \Ord : \alpha \mapsto \omega_\alpha$.
Quello che segue è lo strumento tecnico fondamentale.

\begin{theorem}[Teorema di Hartogs]
	Dato un insieme $X$ esiste un ordinale $\alpha$ che non è equipotente ad alcun sottoinsieme di $X$, ossia $|\alpha| \not\leq |X|$.
\end{theorem}

\begin{proof}
	
\end{proof}

\subsection{Somme e prodotti di aleph}

\pagebreak
\appendix
\section{Soluzioni di altri esercizi e cardinalità note}
\pagebreak
\begin{thebibliography}{9}
	\addcontentsline{toc}{section}{Bibliografia}
	\bibitem{jech}
	Karel Hrbacek, Thomas Jech,
	\textit{Introduction to Set Theory, Revised and Expanded},
	CRC Press, Boca Raton, Florida,
	3rd edition,
	1999.

	\bibitem{diNasso_eti_2019_20}
	Mauro Di Nasso,
	\href{https://people.dm.unipi.it/dinasso/ETI/dispensa-04ss.pdf}{\textit{Elementi di teoria degli insiemi, Dispensa 4}},
	Università di Pisa, Pisa,
	2019-20.


	\bibitem{mamino_eti_20_21}
	Marcello Mamino,
	\href{https://ciovil.li/eti20/}{\textit{Elementi di teoria degli insiemi}},
	Università di Pisa, Pisa,
	2020-21.
\end{thebibliography}

\end{document}